"""
ì˜ˆì œ 1: Playwright í™˜ê²½ ì„¤ì • ë° ê¸°ë³¸ ì‚¬ìš©ë²•

2íšŒì°¨ ê°•ì˜: AI ì—ì´ì „íŠ¸ ê¸°ë°˜ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì‹œìŠ¤í…œ
Playwright ê¸°ë³¸ ì„¤ì •ê³¼ ë¸Œë¼ìš°ì € ê´€ë¦¬ ì‹¤ìŠµ
"""

import asyncio
import time
from datetime import datetime

from crawlers.playwright_basic import PlaywrightManager, ContextualCrawler
from config.settings import settings


async def example_basic_setup():
    """ê¸°ë³¸ Playwright ì„¤ì • ì˜ˆì œ"""
    print("ğŸ­ Playwright ê¸°ë³¸ ì„¤ì • ì˜ˆì œ")
    print("=" * 50)

    # PlaywrightManager ì‚¬ìš©
    async with PlaywrightManager() as manager:
        print(f"âœ… ë¸Œë¼ìš°ì € íƒ€ì…: {settings.crawler.browser_type}")
        print(f"âœ… í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ: {settings.crawler.headless}")

        # ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = await manager.create_context()
        print("âœ… ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ")

        # í˜ì´ì§€ ìƒì„± ë° í…ŒìŠ¤íŠ¸
        page = await context.new_page()

        # ê°„ë‹¨í•œ ì›¹ì‚¬ì´íŠ¸ ë°©ë¬¸
        test_url = "https://httpbin.org/html"
        print(f"ğŸŒ í…ŒìŠ¤íŠ¸ URL ë°©ë¬¸: {test_url}")

        start_time = time.time()
        await page.goto(test_url)
        load_time = time.time() - start_time

        # í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ
        title = await page.title()
        print(f"ğŸ“„ í˜ì´ì§€ ì œëª©: {title}")
        print(f"â±ï¸ ë¡œë“œ ì‹œê°„: {load_time:.2f}ì´ˆ")

        # í˜ì´ì§€ ì •ë¦¬
        await page.close()
        await manager.cleanup_context(context)


async def example_contextual_crawler():
    """ContextualCrawler ì‚¬ìš© ì˜ˆì œ"""
    print("\nğŸ•·ï¸ ContextualCrawler ì‚¬ìš© ì˜ˆì œ")
    print("=" * 50)

    async with ContextualCrawler() as crawler:
        # ë‹¨ì¼ URL í¬ë¡¤ë§
        test_url = "https://httpbin.org/json"
        print(f"ğŸ¯ ë‹¨ì¼ URL í¬ë¡¤ë§: {test_url}")

        result = await crawler.crawl_url(test_url)

        if result['success']:
            print("âœ… í¬ë¡¤ë§ ì„±ê³µ!")
            print(f"   - URL: {result['url']}")
            print(f"   - ì œëª©: {result['title']}")
            print(f"   - ì‘ë‹µ ìƒíƒœ: {result['response_status']}")
            print(f"   - ì½˜í…ì¸  ê¸¸ì´: {len(result['content'])} ë¬¸ì")
        else:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {result['error']}")


async def example_multiple_contexts():
    """ë‹¤ì¤‘ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© ì˜ˆì œ"""
    print("\nğŸ”„ ë‹¤ì¤‘ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© ì˜ˆì œ")
    print("=" * 50)

    async with PlaywrightManager() as manager:
        # ì¼ë°˜ ì»¨í…ìŠ¤íŠ¸
        context1 = await manager.create_context()
        print("âœ… ì»¨í…ìŠ¤íŠ¸ 1 ìƒì„± (ì¼ë°˜)")

        # ìŠ¤í…”ìŠ¤ ì»¨í…ìŠ¤íŠ¸
        context2 = await manager.create_stealth_context()
        print("âœ… ì»¨í…ìŠ¤íŠ¸ 2 ìƒì„± (ìŠ¤í…”ìŠ¤)")

        # ê° ì»¨í…ìŠ¤íŠ¸ì—ì„œ í˜ì´ì§€ ìƒì„±
        page1 = await context1.new_page()
        page2 = await context2.new_page()

        # ë™ì‹œì— ë‹¤ë¥¸ í˜ì´ì§€ ë°©ë¬¸
        tasks = [
            page1.goto("https://httpbin.org/user-agent"),
            page2.goto("https://httpbin.org/headers")
        ]

        start_time = time.time()
        await asyncio.gather(*tasks)
        parallel_time = time.time() - start_time

        print(f"âš¡ ë³‘ë ¬ ë¡œë”© ì‹œê°„: {parallel_time:.2f}ì´ˆ")

        # User-Agent ë¹„êµ
        content1 = await page1.content()
        content2 = await page2.content()

        print("ğŸ” ì»¨í…ìŠ¤íŠ¸ë³„ ì„¤ì • í™•ì¸:")
        print(f"   - ì»¨í…ìŠ¤íŠ¸ 1 ì½˜í…ì¸  ê¸¸ì´: {len(content1)} ë¬¸ì")
        print(f"   - ì»¨í…ìŠ¤íŠ¸ 2 ì½˜í…ì¸  ê¸¸ì´: {len(content2)} ë¬¸ì")

        # ì •ë¦¬
        await page1.close()
        await page2.close()
        await manager.cleanup_context(context1)
        await manager.cleanup_context(context2)


async def example_error_handling():
    """ì˜¤ë¥˜ ì²˜ë¦¬ ì˜ˆì œ"""
    print("\nâš ï¸ ì˜¤ë¥˜ ì²˜ë¦¬ ì˜ˆì œ")
    print("=" * 50)

    async with ContextualCrawler() as crawler:
        # ì˜ëª»ëœ URL í…ŒìŠ¤íŠ¸
        invalid_urls = [
            "https://invalid-domain-12345.com",
            "https://httpbin.org/status/404",
            "https://httpbin.org/delay/10"  # íƒ€ì„ì•„ì›ƒ í…ŒìŠ¤íŠ¸
        ]

        for url in invalid_urls:
            print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ URL: {url}")
            result = await crawler.crawl_url(url, wait_for_load_state="domcontentloaded")

            if result['success']:
                print(f"   âœ… ì„±ê³µ: ìƒíƒœ {result['response_status']}")
            else:
                print(f"   âŒ ì‹¤íŒ¨: {result['error_type']} - {result['error']}")


async def example_performance_comparison():
    """ì„±ëŠ¥ ë¹„êµ ì˜ˆì œ"""
    print("\nğŸ“Š ì„±ëŠ¥ ë¹„êµ ì˜ˆì œ")
    print("=" * 50)

    test_urls = [
        "https://httpbin.org/html",
        "https://httpbin.org/json", 
        "https://httpbin.org/xml",
    ]

    async with ContextualCrawler() as crawler:
        # ìˆœì°¨ ì²˜ë¦¬
        print("ğŸŒ ìˆœì°¨ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸...")
        start_time = time.time()

        sequential_results = []
        for url in test_urls:
            result = await crawler.crawl_url(url)
            sequential_results.append(result)

        sequential_time = time.time() - start_time

        # ë³‘ë ¬ ì²˜ë¦¬
        print("ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸...")
        start_time = time.time()

        parallel_results = await crawler.crawl_multiple_urls(
            test_urls, 
            max_concurrent=3,
            delay_between_requests=0.1
        )

        parallel_time = time.time() - start_time

        # ê²°ê³¼ ë¹„êµ
        print(f"\nğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:")
        print(f"   - ìˆœì°¨ ì²˜ë¦¬: {sequential_time:.2f}ì´ˆ")
        print(f"   - ë³‘ë ¬ ì²˜ë¦¬: {parallel_time:.2f}ì´ˆ")
        print(f"   - ê°œì„ ìœ¨: {sequential_time/parallel_time:.1f}ë°°")

        # ì„±ê³µë¥  í™•ì¸
        sequential_success = sum(1 for r in sequential_results if r.get('success', False))
        parallel_success = sum(1 for r in parallel_results if r.get('success', False))

        print(f"\nâœ… ì„±ê³µë¥ :")
        print(f"   - ìˆœì°¨ ì²˜ë¦¬: {sequential_success}/{len(test_urls)} ({sequential_success/len(test_urls)*100:.1f}%)")
        print(f"   - ë³‘ë ¬ ì²˜ë¦¬: {parallel_success}/{len(test_urls)} ({parallel_success/len(test_urls)*100:.1f}%)")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ­ Playwright í™˜ê²½ ì„¤ì • ë° ê¸°ë³¸ ì‚¬ìš©ë²•")
    print("2íšŒì°¨ ê°•ì˜ - ì˜ˆì œ 1")
    print("=" * 60)
    print(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    try:
        await example_basic_setup()
        await example_contextual_crawler()
        await example_multiple_contexts()
        await example_error_handling()
        await example_performance_comparison()

        print("\nğŸ‰ ëª¨ë“  ì˜ˆì œê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    except Exception as e:
        print(f"\nâŒ ì˜ˆì œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

    finally:
        print("\nğŸ“š ë‹¤ìŒ ì˜ˆì œ: python examples/02_context_pages.py")


if __name__ == "__main__":
    # ì´ë²¤íŠ¸ ë£¨í”„ ì„¤ì • (Windows í˜¸í™˜ì„±)
    if hasattr(asyncio, 'WindowsProactorEventLoopPolicy'):
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

    asyncio.run(main())
