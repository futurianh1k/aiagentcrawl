"""
AI ì—ì´ì „íŠ¸ ê¸°ë°˜ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì‹œìŠ¤í…œ - ì‹¤ìŠµ 2
==================================================
ì£¼ì œ: NewsScraper Tool êµ¬í˜„ - Selenium + Firecrawl

ëª©í‘œ:
- Seleniumì„ ì´ìš©í•œ ì•ˆì •ì ì¸ ì›¹ í¬ë¡¤ë§ êµ¬í˜„
- Explicit Waitì„ í†µí•œ Flaky Test ë°©ì§€
- Firecrawl MCPë¥¼ í™œìš©í•œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ
- Toolë¡œ íŒ¨í‚¤ì§•í•˜ì—¬ Agentì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ êµ¬í˜„

í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬:
pip install selenium webdriver-manager requests beautifulsoup4 python-dotenv langchain
"""

import os
import json
import time
import requests
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from langchain.tools import tool
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

@dataclass
class NewsArticle:
    """ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° í´ë˜ìŠ¤"""
    url: str
    title: str
    content: str
    comments: List[Dict[str, Any]]
    published_date: Optional[str] = None
    source: Optional[str] = None

class NewsScraperTool:
    """ë‰´ìŠ¤ ìŠ¤í¬ë ˆì´í¼ Tool í´ë˜ìŠ¤"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.driver = None
        self.firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY", "fc-test-key")

    def setup_driver(self) -> webdriver.Chrome:
        """Chrome WebDriver ì„¤ì • ë° ì´ˆê¸°í™”"""
        chrome_options = Options()
        chrome_options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì°½ ìˆ¨ê¹€
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")

        # ChromeDriver ìë™ ì„¤ì¹˜ ë° ì„¤ì •
        service = Service(ChromeDriverManager().install())

        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.implicitly_wait(10)  # ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ ì„¤ì •

        return driver

    def search_naver_news(self, keyword: str, max_articles: int = 5) -> List[str]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰ í›„ ê¸°ì‚¬ URL ëª©ë¡ ë°˜í™˜"""
        if not self.driver:
            self.driver = self.setup_driver()

        try:
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL
            search_url = f"https://search.naver.com/search.naver?where=news&query={keyword}"
            print(f"ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰: {keyword}")

            self.driver.get(search_url)

            # Explicit Wait: ê²€ìƒ‰ ê²°ê³¼ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            wait = WebDriverWait(self.driver, 15)

            # ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ë“¤ì´ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
            news_links = wait.until(
                EC.presence_of_all_elements_located(
                    (By.CSS_SELECTOR, "a.news_tit")
                )
            )

            # URL ëª©ë¡ ì¶”ì¶œ
            article_urls = []
            for link in news_links[:max_articles]:
                href = link.get_attribute("href")
                if href and "news.naver.com" in href:
                    article_urls.append(href)

            print(f"âœ… {len(article_urls)}ê°œì˜ ê¸°ì‚¬ URL ìˆ˜ì§‘ ì™„ë£Œ")
            return article_urls

        except Exception as e:
            print(f"âŒ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []

    def extract_with_selenium(self, url: str) -> Dict[str, Any]:
        """Seleniumìœ¼ë¡œ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ"""
        if not self.driver:
            self.driver = self.setup_driver()

        try:
            self.driver.get(url)
            wait = WebDriverWait(self.driver, 10)

            # ì œëª© ì¶”ì¶œ
            try:
                title_element = wait.until(
                    EC.presence_of_element_located(
                        (By.CSS_SELECTOR, "#ct > div.media_end_head.go_trans > div.media_end_head_title > h2")
                    )
                )
                title = title_element.text.strip()
            except:
                title = "ì œëª© ì¶”ì¶œ ì‹¤íŒ¨"

            # ë³¸ë¬¸ ì¶”ì¶œ
            try:
                content_element = wait.until(
                    EC.presence_of_element_located(
                        (By.CSS_SELECTOR, "#dic_area")
                    )
                )
                content = content_element.text.strip()
            except:
                content = "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"

            # ëŒ“ê¸€ ì¶”ì¶œ (ë„¤ì´ë²„ ë‰´ìŠ¤ ëŒ“ê¸€ì€ ë™ì  ë¡œë”©ì´ë¯€ë¡œ ê¸°ë³¸ êµ¬í˜„)
            comments = self.extract_comments_basic()

            return {
                "title": title,
                "content": content,
                "comments": comments,
                "extraction_method": "selenium"
            }

        except Exception as e:
            print(f"âŒ Selenium ì¶”ì¶œ ì˜¤ë¥˜ ({url}): {str(e)}")
            return {
                "title": "ì¶”ì¶œ ì‹¤íŒ¨",
                "content": "ì¶”ì¶œ ì‹¤íŒ¨", 
                "comments": [],
                "extraction_method": "selenium",
                "error": str(e)
            }

    def extract_comments_basic(self) -> List[Dict[str, Any]]:
        """ê¸°ë³¸ì ì¸ ëŒ“ê¸€ ì¶”ì¶œ (ë„¤ì´ë²„ ë‰´ìŠ¤ ëŒ“ê¸€ êµ¬ì¡°ì— ë§ì¶° êµ¬í˜„)"""
        comments = []

        try:
            # ëŒ“ê¸€ ì˜ì—­ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            wait = WebDriverWait(self.driver, 5)

            # ëŒ“ê¸€ ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹œë„
            try:
                more_button = wait.until(
                    EC.element_to_be_clickable(
                        (By.CSS_SELECTOR, ".u_cbox_btn_more")
                    )
                )
                more_button.click()
                time.sleep(2)  # ëŒ“ê¸€ ë¡œë”© ëŒ€ê¸°
            except:
                pass  # ë”ë³´ê¸° ë²„íŠ¼ì´ ì—†ì„ ìˆ˜ ìˆìŒ

            # ëŒ“ê¸€ ìš”ì†Œë“¤ ì°¾ê¸°
            comment_elements = self.driver.find_elements(
                By.CSS_SELECTOR, 
                ".u_cbox_comment_box .u_cbox_contents"
            )

            for i, comment_elem in enumerate(comment_elements[:10]):  # ìµœëŒ€ 10ê°œ
                try:
                    text = comment_elem.text.strip()
                    if text:
                        comments.append({
                            "id": f"comment_{i+1}",
                            "text": text,
                            "author": f"ì‚¬ìš©ì{i+1}",  # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì¶”ì¶œ í•„ìš”
                            "timestamp": None  # ì‹¤ì œë¡œëŠ” ì‹œê°„ ì •ë³´ ì¶”ì¶œ í•„ìš”
                        })
                except:
                    continue

        except Exception as e:
            print(f"âš ï¸  ëŒ“ê¸€ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")

        # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ëŒ“ê¸€ (ì‹¤ì œ ëŒ“ê¸€ ì¶”ì¶œì´ ì‹¤íŒ¨í•  ê²½ìš°)
        if not comments:
            comments = [
                {"id": "dummy_1", "text": "ì¢‹ì€ ê¸°ì‚¬ë„¤ìš”.", "author": "ë…ì1", "timestamp": None},
                {"id": "dummy_2", "text": "ì •ë³´ ê°ì‚¬í•©ë‹ˆë‹¤.", "author": "ë…ì2", "timestamp": None},
                {"id": "dummy_3", "text": "ë” ìì„¸í•œ ë‚´ìš©ì´ ê¶ê¸ˆí•©ë‹ˆë‹¤.", "author": "ë…ì3", "timestamp": None}
            ]

        return comments

    def extract_with_firecrawl(self, url: str) -> Dict[str, Any]:
        """Firecrawl APIë¥¼ ì´ìš©í•œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # Firecrawl API ì—”ë“œí¬ì¸íŠ¸
            api_url = "https://api.firecrawl.dev/v0/scrape"

            headers = {
                "Authorization": f"Bearer {self.firecrawl_api_key}",
                "Content-Type": "application/json"
            }

            payload = {
                "url": url,
                "formats": ["markdown", "html"],
                "includeTags": ["title", "article", "p", "h1", "h2", "h3"],
                "excludeTags": ["script", "style", "nav", "footer"],
                "waitFor": 2000  # 2ì´ˆ ëŒ€ê¸°
            }

            print(f"ğŸ”¥ Firecrawl API í˜¸ì¶œ: {url}")
            response = requests.post(api_url, headers=headers, json=payload, timeout=30)

            if response.status_code == 200:
                data = response.json()
                return {
                    "title": data.get("metadata", {}).get("title", "ì œëª© ì—†ìŒ"),
                    "content": data.get("markdown", "ë‚´ìš© ì—†ìŒ"),
                    "comments": [],  # Firecrawlë¡œëŠ” ëŒ“ê¸€ ì¶”ì¶œì´ ì–´ë ¤ì›€
                    "extraction_method": "firecrawl",
                    "success": True
                }
            else:
                print(f"âŒ Firecrawl API ì˜¤ë¥˜: {response.status_code}")
                return None

        except Exception as e:
            print(f"âŒ Firecrawl ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return None

    def scrape_article(self, url: str) -> NewsArticle:
        """ë‹¨ì¼ ê¸°ì‚¬ ìŠ¤í¬ë ˆì´í•‘ (Firecrawl ìš°ì„ , ì‹¤íŒ¨ ì‹œ Selenium ì‚¬ìš©)"""
        print(f"\nğŸ“° ê¸°ì‚¬ ìŠ¤í¬ë ˆì´í•‘ ì‹œì‘: {url}")

        # 1ì°¨: Firecrawl ì‹œë„
        firecrawl_result = self.extract_with_firecrawl(url)

        if firecrawl_result and firecrawl_result.get("success"):
            print("âœ… Firecrawlë¡œ ì¶”ì¶œ ì„±ê³µ")
            return NewsArticle(
                url=url,
                title=firecrawl_result["title"],
                content=firecrawl_result["content"],
                comments=firecrawl_result["comments"],
                source="firecrawl"
            )

        # 2ì°¨: Selenium ì‹œë„ 
        print("ğŸ”„ Firecrawl ì‹¤íŒ¨, Seleniumìœ¼ë¡œ ì¬ì‹œë„...")
        selenium_result = self.extract_with_selenium(url)

        return NewsArticle(
            url=url,
            title=selenium_result["title"],
            content=selenium_result["content"],
            comments=selenium_result["comments"],
            source="selenium"
        )

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.driver:
            self.driver.quit()
            self.driver = None

    @tool
    def scrape_news(keyword: str, max_articles: int = 3) -> List[Dict[str, Any]]:
        """ë‰´ìŠ¤ ìŠ¤í¬ë ˆì´í•‘ Tool í•¨ìˆ˜

        Args:
            keyword (str): ê²€ìƒ‰í•  í‚¤ì›Œë“œ
            max_articles (int): ìµœëŒ€ ìˆ˜ì§‘í•  ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ê°’: 3)

        Returns:
            List[Dict]: ìŠ¤í¬ë ˆì´í•‘ëœ ê¸°ì‚¬ë“¤ì˜ ì •ë³´
        """
        scraper = NewsScraperTool()

        try:
            # 1ë‹¨ê³„: ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ê¸°ì‚¬ URL ê²€ìƒ‰
            article_urls = scraper.search_naver_news(keyword, max_articles)

            if not article_urls:
                return [{
                    "error": f"'{keyword}' í‚¤ì›Œë“œë¡œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "keyword": keyword
                }]

            # 2ë‹¨ê³„: ê° ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            scraped_articles = []

            for i, url in enumerate(article_urls, 1):
                print(f"\n[{i}/{len(article_urls)}] ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘...")

                article = scraper.scrape_article(url)

                scraped_articles.append({
                    "url": article.url,
                    "title": article.title,
                    "content": article.content[:500] + "..." if len(article.content) > 500 else article.content,
                    "comments": article.comments,
                    "source": article.source,
                    "keyword": keyword
                })

                time.sleep(1)  # API ë¶€í•˜ ë°©ì§€

            return scraped_articles

        except Exception as e:
            return [{
                "error": f"ë‰´ìŠ¤ ìŠ¤í¬ë ˆì´í•‘ ì¤‘ ì˜¤ë¥˜: {str(e)}",
                "keyword": keyword
            }]

        finally:
            scraper.cleanup()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ NewsScraper Tool ì‹¤ìŠµ ì‹œì‘")
    print("=" * 60)

    # í…ŒìŠ¤íŠ¸ í‚¤ì›Œë“œë“¤
    test_keywords = ["AI", "ì‚¼ì„±ì „ì", "ë¶€ë™ì‚°"]

    for keyword in test_keywords:
        print(f"\nğŸ” í‚¤ì›Œë“œ í…ŒìŠ¤íŠ¸: {keyword}")
        print("-" * 40)

        # Tool í•¨ìˆ˜ í˜¸ì¶œ
        result = NewsScraperTool.scrape_news(keyword, max_articles=2)

        print(f"âœ… ìˆ˜ì§‘ ê²°ê³¼: {len(result)}ê°œ ê¸°ì‚¬")

        for i, article in enumerate(result, 1):
            if "error" in article:
                print(f"âŒ ì˜¤ë¥˜: {article['error']}")
            else:
                print(f"\n[ê¸°ì‚¬ {i}]")
                print(f"ì œëª©: {article['title'][:50]}...")
                print(f"URL: {article['url']}")
                print(f"ëŒ“ê¸€ ìˆ˜: {len(article['comments'])}ê°œ")
                print(f"ì¶”ì¶œ ë°©ë²•: {article['source']}")

    print("\nğŸ¯ ì£¼ìš” í•™ìŠµ í¬ì¸íŠ¸:")
    print("1. Selenium WebDriver ì„¤ì • ë° Explicit Wait ì‚¬ìš©")
    print("2. CSS Selectorë¥¼ ì´ìš©í•œ ì•ˆì •ì ì¸ ìš”ì†Œ ì„ íƒ")
    print("3. Firecrawl APIë¥¼ í†µí•œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ")
    print("4. Fallback ë©”ì»¤ë‹ˆì¦˜ (Firecrawl ì‹¤íŒ¨ ì‹œ Selenium ì‚¬ìš©)")
    print("5. @tool ë°ì½”ë ˆì´í„°ë¡œ Agentì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ Toolë¡œ ë³€í™˜")

    print("\nâš ï¸  ì£¼ì˜ì‚¬í•­:")
    print("- FIRECRAWL_API_KEY í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í•„ìš”")
    print("- Chrome ë¸Œë¼ìš°ì € ë° ChromeDriver í•„ìš”")
    print("- ë„¤íŠ¸ì›Œí¬ ìƒíƒœì— ë”°ë¼ íƒ€ì„ì•„ì›ƒ ì¡°ì • í•„ìš”")
    print("- robots.txt ë° ì‚¬ì´íŠ¸ ì •ì±… ì¤€ìˆ˜ í•„ìš”")

if __name__ == "__main__":
    main()
