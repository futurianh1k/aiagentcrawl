"""
3íšŒì°¨ ì‹¤ìŠµ 06: DataAnalyzer Tool ì™„ì „ êµ¬í˜„
í˜ì´ì§€ 12 - í”„ë¡œë•ì…˜ê¸‰ ê°ì„± ë¶„ì„ Tool

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ DataAnalyzer Toolì„ êµ¬í˜„í•©ë‹ˆë‹¤.
- ì¬ì‹œë„(Retry) ë¡œì§
- ë°°ì¹˜ ë¶„ì„ (Batch API)
- ìºì‹± ë° ì¤‘ë³µ ë°©ì§€
- ì—ëŸ¬ í•¸ë“¤ë§ ë° ë¡œê¹…
"""

import os
import logging
import hashlib
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta

from openai import OpenAI
from pydantic import BaseModel, Field
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SentimentResult(BaseModel):
    """ê°ì„± ë¶„ì„ ê²°ê³¼ ëª¨ë¸"""
    sentiment: str = Field(description="ê°ì„± ë¶„ë¥˜")
    confidence: float = Field(ge=0.0, le=1.0, description="ì‹ ë¢°ë„")
    reason: str = Field(description="ë¶„ì„ ê·¼ê±°")
    keywords: List[str] = Field(description="í•µì‹¬ í‚¤ì›Œë“œ")
    processing_time: float = Field(description="ì²˜ë¦¬ ì‹œê°„(ì´ˆ)")
    timestamp: str = Field(description="ë¶„ì„ ì‹œê°")

@dataclass
class CacheEntry:
    """ìºì‹œ ì—”íŠ¸ë¦¬"""
    result: SentimentResult
    created_at: datetime
    ttl_hours: int = 24

class DataAnalyzer:
    """í”„ë¡œë•ì…˜ê¸‰ ê°ì„± ë¶„ì„ Tool"""

    def __init__(self, api_key: str, enable_cache: bool = True, cache_ttl_hours: int = 24):
        self.client = OpenAI(api_key=api_key)
        self.enable_cache = enable_cache
        self.cache_ttl_hours = cache_ttl_hours
        self.cache: Dict[str, CacheEntry] = {}

        # í†µê³„ ì¶”ì 
        self.stats = {
            "total_requests": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "api_errors": 0,
            "retries": 0
        }

        logger.info("DataAnalyzer ì´ˆê¸°í™” ì™„ë£Œ")

    def _generate_cache_key(self, comment: str, model: str = "gpt-4") -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        content = f"{comment}:{model}".encode('utf-8')
        return hashlib.md5(content).hexdigest()

    def _is_cache_valid(self, entry: CacheEntry) -> bool:
        """ìºì‹œ ìœ íš¨ì„± ê²€ì¦"""
        age = datetime.now() - entry.created_at
        return age < timedelta(hours=entry.ttl_hours)

    def _get_from_cache(self, cache_key: str) -> Optional[SentimentResult]:
        """ìºì‹œì—ì„œ ê²°ê³¼ ì¡°íšŒ"""
        if not self.enable_cache or cache_key not in self.cache:
            self.stats["cache_misses"] += 1
            return None

        entry = self.cache[cache_key]
        if self._is_cache_valid(entry):
            self.stats["cache_hits"] += 1
            logger.debug(f"ìºì‹œ íˆíŠ¸: {cache_key[:8]}...")
            return entry.result
        else:
            # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
            del self.cache[cache_key]
            self.stats["cache_misses"] += 1
            return None

    def _save_to_cache(self, cache_key: str, result: SentimentResult):
        """ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥"""
        if self.enable_cache:
            entry = CacheEntry(
                result=result,
                created_at=datetime.now(),
                ttl_hours=self.cache_ttl_hours
            )
            self.cache[cache_key] = entry
            logger.debug(f"ìºì‹œ ì €ì¥: {cache_key[:8]}...")

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        retry=retry_if_exception_type((Exception,))
    )
    def _call_openai_api(self, comment: str, model: str = "gpt-4") -> Dict[str, Any]:
        """OpenAI API í˜¸ì¶œ (ì¬ì‹œë„ í¬í•¨)"""
        self.stats["retries"] += 1

        system_prompt = """ë‹¹ì‹ ì€ ì „ë¬¸ ë‰´ìŠ¤ ëŒ“ê¸€ ê°ì„± ë¶„ì„ê°€ì…ë‹ˆë‹¤.
        ì£¼ì–´ì§„ ëŒ“ê¸€ì„ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.

        ë¶„ë¥˜ ê¸°ì¤€:
        - ê¸ì •: ì§€ì§€, ì¹­ì°¬, ê¸°ëŒ€ê°, ë§Œì¡±
        - ë¶€ì •: ë¹„íŒ, ë¶„ë…¸, ì‹¤ë§, ìš°ë ¤
        - ì¤‘ë¦½: ì‚¬ì‹¤ ì „ë‹¬, ì§ˆë¬¸, ê· í˜• ì˜ê²¬

        ì‘ë‹µ í˜•ì‹:
        {"sentiment": "ê¸ì •|ë¶€ì •|ì¤‘ë¦½", "confidence": 0.0-1.0, "reason": "ê·¼ê±°", "keywords": ["í‚¤ì›Œë“œ"]}"""

        try:
            start_time = datetime.now()

            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"ëŒ“ê¸€: {comment}"}
                ],
                temperature=0.3,
                max_tokens=300
            )

            processing_time = (datetime.now() - start_time).total_seconds()

            # JSON íŒŒì‹±
            content = response.choices[0].message.content
            if '{' in content and '}' in content:
                import json
                start_idx = content.find('{')
                end_idx = content.rfind('}') + 1
                json_str = content[start_idx:end_idx]
                result = json.loads(json_str)

                # ê²°ê³¼ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
                result["processing_time"] = processing_time
                result["timestamp"] = datetime.now().isoformat()

                return result
            else:
                raise ValueError("JSON í˜•ì‹ì˜ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")

        except Exception as e:
            self.stats["api_errors"] += 1
            logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise

    def analyze_sentiment(self, comment: str, model: str = "gpt-4") -> SentimentResult:
        """ë‹¨ì¼ ëŒ“ê¸€ ê°ì„± ë¶„ì„"""
        self.stats["total_requests"] += 1

        # ë¹ˆ ëŒ“ê¸€ ì²´í¬
        if not comment or not comment.strip():
            return SentimentResult(
                sentiment="ì¤‘ë¦½",
                confidence=0.0,
                reason="ë¹ˆ ëŒ“ê¸€",
                keywords=[],
                processing_time=0.0,
                timestamp=datetime.now().isoformat()
            )

        # ìºì‹œ í™•ì¸
        cache_key = self._generate_cache_key(comment, model)
        cached_result = self._get_from_cache(cache_key)

        if cached_result:
            return cached_result

        try:
            # API í˜¸ì¶œ
            raw_result = self._call_openai_api(comment, model)

            # Pydantic ëª¨ë¸ë¡œ ê²€ì¦
            result = SentimentResult(**raw_result)

            # ìºì‹œì— ì €ì¥
            self._save_to_cache(cache_key, result)

            logger.info(f"ê°ì„± ë¶„ì„ ì™„ë£Œ: {result.sentiment} ({result.confidence:.2f})")
            return result

        except Exception as e:
            logger.error(f"ê°ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")

            # í´ë°± ê²°ê³¼ ë°˜í™˜
            return SentimentResult(
                sentiment="ì¤‘ë¦½",
                confidence=0.0,
                reason=f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}",
                keywords=[],
                processing_time=0.0,
                timestamp=datetime.now().isoformat()
            )

    def batch_analyze(self, comments: List[str], model: str = "gpt-4", 
                     batch_size: int = 10) -> List[SentimentResult]:
        """ë°°ì¹˜ ê°ì„± ë¶„ì„"""
        logger.info(f"ë°°ì¹˜ ë¶„ì„ ì‹œì‘: {len(comments)}ê°œ ëŒ“ê¸€")

        results = []

        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, len(comments), batch_size):
            batch = comments[i:i + batch_size]
            logger.info(f"ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ)")

            batch_results = []
            for comment in batch:
                result = self.analyze_sentiment(comment, model)
                batch_results.append(result)

            results.extend(batch_results)

        logger.info(f"ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
        return results

    def get_statistics(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´ ì¡°íšŒ"""
        cache_hit_rate = 0.0
        if self.stats["total_requests"] > 0:
            cache_hit_rate = self.stats["cache_hits"] / (self.stats["cache_hits"] + self.stats["cache_misses"])

        return {
            "total_requests": self.stats["total_requests"],
            "cache_hits": self.stats["cache_hits"],
            "cache_misses": self.stats["cache_misses"],
            "cache_hit_rate": cache_hit_rate,
            "api_errors": self.stats["api_errors"],
            "retries": self.stats["retries"],
            "cache_size": len(self.cache)
        }

    def clear_cache(self):
        """ìºì‹œ í´ë¦¬ì–´"""
        self.cache.clear()
        logger.info("ìºì‹œê°€ í´ë¦¬ì–´ë˜ì—ˆìŠµë‹ˆë‹¤")

if __name__ == "__main__":
    print("ğŸš€ DataAnalyzer Tool ì™„ì „ êµ¬í˜„ ì‹¤ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    print("=" * 70)

    try:
        # 1. DataAnalyzer ì´ˆê¸°í™”
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”")

        analyzer = DataAnalyzer(api_key=api_key, enable_cache=True)

        # 2. ë‹¨ì¼ ë¶„ì„ í…ŒìŠ¤íŠ¸
        print("\n1ï¸âƒ£ ë‹¨ì¼ ëŒ“ê¸€ ë¶„ì„")
        print("-" * 40)

        test_comment = "ì´ ìƒˆë¡œìš´ ì •ì±…ì€ ì •ë§ í›Œë¥­í•©ë‹ˆë‹¤! ì ê·¹ ì§€ì§€í•©ë‹ˆë‹¤."
        result = analyzer.analyze_sentiment(test_comment)

        print(f"ğŸ“ ëŒ“ê¸€: {test_comment}")
        print(f"ğŸ¯ ê²°ê³¼: {result.sentiment} (ì‹ ë¢°ë„: {result.confidence:.2f})")
        print(f"ğŸ“Š ê·¼ê±°: {result.reason}")
        print(f"ğŸ”‘ í‚¤ì›Œë“œ: {result.keywords}")
        print(f"â±ï¸  ì²˜ë¦¬ì‹œê°„: {result.processing_time:.3f}ì´ˆ")

        # 3. ìºì‹œ í…ŒìŠ¤íŠ¸ (ë™ì¼ ëŒ“ê¸€ ì¬ë¶„ì„)
        print("\n2ï¸âƒ£ ìºì‹œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
        print("-" * 40)

        print("ë™ì¼ ëŒ“ê¸€ ì¬ë¶„ì„ (ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸°)...")
        cached_result = analyzer.analyze_sentiment(test_comment)
        print(f"ğŸ¯ ìºì‹œëœ ê²°ê³¼: {cached_result.sentiment}")

        # 4. ë°°ì¹˜ ë¶„ì„ í…ŒìŠ¤íŠ¸
        print("\n3ï¸âƒ£ ë°°ì¹˜ ë¶„ì„ í…ŒìŠ¤íŠ¸")
        print("-" * 40)

        test_comments = [
            "ì •ë§ ì¢‹ì€ ì•„ì´ë””ì–´ë„¤ìš”!",
            "ì´ê±´ ì™„ì „ ìµœì•…ì´ì—ìš”.",
            "ë‚´ì¼ ë‚ ì”¨ëŠ” ì–´ë–¨ê¹Œìš”?",
            "ìƒˆë¡œìš´ ê¸°ìˆ ì´ ê¸°ëŒ€ë©ë‹ˆë‹¤.",
            "ë¬¸ì œê°€ ë„ˆë¬´ ë§ì•„ìš”."
        ]

        batch_results = analyzer.batch_analyze(test_comments, batch_size=2)

        for i, (comment, result) in enumerate(zip(test_comments, batch_results), 1):
            print(f"{i}. {comment[:20]}... â†’ {result.sentiment} ({result.confidence:.2f})")

        # 5. í†µê³„ ì •ë³´ ì¶œë ¥
        print("\n4ï¸âƒ£ ì„±ëŠ¥ í†µê³„")
        print("-" * 40)

        stats = analyzer.get_statistics()
        for key, value in stats.items():
            if key == "cache_hit_rate":
                print(f"{key}: {value:.1%}")
            else:
                print(f"{key}: {value}")

        print("\nâœ… DataAnalyzer Tool ì‹¤ìŠµ ì™„ë£Œ!")
        print("\nğŸ’¡ í•µì‹¬ ê¸°ëŠ¥:")
        print("   1. ìë™ ì¬ì‹œë„ (Exponential Backoff)")
        print("   2. ì¸í…”ë¦¬ì „íŠ¸ ìºì‹± (ì¤‘ë³µ ë°©ì§€)")
        print("   3. ë°°ì¹˜ ì²˜ë¦¬ (íš¨ìœ¨ì„± í–¥ìƒ)")
        print("   4. ì—ëŸ¬ ë³µêµ¬ (í´ë°± ê²°ê³¼)")
        print("   5. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (í†µê³„ ìˆ˜ì§‘)")
        print("\nğŸ“š ë‹¤ìŒ ë‹¨ê³„:")
        print("   - 07_langgraph_sequential.py: Multi-Agent ì›Œí¬í”Œë¡œìš°")
        print("   - 08_langgraph_conditional.py: ì¡°ê±´ë¶€ ë¼ìš°íŒ…")

    except Exception as e:
        print(f"âŒ ì‹¤ìŠµ ì˜¤ë¥˜: {e}")
        print("\nğŸ”§ í•´ê²° ë°©ë²•:")
        print("   1. OpenAI API í‚¤ í™•ì¸")
        print("   2. pip install tenacity pydantic")
        print("   3. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸")
