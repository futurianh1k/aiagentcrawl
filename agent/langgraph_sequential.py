"""
3íšŒì°¨ ì‹¤ìŠµ 07: LangGraph Sequential ì›Œí¬í”Œë¡œìš°
í˜ì´ì§€ 20 - Crawler â†’ Analyzer â†’ Reporter

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” LangGraphë¥¼ ì´ìš©í•œ ìˆœì°¨ì  Multi-Agent ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
- StateGraph ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬
- Agent ê°„ ë°ì´í„° íë¦„
- ìˆœì°¨ ì‹¤í–‰ íŒ¨í„´
- ì—ëŸ¬ í•¸ë“¤ë§ ë° ë¡œê¹…
"""

import os
import json
from typing import TypedDict, List, Dict, Any, Optional
from datetime import datetime

from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class AgentState(TypedDict):
    """Multi-Agent ê³µìœ  ìƒíƒœ"""
    # ì…ë ¥ ë°ì´í„°
    keyword: str
    max_articles: int

    # Crawler Agent ê²°ê³¼
    articles: List[Dict[str, Any]]
    crawler_status: str
    crawler_timestamp: str

    # Analyzer Agent ê²°ê³¼  
    analysis_results: List[Dict[str, Any]]
    analyzer_status: str
    analyzer_timestamp: str

    # Reporter Agent ê²°ê³¼
    final_report: str
    summary_stats: Dict[str, Any]
    reporter_status: str
    reporter_timestamp: str

    # ë©”íƒ€ë°ì´í„°
    workflow_id: str
    total_processing_time: float
    errors: List[str]

def setup_llm():
    """LLM ì´ˆê¸°í™”"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEYê°€ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    return ChatOpenAI(
        model="gpt-4",
        temperature=0.3,
        api_key=api_key
    )

def crawler_agent(state: AgentState) -> AgentState:
    """ë‰´ìŠ¤ í¬ë¡¤ë§ Agent (ëª¨ì˜)"""
    print(f"ğŸ•·ï¸ Crawler Agent ì‹¤í–‰: '{state['keyword']}' ê²€ìƒ‰")

    try:
        start_time = datetime.now()

        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì—¬ê¸°ì„œ Selenium, Playwright, Firecrawl ì‚¬ìš©
        mock_articles = [
            {
                "title": f"{state['keyword']} ê´€ë ¨ ìµœì‹  ë™í–¥",
                "url": "https://news1.example.com/article1",
                "summary": "ê¸ì •ì ì¸ ì „ë§ì„ ì œì‹œí•˜ëŠ” ê¸°ì‚¬ì…ë‹ˆë‹¤.",
                "comments": [
                    "ì •ë§ ì¢‹ì€ ì†Œì‹ì´ë„¤ìš”!",
                    "ë“œë””ì–´ ê°œì„ ë˜ëŠ”êµ°ìš”.", 
                    "ê¸°ëŒ€í•˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤."
                ],
                "crawl_timestamp": datetime.now().isoformat()
            },
            {
                "title": f"{state['keyword']} ë…¼ë€ í™•ì‚°",
                "url": "https://news2.example.com/article2", 
                "summary": "ì¼ë¶€ ë¶€ì •ì  ì˜ê²¬ì´ ì œê¸°ë˜ê³  ìˆìŠµë‹ˆë‹¤.",
                "comments": [
                    "ì´ê±´ ë¬¸ì œê°€ ìˆì–´ ë³´ì—¬ìš”.",
                    "ì™œ ì´ëŸ° ê²°ì •ì„ í–ˆì„ê¹Œìš”?",
                    "ì‹¤ë§ìŠ¤ëŸ½ë„¤ìš”."
                ],
                "crawl_timestamp": datetime.now().isoformat()
            },
            {
                "title": f"{state['keyword']} ì¤‘ë¦½ì  ë¶„ì„ ë¦¬í¬íŠ¸",
                "url": "https://news3.example.com/article3",
                "summary": "ê°ê´€ì ì¸ ë¶„ì„ì„ ì œê³µí•˜ëŠ” ê¸°ì‚¬ì…ë‹ˆë‹¤.", 
                "comments": [
                    "ìì„¸í•œ ë¶„ì„ ê°ì‚¬í•©ë‹ˆë‹¤.",
                    "ë” ì§€ì¼œë´ì•¼ í•  ê²ƒ ê°™ë„¤ìš”.",
                    "ê· í˜•ì¡íŒ ì‹œê°ì´êµ°ìš”."
                ],
                "crawl_timestamp": datetime.now().isoformat()
            }
        ]

        # ìš”ì²­ëœ ìˆ˜ë§Œí¼ë§Œ ë°˜í™˜
        articles = mock_articles[:state['max_articles']]

        processing_time = (datetime.now() - start_time).total_seconds()

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        state["articles"] = articles
        state["crawler_status"] = "completed"
        state["crawler_timestamp"] = datetime.now().isoformat()

        print(f"âœ… Crawler ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ({processing_time:.2f}ì´ˆ)")

        return state

    except Exception as e:
        print(f"âŒ Crawler ì˜¤ë¥˜: {e}")
        state["crawler_status"] = "error"
        state["errors"].append(f"Crawler: {str(e)}")
        state["articles"] = []
        return state

def analyzer_agent(state: AgentState) -> AgentState:
    """ê°ì„± ë¶„ì„ Agent"""
    print("ğŸ” Analyzer Agent ì‹¤í–‰: ëŒ“ê¸€ ê°ì„± ë¶„ì„")

    try:
        start_time = datetime.now()
        llm = setup_llm()

        analysis_results = []

        for article in state["articles"]:
            article_analysis = {
                "article_title": article["title"],
                "article_url": article["url"],
                "comment_analyses": []
            }

            print(f"  ğŸ“° ë¶„ì„ ì¤‘: {article['title'][:30]}...")

            # ê° ëŒ“ê¸€ ë¶„ì„
            for comment in article["comments"]:
                prompt = f"""ë‹¤ìŒ ëŒ“ê¸€ì˜ ê°ì„±ì„ ë¶„ì„í•˜ê³  JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:

                ëŒ“ê¸€: "{comment}"

                ì‘ë‹µ í˜•ì‹:
                {{
                    "sentiment": "ê¸ì •|ë¶€ì •|ì¤‘ë¦½",
                    "confidence": 0.0-1.0,
                    "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"]
                }}"""

                try:
                    response = llm.invoke([HumanMessage(content=prompt)])
                    content = response.content

                    # JSON íŒŒì‹±
                    if '{' in content and '}' in content:
                        start_idx = content.find('{')
                        end_idx = content.rfind('}') + 1
                        json_str = content[start_idx:end_idx]
                        sentiment_data = json.loads(json_str)

                        comment_analysis = {
                            "comment": comment,
                            "sentiment": sentiment_data.get("sentiment", "ì¤‘ë¦½"),
                            "confidence": sentiment_data.get("confidence", 0.5),
                            "keywords": sentiment_data.get("keywords", [])
                        }
                    else:
                        # í´ë°±
                        comment_analysis = {
                            "comment": comment,
                            "sentiment": "ì¤‘ë¦½",
                            "confidence": 0.0,
                            "keywords": []
                        }

                    article_analysis["comment_analyses"].append(comment_analysis)

                except Exception as e:
                    print(f"    âš ï¸ ëŒ“ê¸€ ë¶„ì„ ì‹¤íŒ¨: {e}")
                    # ì—ëŸ¬ ì‹œ ê¸°ë³¸ê°’
                    article_analysis["comment_analyses"].append({
                        "comment": comment,
                        "sentiment": "ì¤‘ë¦½", 
                        "confidence": 0.0,
                        "keywords": [],
                        "error": str(e)
                    })

            analysis_results.append(article_analysis)

        processing_time = (datetime.now() - start_time).total_seconds()

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        state["analysis_results"] = analysis_results
        state["analyzer_status"] = "completed"
        state["analyzer_timestamp"] = datetime.now().isoformat()

        total_comments = sum(len(article["comments"]) for article in state["articles"])
        print(f"âœ… Analyzer ì™„ë£Œ: {total_comments}ê°œ ëŒ“ê¸€ ë¶„ì„ ({processing_time:.2f}ì´ˆ)")

        return state

    except Exception as e:
        print(f"âŒ Analyzer ì˜¤ë¥˜: {e}")
        state["analyzer_status"] = "error"
        state["errors"].append(f"Analyzer: {str(e)}")
        state["analysis_results"] = []
        return state

def reporter_agent(state: AgentState) -> AgentState:
    """ë¦¬í¬íŠ¸ ìƒì„± Agent"""
    print("ğŸ“Š Reporter Agent ì‹¤í–‰: ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±")

    try:
        start_time = datetime.now()

        # í†µê³„ ê³„ì‚°
        all_sentiments = []
        all_confidences = []

        for article_analysis in state["analysis_results"]:
            for comment_analysis in article_analysis["comment_analyses"]:
                all_sentiments.append(comment_analysis["sentiment"])
                all_confidences.append(comment_analysis["confidence"])

        # ê°ì„± ë¶„í¬ ê³„ì‚°
        sentiment_counts = {}
        for sentiment in all_sentiments:
            sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1

        total_comments = len(all_sentiments)
        sentiment_percentages = {}
        if total_comments > 0:
            for sentiment, count in sentiment_counts.items():
                sentiment_percentages[sentiment] = (count / total_comments) * 100

        avg_confidence = sum(all_confidences) / len(all_confidences) if all_confidences else 0.0

        # ì „ì²´ ê²½í–¥ íŒë‹¨
        positive_pct = sentiment_percentages.get("ê¸ì •", 0)
        negative_pct = sentiment_percentages.get("ë¶€ì •", 0)
        neutral_pct = sentiment_percentages.get("ì¤‘ë¦½", 0)

        if positive_pct > negative_pct and positive_pct > neutral_pct:
            overall_trend = "ê¸ì •ì "
        elif negative_pct > positive_pct and negative_pct > neutral_pct:
            overall_trend = "ë¶€ì •ì "
        else:
            overall_trend = "ì¤‘ë¦½ì "

        # ìš”ì•½ í†µê³„
        summary_stats = {
            "total_articles": len(state["articles"]),
            "total_comments": total_comments,
            "sentiment_distribution": sentiment_percentages,
            "average_confidence": avg_confidence,
            "overall_trend": overall_trend
        }

        # ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
        report = f"""
ğŸ¯ {state['keyword']} ê°ì„± ë¶„ì„ ë¦¬í¬íŠ¸
{'=' * 50}

ğŸ“Š ë¶„ì„ ê°œìš”:
- ë¶„ì„ ê¸°ì‚¬ ìˆ˜: {summary_stats['total_articles']}ê°œ
- ë¶„ì„ ëŒ“ê¸€ ìˆ˜: {summary_stats['total_comments']}ê°œ
- ì „ì²´ ê²½í–¥: {overall_trend}
- í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.2f}

ğŸ“ˆ ê°ì„± ë¶„í¬:
- ê¸ì •: {positive_pct:.1f}%
- ë¶€ì •: {negative_pct:.1f}%  
- ì¤‘ë¦½: {neutral_pct:.1f}%

ğŸ“ ìƒì„¸ ë¶„ì„:
"""

        for i, article_analysis in enumerate(state["analysis_results"], 1):
            report += f"\n{i}. {article_analysis['article_title']}\n"

            article_sentiments = [ca["sentiment"] for ca in article_analysis["comment_analyses"]]
            pos = article_sentiments.count("ê¸ì •")
            neg = article_sentiments.count("ë¶€ì •") 
            neu = article_sentiments.count("ì¤‘ë¦½")

            report += f"   ëŒ“ê¸€ ë°˜ì‘: ê¸ì • {pos}ê°œ, ë¶€ì • {neg}ê°œ, ì¤‘ë¦½ {neu}ê°œ\n"

        report += f"\nâ±ï¸ ì²˜ë¦¬ ì‹œê°„: {datetime.now().isoformat()}"

        processing_time = (datetime.now() - start_time).total_seconds()

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        state["final_report"] = report
        state["summary_stats"] = summary_stats
        state["reporter_status"] = "completed"
        state["reporter_timestamp"] = datetime.now().isoformat()

        print(f"âœ… Reporter ì™„ë£Œ: ë¦¬í¬íŠ¸ ìƒì„± ({processing_time:.2f}ì´ˆ)")

        return state

    except Exception as e:
        print(f"âŒ Reporter ì˜¤ë¥˜: {e}")
        state["reporter_status"] = "error"
        state["errors"].append(f"Reporter: {str(e)}")
        state["final_report"] = f"ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}"
        state["summary_stats"] = {}
        return state

def create_workflow() -> StateGraph:
    """LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±"""

    # StateGraph ì´ˆê¸°í™”
    workflow = StateGraph(AgentState)

    # Agent ë…¸ë“œ ì¶”ê°€
    workflow.add_node("crawler", crawler_agent)
    workflow.add_node("analyzer", analyzer_agent) 
    workflow.add_node("reporter", reporter_agent)

    # ìˆœì°¨ì  íë¦„ ì •ì˜
    workflow.add_edge("crawler", "analyzer")
    workflow.add_edge("analyzer", "reporter")
    workflow.add_edge("reporter", END)

    # ì‹œì‘ì  ì„¤ì •
    workflow.set_entry_point("crawler")

    return workflow.compile()

if __name__ == "__main__":
    print("ğŸš€ LangGraph Sequential ì›Œí¬í”Œë¡œìš° ì‹¤ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    print("=" * 70)

    try:
        # 1. ì›Œí¬í”Œë¡œìš° ìƒì„±
        app = create_workflow()
        print("âœ… LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„± ì™„ë£Œ")

        # 2. ì´ˆê¸° ìƒíƒœ ì„¤ì •
        initial_state: AgentState = {
            "keyword": "ì‚¼ì„±ì „ì",
            "max_articles": 3,
            "articles": [],
            "crawler_status": "pending",
            "crawler_timestamp": "",
            "analysis_results": [],
            "analyzer_status": "pending", 
            "analyzer_timestamp": "",
            "final_report": "",
            "summary_stats": {},
            "reporter_status": "pending",
            "reporter_timestamp": "",
            "workflow_id": f"wf_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "total_processing_time": 0.0,
            "errors": []
        }

        print(f"\nğŸ¯ ë¶„ì„ í‚¤ì›Œë“œ: {initial_state['keyword']}")
        print(f"ğŸ“Š ìµœëŒ€ ê¸°ì‚¬ ìˆ˜: {initial_state['max_articles']}ê°œ")
        print(f"ğŸ†” ì›Œí¬í”Œë¡œìš° ID: {initial_state['workflow_id']}")

        # 3. ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        print("\nğŸ”„ Multi-Agent ì›Œí¬í”Œë¡œìš° ì‹¤í–‰")
        print("-" * 50)

        overall_start = datetime.now()

        # LangGraph ì‹¤í–‰
        final_state = app.invoke(initial_state)

        overall_time = (datetime.now() - overall_start).total_seconds()

        print("\n" + "=" * 70)
        print("ğŸ‰ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ!")
        print("=" * 70)

        # 4. ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“‹ ì‹¤í–‰ ìƒíƒœ:")
        print(f"   ğŸ•·ï¸ Crawler: {final_state['crawler_status']}")
        print(f"   ğŸ” Analyzer: {final_state['analyzer_status']}")  
        print(f"   ğŸ“Š Reporter: {final_state['reporter_status']}")
        print(f"   â±ï¸ ì´ ì²˜ë¦¬ì‹œê°„: {overall_time:.2f}ì´ˆ")

        if final_state["errors"]:
            print(f"\nâš ï¸ ì˜¤ë¥˜ ëª©ë¡:")
            for error in final_state["errors"]:
                print(f"   - {error}")

        # 5. ìµœì¢… ë¦¬í¬íŠ¸ ì¶œë ¥
        if final_state["final_report"]:
            print(f"\n{final_state['final_report']}")

        # 6. ìš”ì•½ í†µê³„
        if final_state["summary_stats"]:
            stats = final_state["summary_stats"]
            print(f"\nğŸ“ˆ í•µì‹¬ í†µê³„:")
            print(f"   ì „ì²´ ê²½í–¥: {stats.get('overall_trend', 'N/A')}")
            print(f"   ì‹ ë¢°ë„: {stats.get('average_confidence', 0):.2f}")

            dist = stats.get('sentiment_distribution', {})
            for sentiment, pct in dist.items():
                print(f"   {sentiment}: {pct:.1f}%")

        print("\nâœ… LangGraph Sequential ì‹¤ìŠµ ì™„ë£Œ!")
        print("\nğŸ’¡ í•µì‹¬ ê°œë…:")
        print("   1. StateGraph: ìƒíƒœ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°")
        print("   2. Sequential Flow: A â†’ B â†’ C ìˆœì°¨ ì‹¤í–‰")
        print("   3. State Sharing: Agent ê°„ ë°ì´í„° ê³µìœ ")
        print("   4. Error Handling: ê°œë³„ Agent ì˜¤ë¥˜ ì²˜ë¦¬")
        print("\nğŸ“š ë‹¤ìŒ ë‹¨ê³„:")
        print("   - 08_langgraph_conditional.py: ì¡°ê±´ë¶€ ë¼ìš°íŒ…")
        print("   - 09_langchain_memory.py: ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬")

    except Exception as e:
        print(f"âŒ ì‹¤ìŠµ ì˜¤ë¥˜: {e}")
        print("\nğŸ”§ í•´ê²° ë°©ë²•:")
        print("   1. OpenAI API í‚¤ í™•ì¸")
        print("   2. pip install langgraph langchain-openai")
        print("   3. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸")
