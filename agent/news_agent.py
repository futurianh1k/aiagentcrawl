"""
News Analysis Agent

ë‰´ìŠ¤ ê°ì„± ë¶„ì„ì„ ìœ„í•œ í†µí•© AI Agent
ë„¤ì´ë²„ ë‰´ìŠ¤ì™€ êµ¬ê¸€ ë‰´ìŠ¤ë¥¼ ì§€ì›í•˜ë©°, ì‹¤ì œ Toolsë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

import json
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime

try:
    from langchain.agents import initialize_agent, AgentType
    from langchain.llms import OpenAI
    from langchain.memory import ConversationBufferMemory
    AGENT_AVAILABLE = True
except ImportError:
    AGENT_AVAILABLE = False

from common.config import get_config
from common.utils import safe_log, validate_input
from agent.tools import scrape_news, analyze_sentiment, analyze_news_trend


class NewsAnalysisAgent:
    """ë‰´ìŠ¤ ê°ì„± ë¶„ì„ì„ ìœ„í•œ í†µí•© AI Agent"""

    def __init__(self, api_key: Optional[str] = None):
        """
        Agent ì´ˆê¸°í™”

        Args:
            api_key: OpenAI API í‚¤ (Noneì´ë©´ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ìŒ)
        """
        if not AGENT_AVAILABLE:
            raise RuntimeError(
                "LangChain Agentê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
                "'pip install langchain openai' ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
            )

        config = get_config()
        self.openai_api_key = api_key or config.get_openai_key()

        if not self.openai_api_key:
            raise RuntimeError("OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        # LLM ì´ˆê¸°í™”
        try:
            self.llm = OpenAI(
                temperature=0.1,
                openai_api_key=self.openai_api_key,
                max_tokens=2000,
                verbose=True
            )
        except Exception as e:
            safe_log("LLM ì´ˆê¸°í™” ì‹¤íŒ¨", level="error", error=str(e))
            raise RuntimeError(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

        # ë©”ëª¨ë¦¬ ì„¤ì •
        try:
            self.memory = ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True,
                input_key="input",
                output_key="output"
            )
        except Exception as e:
            safe_log("ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨", level="warning", error=str(e))
            self.memory = None

        # Tools ë“±ë¡ (ì‹¤ì œ Tools ì‚¬ìš©)
        self.tools = [
            scrape_news,
            analyze_sentiment,
            analyze_news_trend,
        ]

        # Agent ì´ˆê¸°í™”
        try:
            self.agent = initialize_agent(
                tools=self.tools,
                llm=self.llm,
                agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
                memory=self.memory,
                verbose=True,
                max_iterations=10,
                early_stopping_method="generate"
            )
            safe_log("NewsAnalysisAgent ì´ˆê¸°í™” ì™„ë£Œ", level="info", tools_count=len(self.tools))
        except Exception as e:
            safe_log("Agent ì´ˆê¸°í™” ì‹¤íŒ¨", level="error", error=str(e))
            raise RuntimeError(f"Agent ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    async def analyze_news_async(
        self,
        keyword: str,
        sources: List[str] = None,
        max_articles: int = 10
    ) -> Dict[str, Any]:
        """
        ë¹„ë™ê¸° ë‰´ìŠ¤ ë¶„ì„ ì‹¤í–‰

        Args:
            keyword: ê²€ìƒ‰í•  í‚¤ì›Œë“œ
            sources: ë‰´ìŠ¤ ì†ŒìŠ¤ ëª©ë¡ (["ë„¤ì´ë²„", "êµ¬ê¸€"])
            max_articles: ìµœëŒ€ ê¸°ì‚¬ ìˆ˜

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if sources is None:
            sources = ["ë„¤ì´ë²„"]

        # ì…ë ¥ ê²€ì¦
        if not validate_input(keyword, max_length=100):
            raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ í‚¤ì›Œë“œì…ë‹ˆë‹¤.")

        safe_log("ë‰´ìŠ¤ ë¶„ì„ ì‹œì‘", level="info", keyword=keyword, sources=sources)

        try:
            # 1ë‹¨ê³„: ë‰´ìŠ¤ ìˆ˜ì§‘
            articles_data = scrape_news(keyword, sources, max_articles)

            if not articles_data or (len(articles_data) == 1 and "error" in articles_data[0]):
                return {
                    "error": articles_data[0].get("error", "ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨") if articles_data else "ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨",
                    "keyword": keyword,
                    "sources": sources
                }

            # 2ë‹¨ê³„: ê° ê¸°ì‚¬ ë° ëŒ“ê¸€ ê°ì„± ë¶„ì„
            analyzed_articles = []
            all_comments = []

            for article in articles_data:
                if "error" in article:
                    continue

                # ê¸°ì‚¬ ë³¸ë¬¸ ê°ì„± ë¶„ì„
                article_text = f"{article.get('title', '')} {article.get('content', '')}"
                article_sentiment = analyze_sentiment(article_text[:500])  # ìµœëŒ€ 500ì

                # ëŒ“ê¸€ ê°ì„± ë¶„ì„
                article_comments = article.get("comments", [])
                analyzed_comments = []

                for comment in article_comments[:10]:  # ìµœëŒ€ 10ê°œ ëŒ“ê¸€
                    comment_text = comment.get("text", "") if isinstance(comment, dict) else str(comment)
                    if comment_text:
                        comment_sentiment = analyze_sentiment(comment_text)
                        analyzed_comments.append({
                            **comment if isinstance(comment, dict) else {"text": comment},
                            **comment_sentiment
                        })
                        all_comments.append(comment_text)

                analyzed_articles.append({
                    **article,
                    **article_sentiment,
                    "comments": analyzed_comments,
                    "comment_count": len(analyzed_comments)
                })

            # 3ë‹¨ê³„: ì „ì²´ ë™í–¥ ë¶„ì„
            if all_comments:
                trend_result = analyze_news_trend(
                    [{"text": c} for c in all_comments],
                    keyword
                )
            else:
                trend_result = {
                    "keyword": keyword,
                    "overall_sentiment": "ì¤‘ë¦½",
                    "sentiment_distribution": {"ê¸ì •": 0.33, "ë¶€ì •": 0.33, "ì¤‘ë¦½": 0.34},
                    "key_topics": [],
                    "summary": "ëŒ“ê¸€ì´ ì—†ì–´ ë™í–¥ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "total_comments": 0
                }

            # 4ë‹¨ê³„: ê°ì„± ë¶„í¬ ê³„ì‚°
            sentiment_distribution = self._calculate_sentiment_distribution(analyzed_articles)

            # 5ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords(analyzed_articles, keyword)

            result = {
                "keyword": keyword,
                "sources": sources,
                "total_articles": len(analyzed_articles),
                "articles": analyzed_articles,
                "sentiment_distribution": sentiment_distribution,
                "trend_analysis": trend_result,
                "keywords": keywords,
                "analyzed_at": datetime.now().isoformat()
            }

            safe_log("ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ", level="info", total_articles=len(analyzed_articles))
            return result

        except Exception as e:
            safe_log("ë‰´ìŠ¤ ë¶„ì„ ì˜¤ë¥˜", level="error", error=str(e))
            return {
                "error": f"ë‰´ìŠ¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}",
                "keyword": keyword,
                "sources": sources
            }

    def analyze_news_sentiment(self, user_query: str) -> str:
        """
        ìì—°ì–´ ì§ˆì˜ë¥¼ ë°›ì•„ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ìˆ˜í–‰

        Args:
            user_query: ì‚¬ìš©ì ì§ˆì˜ (ì˜ˆ: "AI ê¸°ìˆ ì— ëŒ€í•œ ìµœê·¼ ë‰´ìŠ¤ì˜ ì—¬ë¡ ì„ ë¶„ì„í•´ì¤˜")

        Returns:
            Agent ì‘ë‹µ ë¬¸ìì—´
        """
        if not validate_input(user_query, max_length=500):
            return "ìœ íš¨í•˜ì§€ ì•Šì€ ì§ˆì˜ì…ë‹ˆë‹¤."

        safe_log("Agent ì‹¤í–‰ ì‹œì‘", level="info", query=user_query[:50])

        try:
            response = self.agent.run(input=user_query)
            safe_log("Agent ì‹¤í–‰ ì™„ë£Œ", level="info")
            return response
        except Exception as e:
            error_msg = f"Agent ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"
            safe_log("Agent ì‹¤í–‰ ì˜¤ë¥˜", level="error", error=str(e))
            return error_msg

    def _calculate_sentiment_distribution(self, articles: List[Dict]) -> Dict[str, int]:
        """ê°ì„± ë¶„í¬ ê³„ì‚°"""
        distribution = {"positive": 0, "negative": 0, "neutral": 0}

        for article in articles:
            sentiment = article.get("sentiment", "ì¤‘ë¦½")
            if sentiment == "ê¸ì •":
                distribution["positive"] += 1
            elif sentiment == "ë¶€ì •":
                distribution["negative"] += 1
            else:
                distribution["neutral"] += 1

        return distribution

    def _extract_keywords(self, articles: List[Dict], main_keyword: str) -> List[Dict]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¹ˆë„ ê³„ì‚°"""
        keyword_freq = {}

        for article in articles:
            text = f"{article.get('title', '')} {article.get('content', '')}"
            words = text.split()

            for word in words:
                if len(word) > 1 and word != main_keyword:
                    keyword_freq[word] = keyword_freq.get(word, 0) + 1

        # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ ë°˜í™˜
        sorted_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)[:10]

        return [
            {
                "keyword": keyword,
                "frequency": freq
            }
            for keyword, freq in sorted_keywords
        ]

    def get_conversation_history(self) -> List[Dict]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        if self.memory:
            return self.memory.chat_memory.messages
        return []


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ë¹„ë™ê¸°)"""
    print("ğŸš€ News Analysis Agent í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)

    try:
        config = get_config()
        agent = NewsAnalysisAgent(config.get_openai_key())

        # í…ŒìŠ¤íŠ¸: ë¹„ë™ê¸° ë¶„ì„
        print("\nğŸ“ ë¹„ë™ê¸° ë‰´ìŠ¤ ë¶„ì„ í…ŒìŠ¤íŠ¸:")
        result = await agent.analyze_news_async(
            keyword="AI",
            sources=["ë„¤ì´ë²„", "êµ¬ê¸€"],
            max_articles=5
        )

        if "error" in result:
            print(f"âŒ ì˜¤ë¥˜: {result['error']}")
        else:
            print(f"âœ… ë¶„ì„ ì™„ë£Œ:")
            print(f"   - ì´ ê¸°ì‚¬ ìˆ˜: {result['total_articles']}")
            print(f"   - ê°ì„± ë¶„í¬: {result['sentiment_distribution']}")
            print(f"   - í‚¤ì›Œë“œ ìˆ˜: {len(result['keywords'])}")

        # í…ŒìŠ¤íŠ¸: ìì—°ì–´ ì§ˆì˜
        print("\nğŸ“ ìì—°ì–´ ì§ˆì˜ í…ŒìŠ¤íŠ¸:")
        response = agent.analyze_news_sentiment("AI ê¸°ìˆ ì— ëŒ€í•œ ìµœê·¼ ë‰´ìŠ¤ì˜ ì—¬ë¡ ì„ ë¶„ì„í•´ì¤˜")
        print(f"âœ… ì‘ë‹µ: {response[:200]}...")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    asyncio.run(main())

