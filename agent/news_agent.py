"""
News Analysis Agent

ë‰´ìŠ¤ ê°ì„± ë¶„ì„ì„ ìœ„í•œ í†µí•© AI Agent
ë„¤ì´ë²„ ë‰´ìŠ¤ì™€ êµ¬ê¸€ ë‰´ìŠ¤ë¥¼ ì§€ì›í•˜ë©°, ì‹¤ì œ Toolsë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

import json
import asyncio
import time
from typing import List, Dict, Any, Optional
from datetime import datetime

# LangChain import (ìµœì‹  ë²„ì „ 1.2.0 í˜¸í™˜)
try:
    # LangGraph ê¸°ë°˜ ReAct Agent (ìµœì‹  ë°©ì‹)
    from langgraph.prebuilt import create_react_agent
    from langchain_openai import ChatOpenAI
    from langchain_core.chat_history import InMemoryChatMessageHistory
    from langchain_core.messages import HumanMessage, AIMessage
    AGENT_AVAILABLE = True
    USE_LANGGRAPH = True
except ImportError:
    try:
        # ëŒ€ì²´: LangChainì˜ ë‹¤ë¥¸ ë°©ì‹ ì‹œë„
        from langchain_openai import ChatOpenAI
        from langchain_core.chat_history import InMemoryChatMessageHistory
        AGENT_AVAILABLE = True
        USE_LANGGRAPH = False
    except ImportError:
        AGENT_AVAILABLE = False
        USE_LANGGRAPH = False

from common.config import get_config
from common.utils import safe_log, validate_input
from agent.tools import scrape_news, analyze_sentiment, analyze_sentiment_func, analyze_news_trend, analyze_news_trend_func
from agent.tools.news_scraper import NewsScraperTool

# OpenAI ìš”ì•½ ê¸°ëŠ¥ì„ ìœ„í•œ import
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# Playwright ìŠ¤í¬ë˜í¼ (ë³‘ë ¬ì²˜ë¦¬ ì§€ì›)
try:
    from agent.tools.news_scraper.playwright_scraper import PlaywrightNewsScraper
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    safe_log("Playwright ì‚¬ìš© ë¶ˆê°€ - Selenium í´ë°±", level="warning")


class NewsAnalysisAgent:
    """ë‰´ìŠ¤ ê°ì„± ë¶„ì„ì„ ìœ„í•œ í†µí•© AI Agent"""

    def __init__(self, api_key: Optional[str] = None):
        """
        Agent ì´ˆê¸°í™”

        Args:
            api_key: OpenAI API í‚¤ (Noneì´ë©´ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ìŒ)
        """
        config = get_config()
        self.openai_api_key = api_key or config.get_openai_key()

        if not self.openai_api_key:
            raise RuntimeError("OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        # LangChain AgentëŠ” ì„ íƒì  (analyze_news_sentimentì—ì„œë§Œ ì‚¬ìš©)
        # analyze_news_asyncëŠ” LangChain ì—†ì´ë„ ì‘ë™
        self.agent = None
        self.llm = None
        self.memory = None

        if AGENT_AVAILABLE:
            try:
                # LLM ì´ˆê¸°í™”
                self.llm = ChatOpenAI(
                    temperature=0.1,
                    openai_api_key=self.openai_api_key,
                    max_tokens=2000,
                    model="gpt-4o-mini"
                )

                # ë©”ëª¨ë¦¬ ì„¤ì • (ìµœì‹  ë°©ì‹)
                try:
                    self.memory = InMemoryChatMessageHistory()
                except Exception as e:
                    safe_log("ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨", level="warning", error=str(e))
                    self.memory = None

                # Tools ë“±ë¡ (ì‹¤ì œ Tools ì‚¬ìš©)
                self.tools = [
                    scrape_news,
                    analyze_sentiment,
                    analyze_news_trend,
                ]

                # Agent ì´ˆê¸°í™” (LangGraph ë°©ì‹)
                if USE_LANGGRAPH and create_react_agent:
                    try:
                        # create_react_agentëŠ” modelê³¼ toolsë§Œ í•„ìš”
                        self.agent = create_react_agent(
                            model=self.llm,
                            tools=self.tools
                        )
                        safe_log("NewsAnalysisAgent ì´ˆê¸°í™” ì™„ë£Œ (LangGraph Agent í¬í•¨)", level="info", tools_count=len(self.tools))
                    except Exception as e:
                        safe_log("LangGraph Agent ì´ˆê¸°í™” ì‹¤íŒ¨ (ê³„ì† ì§„í–‰)", level="warning", error=str(e))
                        self.agent = None
                else:
                    safe_log("LangGraphë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ (analyze_news_asyncë§Œ ì‚¬ìš© ê°€ëŠ¥)", level="warning")
                    self.agent = None

            except Exception as e:
                safe_log("LangChain ì´ˆê¸°í™” ì‹¤íŒ¨ (ê³„ì† ì§„í–‰)", level="warning", error=str(e))
                # LangChain ì—†ì´ë„ analyze_news_asyncëŠ” ì‘ë™ ê°€ëŠ¥
        else:
            safe_log("LangChainì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ (analyze_news_asyncë§Œ ì‚¬ìš© ê°€ëŠ¥)", level="warning")

    def _parse_keyword_operators(self, keyword: str) -> Dict[str, Any]:
        """
        í‚¤ì›Œë“œì—ì„œ ê²€ìƒ‰ ì—°ì‚°ì íŒŒì‹± (OR, AND)
        
        ì§€ì›í•˜ëŠ” í˜•ì‹:
        - "ì‚¼ì„±ì „ì || LGì „ì" â†’ OR ê²€ìƒ‰
        - "ì‚¼ì„±ì „ì OR LGì „ì" â†’ OR ê²€ìƒ‰
        - "ì‚¼ì„±ì „ì LGì „ì" â†’ AND ê²€ìƒ‰ (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)
        
        Returns:
            {"type": "or" | "and" | "single", "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...]}
        """
        import re
        
        # ì•ë’¤ ê³µë°± ì œê±°
        keyword = keyword.strip()
        
        # OR ê²€ìƒ‰ ì²´í¬ (|| ë˜ëŠ” OR)
        or_pattern = r'\s*(?:\|\||OR)\s*'
        if re.search(or_pattern, keyword, re.IGNORECASE):
            keywords = [k.strip() for k in re.split(or_pattern, keyword, flags=re.IGNORECASE) if k.strip()]
            return {"type": "or", "keywords": keywords}
        
        # ë‹¨ì¼ í‚¤ì›Œë“œ (ê³µë°± í¬í•¨ ê°€ëŠ¥ - AND ê²€ìƒ‰)
        return {"type": "single", "keywords": [keyword]}

    async def analyze_news_async(
        self,
        keyword: str,
        sources: List[str] = None,
        max_articles: int = 10
    ) -> Dict[str, Any]:
        """
        ë¹„ë™ê¸° ë‰´ìŠ¤ ë¶„ì„ ì‹¤í–‰

        Args:
            keyword: ê²€ìƒ‰í•  í‚¤ì›Œë“œ (OR ì—°ì‚°ì ì§€ì›: "ì‚¼ì„±ì „ì || LGì „ì")
            sources: ë‰´ìŠ¤ ì†ŒìŠ¤ ëª©ë¡ (["ë„¤ì´ë²„", "êµ¬ê¸€"])
            max_articles: ìµœëŒ€ ê¸°ì‚¬ ìˆ˜

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if sources is None:
            sources = ["ë„¤ì´ë²„"]

        # ì…ë ¥ ê²€ì¦
        if not validate_input(keyword, max_length=200):
            raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ í‚¤ì›Œë“œì…ë‹ˆë‹¤.")
        
        # OR ê²€ìƒ‰ íŒŒì‹±
        parsed = self._parse_keyword_operators(keyword)
        
        if parsed["type"] == "or" and len(parsed["keywords"]) > 1:
            # OR ê²€ìƒ‰: ê° í‚¤ì›Œë“œë³„ë¡œ ë¶„ì„ í›„ ë³‘í•©
            safe_log("OR ê²€ìƒ‰ ê°ì§€", level="info", keywords=parsed["keywords"])
            return await self._analyze_multiple_keywords_or(
                parsed["keywords"], sources, max_articles
            )

        safe_log("ë‰´ìŠ¤ ë¶„ì„ ì‹œì‘", level="info", keyword=keyword, sources=sources)

        # ì„±ëŠ¥ ì¸¡ì •ì„ ìœ„í•œ ì‹œê°„ ê¸°ë¡
        timing_info = {
            "crawling_time": 0.0,
            "sentiment_time": 0.0,
            "summary_time": 0.0,
            "total_time": 0.0
        }
        
        # LLM í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
        token_usage = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
            "estimated_cost": 0.0  # USD
        }
        
        total_start_time = time.time()

        try:
            # 1ë‹¨ê³„: ë‰´ìŠ¤ ìˆ˜ì§‘ (Playwright ë³‘ë ¬ì²˜ë¦¬ ë˜ëŠ” Selenium í´ë°±)
            crawling_start = time.time()
            articles_data = []
            
            # ì†ŒìŠ¤ í•„í„°ë§ ë° ë§¤í•‘
            source_mapping = {
                "ë„¤ì´ë²„": "ë„¤ì´ë²„", "naver": "ë„¤ì´ë²„",
                "êµ¬ê¸€": "êµ¬ê¸€", "google": "êµ¬ê¸€",
            }
            unsupported_sources = ["ë‹¤ìŒ", "Daum", "KBS", "SBS", "MBC", "YTN", "JTBC", "ì—°í•©ë‰´ìŠ¤"]
            
            valid_sources = []
            rejected_sources = []
            
            for source in (sources or ["ë„¤ì´ë²„"]):
                if source in unsupported_sources:
                    rejected_sources.append(source)
                    safe_log("ì§€ì›í•˜ì§€ ì•ŠëŠ” ë‰´ìŠ¤ ì†ŒìŠ¤", level="warning", source=source)
                    continue
                
                normalized_source = source_mapping.get(source)
                if normalized_source:
                    if normalized_source not in valid_sources:
                        valid_sources.append(normalized_source)
                else:
                    rejected_sources.append(source)
            
            if not valid_sources and rejected_sources:
                return {
                    "error": f"ì„ íƒí•œ ë‰´ìŠ¤ ì†ŒìŠ¤({', '.join(rejected_sources)})ëŠ” ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë„¤ì´ë²„ ë˜ëŠ” êµ¬ê¸€ì„ ì„ íƒí•´ì£¼ì„¸ìš”.",
                    "keyword": keyword,
                    "rejected_sources": rejected_sources,
                    "supported_sources": ["ë„¤ì´ë²„", "êµ¬ê¸€"]
                }
            
            if not valid_sources:
                valid_sources = ["ë„¤ì´ë²„"]
            
            # Playwright ë³‘ë ¬ì²˜ë¦¬ ì‚¬ìš© (ê°€ëŠ¥í•œ ê²½ìš°)
            if PLAYWRIGHT_AVAILABLE:
                safe_log("Playwright ë³‘ë ¬ í¬ë¡¤ë§ ì‹œì‘", level="info", keyword=keyword, sources=valid_sources)
                print(f"[DEBUG] ğŸš€ Playwright ë³‘ë ¬ì²˜ë¦¬ ì‚¬ìš© - ì†ŒìŠ¤: {valid_sources}")
                
                playwright_scraper = PlaywrightNewsScraper()
                try:
                    # ë³‘ë ¬ë¡œ ëª¨ë“  ê¸°ì‚¬ ìˆ˜ì§‘ ë° ì¶”ì¶œ (ê²€ìƒ‰ + ì¶”ì¶œ ëª¨ë‘ ë³‘ë ¬)
                    articles_data = await asyncio.wait_for(
                        playwright_scraper.scrape_all(keyword, valid_sources, max_articles),
                        timeout=180  # 3ë¶„ (ë³‘ë ¬ì²˜ë¦¬ë¡œ ì¶©ë¶„)
                    )
                    
                    # í‚¤ì›Œë“œ ì¶”ê°€
                    for article in articles_data:
                        article["keyword"] = keyword
                        
                except asyncio.TimeoutError:
                    safe_log("Playwright íƒ€ì„ì•„ì›ƒ (3ë¶„ ì´ˆê³¼)", level="warning")
                    return {
                        "error": f"'{keyword}' í‚¤ì›Œë“œë¡œ ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘ ì‹œê°„ ì´ˆê³¼ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                        "keyword": keyword,
                        "sources": valid_sources
                    }
                finally:
                    await playwright_scraper.cleanup()
            else:
                # Selenium í´ë°± (ê¸°ì¡´ ë°©ì‹)
                safe_log("Selenium ìˆœì°¨ í¬ë¡¤ë§ ì‹œì‘ (Playwright ë¶ˆê°€)", level="info")
                print(f"[DEBUG] âš ï¸ Selenium ìˆœì°¨ì²˜ë¦¬ í´ë°±")
                
                scraper = NewsScraperTool()
                try:
                    article_urls = await asyncio.wait_for(
                        asyncio.to_thread(scraper.search_news, keyword, valid_sources, max_articles),
                        timeout=120
                    )
                    
                    if not article_urls:
                        return {
                            "error": f"'{keyword}' í‚¤ì›Œë“œë¡œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                            "keyword": keyword,
                            "sources": valid_sources
                        }
                    
                    # ìˆœì°¨ ì¶”ì¶œ
                    for url in article_urls:
                        source = "naver" if "naver.com" in url else "google"
                        try:
                            article = scraper.scrape_article(url, source)
                            article_dict = article.to_dict()
                            article_dict["keyword"] = keyword
                            article_dict["source"] = "ë„¤ì´ë²„" if source == "naver" else "êµ¬ê¸€"
                            articles_data.append(article_dict)
                        except Exception as e:
                            safe_log(f"ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {url}", level="warning", error=str(e))
                        time.sleep(1)
                        
                except asyncio.TimeoutError:
                    return {
                        "error": f"'{keyword}' í‚¤ì›Œë“œë¡œ ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘ ì‹œê°„ ì´ˆê³¼ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                        "keyword": keyword,
                        "sources": valid_sources
                    }
                finally:
                    scraper.cleanup()

            if not articles_data or (len(articles_data) == 1 and "error" in articles_data[0]):
                return {
                    "error": articles_data[0].get("error", "ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨") if articles_data else "ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨",
                    "keyword": keyword,
                    "sources": sources
                }

            # í¬ë¡¤ë§ ì‹œê°„ ê¸°ë¡
            timing_info["crawling_time"] = round(time.time() - crawling_start, 2)
            safe_log(f"í¬ë¡¤ë§ ì™„ë£Œ: {timing_info['crawling_time']}ì´ˆ", level="info")

            # 2ë‹¨ê³„: ê° ê¸°ì‚¬ ë° ëŒ“ê¸€ ê°ì„± ë¶„ì„
            sentiment_start = time.time()
            analyzed_articles = []
            all_comments = []

            for article in articles_data:
                if "error" in article:
                    safe_log("ê¸°ì‚¬ ìŠ¤í‚µ (ì—ëŸ¬ í¬í•¨)", level="warning", error=article.get("error"))
                    continue

                # ê¸°ì‚¬ ë³¸ë¬¸ ê°ì„± ë¶„ì„
                article_text = f"{article.get('title', '')} {article.get('content', '')}"
                
                try:
                    # analyze_sentiment_func ì‚¬ìš© (ì§ì ‘ í˜¸ì¶œ ê°€ëŠ¥í•œ í•¨ìˆ˜)
                    article_sentiment = analyze_sentiment_func(article_text[:500])  # ìµœëŒ€ 500ì
                except Exception as e:
                    safe_log("ê¸°ì‚¬ ê°ì„± ë¶„ì„ ì‹¤íŒ¨", level="error", error=str(e))
                    article_sentiment = {
                        "sentiment": "ì¤‘ë¦½",
                        "sentiment_score": 0.0,
                        "sentiment_label": "neutral",
                        "confidence": 0.0
                    }

                # ëŒ“ê¸€ ê°ì„± ë¶„ì„
                article_comments = article.get("comments", [])
                analyzed_comments = []

                for comment in article_comments[:10]:  # ìµœëŒ€ 10ê°œ ëŒ“ê¸€
                    comment_text = comment.get("text", "") if isinstance(comment, dict) else str(comment)
                    if comment_text:
                        try:
                            # analyze_sentiment_func ì‚¬ìš© (ì§ì ‘ í˜¸ì¶œ ê°€ëŠ¥í•œ í•¨ìˆ˜)
                            comment_sentiment = analyze_sentiment_func(comment_text)
                            # ëŒ“ê¸€ ë°ì´í„°ì™€ ê°ì„± ë¶„ì„ ê²°ê³¼ ë³‘í•©
                            comment_data = comment if isinstance(comment, dict) else {"text": comment}
                            analyzed_comments.append({
                                **comment_data,
                                **comment_sentiment
                            })
                            all_comments.append(comment_text)
                        except Exception as e:
                            safe_log("ëŒ“ê¸€ ê°ì„± ë¶„ì„ ì‹¤íŒ¨", level="warning", error=str(e))
                            continue

                analyzed_articles.append({
                    **article,
                    **article_sentiment,
                    "summary": "",  # ìš”ì•½ì€ ë³„ë„ ë‹¨ê³„ì—ì„œ ì²˜ë¦¬
                    "comments": analyzed_comments,
                    "comment_count": len(analyzed_comments)
                })

            # ê°ì„± ë¶„ì„ ì‹œê°„ ê¸°ë¡
            timing_info["sentiment_time"] = round(time.time() - sentiment_start, 2)
            safe_log(f"ê°ì„± ë¶„ì„ ì™„ë£Œ: {timing_info['sentiment_time']}ì´ˆ", level="info")

            # ê¸°ì‚¬ ìš”ì•½ ìƒì„± (ë³„ë„ ë‹¨ê³„ë¡œ ë¶„ë¦¬)
            summary_start = time.time()
            for i, analyzed_article in enumerate(analyzed_articles):
                summary_result = self._summarize_article(
                    analyzed_article.get('title', ''),
                    analyzed_article.get('content', '')
                )
                analyzed_articles[i]["summary"] = summary_result["summary"]
                
                # í† í° ì‚¬ìš©ëŸ‰ ëˆ„ì 
                usage = summary_result.get("usage", {})
                token_usage["prompt_tokens"] += usage.get("prompt_tokens", 0)
                token_usage["completion_tokens"] += usage.get("completion_tokens", 0)
                token_usage["total_tokens"] += usage.get("total_tokens", 0)

            # 3ë‹¨ê³„: ì „ì²´ ë™í–¥ ë¶„ì„
            if all_comments:
                # analyze_news_trend_func ì‚¬ìš© (ì§ì ‘ í˜¸ì¶œ ê°€ëŠ¥í•œ í•¨ìˆ˜)
                trend_result = analyze_news_trend_func(
                    [{"text": c} for c in all_comments],
                    keyword
                )
            else:
                trend_result = {
                    "keyword": keyword,
                    "overall_sentiment": "ì¤‘ë¦½",
                    "sentiment_distribution": {"ê¸ì •": 0.33, "ë¶€ì •": 0.33, "ì¤‘ë¦½": 0.34},
                    "key_topics": [],
                    "summary": "ëŒ“ê¸€ì´ ì—†ì–´ ë™í–¥ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "total_comments": 0
                }

            # 4ë‹¨ê³„: ê°ì„± ë¶„í¬ ê³„ì‚°
            sentiment_distribution = self._calculate_sentiment_distribution(analyzed_articles)

            # 5ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords(analyzed_articles, keyword)

            # 6ë‹¨ê³„: ì „ì²´ ì¢…í•© ìš”ì•½ ìƒì„±
            overall_result = self._generate_overall_summary(
                analyzed_articles, 
                keyword, 
                sentiment_distribution
            )
            overall_summary = overall_result["summary"]
            
            # ì¢…í•© ìš”ì•½ í† í° ì‚¬ìš©ëŸ‰ ì¶”ê°€
            overall_usage = overall_result.get("usage", {})
            token_usage["prompt_tokens"] += overall_usage.get("prompt_tokens", 0)
            token_usage["completion_tokens"] += overall_usage.get("completion_tokens", 0)
            token_usage["total_tokens"] += overall_usage.get("total_tokens", 0)
            
            # ì˜ˆìƒ ë¹„ìš© ê³„ì‚° (gpt-4o-mini ê°€ê²© ê¸°ì¤€: input $0.15/1M, output $0.6/1M)
            token_usage["estimated_cost"] = round(
                (token_usage["prompt_tokens"] * 0.15 / 1_000_000) +
                (token_usage["completion_tokens"] * 0.6 / 1_000_000),
                6
            )

            # ìš”ì•½ ì‹œê°„ ê¸°ë¡ (ê¸°ì‚¬ ìš”ì•½ + ì¢…í•© ìš”ì•½)
            timing_info["summary_time"] = round(time.time() - summary_start, 2)
            safe_log(f"ìš”ì•½ ìƒì„± ì™„ë£Œ: {timing_info['summary_time']}ì´ˆ, í† í°: {token_usage['total_tokens']}", level="info")

            # ì´ ì†Œìš” ì‹œê°„
            timing_info["total_time"] = round(time.time() - total_start_time, 2)

            result = {
                "keyword": keyword,
                "sources": sources,
                "total_articles": len(analyzed_articles),
                "articles": analyzed_articles,
                "sentiment_distribution": sentiment_distribution,
                "trend_analysis": trend_result,
                "keywords": keywords,
                "overall_summary": overall_summary,
                "timing": timing_info,  # ì„±ëŠ¥ ì¸¡ì • ì •ë³´ ì¶”ê°€
                "token_usage": token_usage,  # LLM í† í° ì‚¬ìš©ëŸ‰ ì¶”ê°€
                "analyzed_at": datetime.now().isoformat()
            }

            safe_log(f"ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ (ì´ {timing_info['total_time']}ì´ˆ, í† í°: {token_usage['total_tokens']})", level="info", total_articles=len(analyzed_articles))
            return result

        except Exception as e:
            safe_log("ë‰´ìŠ¤ ë¶„ì„ ì˜¤ë¥˜", level="error", error=str(e))
            return {
                "error": f"ë‰´ìŠ¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}",
                "keyword": keyword,
                "sources": sources
            }

    def analyze_news_sentiment(self, user_query: str) -> str:
        """
        ìì—°ì–´ ì§ˆì˜ë¥¼ ë°›ì•„ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ìˆ˜í–‰

        Args:
            user_query: ì‚¬ìš©ì ì§ˆì˜ (ì˜ˆ: "AI ê¸°ìˆ ì— ëŒ€í•œ ìµœê·¼ ë‰´ìŠ¤ì˜ ì—¬ë¡ ì„ ë¶„ì„í•´ì¤˜")

        Returns:
            Agent ì‘ë‹µ ë¬¸ìì—´
        """
        if not self.agent:
            return "LangChain Agentê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. analyze_news_asyncë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."

        if not validate_input(user_query, max_length=500):
            return "ìœ íš¨í•˜ì§€ ì•Šì€ ì§ˆì˜ì…ë‹ˆë‹¤."

        safe_log("Agent ì‹¤í–‰ ì‹œì‘", level="info", query=user_query[:50])

        try:
            # LangGraph Agent ì‹¤í–‰ (ìµœì‹  ë°©ì‹)
            if USE_LANGGRAPH:
                from langchain_core.messages import HumanMessage
                messages = [HumanMessage(content=user_query)]
                if self.memory:
                    messages = list(self.memory.messages) + messages
                
                response = self.agent.invoke({"messages": messages})
                
                # ë©”ëª¨ë¦¬ì— ì‘ë‹µ ì €ì¥
                if self.memory:
                    self.memory.add_message(HumanMessage(content=user_query))
                    if isinstance(response, dict) and "messages" in response:
                        self.memory.add_messages(response["messages"][-1:])
                
                # ì‘ë‹µ ì¶”ì¶œ
                if isinstance(response, dict) and "messages" in response:
                    last_message = response["messages"][-1]
                    result = last_message.content if hasattr(last_message, "content") else str(last_message)
                else:
                    result = str(response)
            else:
                # ëŒ€ì²´ ë°©ì‹ (ì—†ìœ¼ë©´ ì—ëŸ¬)
                result = "LangGraph Agentë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            safe_log("Agent ì‹¤í–‰ ì™„ë£Œ", level="info")
            return result
        except Exception as e:
            error_msg = f"Agent ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"
            safe_log("Agent ì‹¤í–‰ ì˜¤ë¥˜", level="error", error=str(e))
            return error_msg

    def _calculate_sentiment_distribution(self, articles: List[Dict]) -> Dict[str, int]:
        """ê°ì„± ë¶„í¬ ê³„ì‚°"""
        distribution = {"positive": 0, "negative": 0, "neutral": 0}

        for article in articles:
            sentiment = article.get("sentiment", "ì¤‘ë¦½")
            if sentiment == "ê¸ì •":
                distribution["positive"] += 1
            elif sentiment == "ë¶€ì •":
                distribution["negative"] += 1
            else:
                distribution["neutral"] += 1

        return distribution

    async def _analyze_multiple_keywords_or(
        self,
        keywords: List[str],
        sources: List[str],
        max_articles: int
    ) -> Dict[str, Any]:
        """
        OR ê²€ìƒ‰: ì—¬ëŸ¬ í‚¤ì›Œë“œë¥¼ ê°ê° ë¶„ì„ í›„ ê²°ê³¼ ë³‘í•©
        
        Args:
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡
            sources: ë‰´ìŠ¤ ì†ŒìŠ¤ ëª©ë¡
            max_articles: í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
        
        Returns:
            ë³‘í•©ëœ ë¶„ì„ ê²°ê³¼
        """
        total_start_time = time.time()
        timing_info = {
            "crawling_time": 0.0,
            "sentiment_time": 0.0,
            "summary_time": 0.0,
            "total_time": 0.0
        }
        token_usage = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
            "estimated_cost": 0.0
        }
        
        all_articles = []
        all_keywords_data = []
        combined_sentiment = {"positive": 0, "negative": 0, "neutral": 0}
        keyword_results = []
        
        # ê° í‚¤ì›Œë“œë³„ë¡œ ë¶„ì„ ì‹¤í–‰
        articles_per_keyword = max(3, max_articles // len(keywords))  # í‚¤ì›Œë“œë‹¹ ê¸°ì‚¬ ìˆ˜ ë¶„ë°°
        
        for kw in keywords:
            safe_log(f"OR ê²€ìƒ‰ - '{kw}' ë¶„ì„ ì‹œì‘", level="info")
            
            try:
                # ë‹¨ì¼ í‚¤ì›Œë“œ ë¶„ì„ (ì¬ê·€ í˜¸ì¶œ ë°©ì§€ë¥¼ ìœ„í•´ ì§ì ‘ ë¶„ì„ ë¡œì§ ì‚¬ìš©)
                result = await self._analyze_single_keyword(
                    kw, sources, articles_per_keyword
                )
                
                if "error" not in result:
                    keyword_results.append({
                        "keyword": kw,
                        "article_count": result.get("total_articles", 0),
                        "sentiment": result.get("sentiment_distribution", {})
                    })
                    
                    # ê¸°ì‚¬ ë³‘í•©
                    for article in result.get("articles", []):
                        article["search_keyword"] = kw  # ì–´ë–¤ í‚¤ì›Œë“œë¡œ ì°¾ì•˜ëŠ”ì§€ í‘œì‹œ
                        all_articles.append(article)
                    
                    # ê°ì • ë¶„í¬ í•©ì‚°
                    for key in combined_sentiment:
                        combined_sentiment[key] += result.get("sentiment_distribution", {}).get(key, 0)
                    
                    # íƒ€ì´ë° ì •ë³´ í•©ì‚°
                    result_timing = result.get("timing", {})
                    timing_info["crawling_time"] += result_timing.get("crawling_time", 0)
                    timing_info["sentiment_time"] += result_timing.get("sentiment_time", 0)
                    timing_info["summary_time"] += result_timing.get("summary_time", 0)
                    
                    # í† í° ì‚¬ìš©ëŸ‰ í•©ì‚°
                    result_tokens = result.get("token_usage", {})
                    token_usage["prompt_tokens"] += result_tokens.get("prompt_tokens", 0)
                    token_usage["completion_tokens"] += result_tokens.get("completion_tokens", 0)
                    token_usage["total_tokens"] += result_tokens.get("total_tokens", 0)
                    
                    # í‚¤ì›Œë“œ ë°ì´í„° ë³‘í•©
                    all_keywords_data.extend(result.get("keywords", []))
                    
            except Exception as e:
                safe_log(f"OR ê²€ìƒ‰ - '{kw}' ë¶„ì„ ì‹¤íŒ¨", level="warning", error=str(e))
                continue
        
        if not all_articles:
            return {
                "error": f"'{' || '.join(keywords)}' í‚¤ì›Œë“œë¡œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "keyword": " || ".join(keywords),
                "sources": sources
            }
        
        # ì „ì²´ ì¢…í•© ìš”ì•½ ìƒì„±
        summary_start = time.time()
        overall_result = self._generate_overall_summary(
            all_articles,
            " || ".join(keywords),
            combined_sentiment
        )
        overall_summary = overall_result["summary"]
        
        # ì¢…í•© ìš”ì•½ í† í° ì‚¬ìš©ëŸ‰ ì¶”ê°€
        overall_usage = overall_result.get("usage", {})
        token_usage["prompt_tokens"] += overall_usage.get("prompt_tokens", 0)
        token_usage["completion_tokens"] += overall_usage.get("completion_tokens", 0)
        token_usage["total_tokens"] += overall_usage.get("total_tokens", 0)
        
        # ì˜ˆìƒ ë¹„ìš© ê³„ì‚°
        token_usage["estimated_cost"] = round(
            (token_usage["prompt_tokens"] * 0.15 / 1_000_000) +
            (token_usage["completion_tokens"] * 0.6 / 1_000_000),
            6
        )
        
        timing_info["summary_time"] += round(time.time() - summary_start, 2)
        timing_info["total_time"] = round(time.time() - total_start_time, 2)
        
        # ê²°ê³¼ ì¡°í•©
        return {
            "keyword": " || ".join(keywords),
            "search_type": "or",
            "keyword_results": keyword_results,
            "sources": sources,
            "total_articles": len(all_articles),
            "articles": all_articles,
            "sentiment_distribution": combined_sentiment,
            "keywords": all_keywords_data[:20],  # ìƒìœ„ 20ê°œ
            "overall_summary": overall_summary,
            "timing": timing_info,
            "token_usage": token_usage,
            "analyzed_at": datetime.now().isoformat()
        }

    async def _analyze_single_keyword(
        self,
        keyword: str,
        sources: List[str],
        max_articles: int
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ í‚¤ì›Œë“œ ë¶„ì„ (ë‚´ë¶€ ì‚¬ìš©)"""
        # ê¸°ì¡´ analyze_news_async ë¡œì§ì˜ í•µì‹¬ ë¶€ë¶„ì„ ë¶„ë¦¬
        timing_info = {
            "crawling_time": 0.0,
            "sentiment_time": 0.0,
            "summary_time": 0.0,
            "total_time": 0.0
        }
        token_usage = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
            "estimated_cost": 0.0
        }
        total_start_time = time.time()
        
        try:
            # 1ë‹¨ê³„: ë‰´ìŠ¤ ìˆ˜ì§‘
            crawling_start = time.time()
            articles_data = []
            
            source_mapping = {
                "ë„¤ì´ë²„": "ë„¤ì´ë²„", "naver": "ë„¤ì´ë²„",
                "êµ¬ê¸€": "êµ¬ê¸€", "google": "êµ¬ê¸€",
            }
            
            valid_sources = []
            for source in sources:
                normalized_source = source_mapping.get(source)
                if normalized_source and normalized_source not in valid_sources:
                    valid_sources.append(normalized_source)
            
            if not valid_sources:
                valid_sources = ["ë„¤ì´ë²„"]
            
            if PLAYWRIGHT_AVAILABLE:
                playwright_scraper = PlaywrightNewsScraper()
                try:
                    articles_data = await asyncio.wait_for(
                        playwright_scraper.scrape_all(keyword, valid_sources, max_articles),
                        timeout=120
                    )
                    for article in articles_data:
                        article["keyword"] = keyword
                except asyncio.TimeoutError:
                    return {"error": f"'{keyword}' ê²€ìƒ‰ ì‹œê°„ ì´ˆê³¼"}
                finally:
                    await playwright_scraper.cleanup()
            else:
                scraper = NewsScraperTool()
                try:
                    article_urls = await asyncio.wait_for(
                        asyncio.to_thread(scraper.search_news, keyword, valid_sources, max_articles),
                        timeout=120
                    )
                    for url in (article_urls or []):
                        source = "naver" if "naver.com" in url else "google"
                        try:
                            article = scraper.scrape_article(url, source)
                            article_dict = article.to_dict()
                            article_dict["keyword"] = keyword
                            articles_data.append(article_dict)
                        except:
                            pass
                        time.sleep(0.5)
                finally:
                    scraper.cleanup()
            
            timing_info["crawling_time"] = round(time.time() - crawling_start, 2)
            
            if not articles_data:
                return {"error": f"'{keyword}' ê¸°ì‚¬ ì—†ìŒ", "keyword": keyword}
            
            # 2ë‹¨ê³„: ê°ì„± ë¶„ì„
            sentiment_start = time.time()
            analyzed_articles = []
            
            for article in articles_data:
                if "error" in article:
                    continue
                    
                article_text = f"{article.get('title', '')} {article.get('content', '')}"
                try:
                    article_sentiment = analyze_sentiment_func(article_text[:500])
                except:
                    article_sentiment = {"sentiment": "ì¤‘ë¦½", "sentiment_score": 0.0, "sentiment_label": "neutral", "confidence": 0.0}
                
                analyzed_articles.append({
                    **article,
                    **article_sentiment,
                    "summary": "",
                    "comments": [],
                    "comment_count": 0
                })
            
            timing_info["sentiment_time"] = round(time.time() - sentiment_start, 2)
            
            # 3ë‹¨ê³„: ìš”ì•½ ìƒì„±
            summary_start = time.time()
            for i, analyzed_article in enumerate(analyzed_articles):
                summary_result = self._summarize_article(
                    analyzed_article.get('title', ''),
                    analyzed_article.get('content', '')
                )
                analyzed_articles[i]["summary"] = summary_result["summary"]
                
                # í† í° ì‚¬ìš©ëŸ‰ ëˆ„ì 
                usage = summary_result.get("usage", {})
                token_usage["prompt_tokens"] += usage.get("prompt_tokens", 0)
                token_usage["completion_tokens"] += usage.get("completion_tokens", 0)
                token_usage["total_tokens"] += usage.get("total_tokens", 0)
            
            # ì˜ˆìƒ ë¹„ìš© ê³„ì‚°
            token_usage["estimated_cost"] = round(
                (token_usage["prompt_tokens"] * 0.15 / 1_000_000) +
                (token_usage["completion_tokens"] * 0.6 / 1_000_000),
                6
            )
            
            sentiment_distribution = self._calculate_sentiment_distribution(analyzed_articles)
            keywords_data = self._extract_keywords(analyzed_articles, keyword)
            
            timing_info["summary_time"] = round(time.time() - summary_start, 2)
            timing_info["total_time"] = round(time.time() - total_start_time, 2)
            
            return {
                "keyword": keyword,
                "total_articles": len(analyzed_articles),
                "articles": analyzed_articles,
                "sentiment_distribution": sentiment_distribution,
                "keywords": keywords_data,
                "timing": timing_info,
                "token_usage": token_usage
            }
            
        except Exception as e:
            return {"error": str(e), "keyword": keyword}

    def _extract_keywords(self, articles: List[Dict], main_keyword: str) -> List[Dict]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¹ˆë„ ê³„ì‚°"""
        keyword_freq = {}

        for article in articles:
            text = f"{article.get('title', '')} {article.get('content', '')}"
            words = text.split()

            for word in words:
                if len(word) > 1 and word != main_keyword:
                    keyword_freq[word] = keyword_freq.get(word, 0) + 1

        # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ ë°˜í™˜
        sorted_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)[:10]

        return [
            {
                "keyword": keyword,
                "frequency": freq
            }
            for keyword, freq in sorted_keywords
        ]

    def _summarize_article(self, title: str, content: str) -> Dict[str, Any]:
        """
        OpenAIë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ ë‚´ìš© ìš”ì•½
        
        Returns:
            {"summary": "ìš”ì•½ í…ìŠ¤íŠ¸", "usage": {"prompt_tokens": N, "completion_tokens": N, "total_tokens": N}}
        """
        result = {"summary": "", "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}}
        
        if not OPENAI_AVAILABLE or not self.openai_api_key:
            return result
        
        try:
            client = OpenAI(api_key=self.openai_api_key)
            
            # ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ë‚´ê¸°
            text = f"ì œëª©: {title}\n\në‚´ìš©: {content[:3000]}"
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ ë‰´ìŠ¤ ê¸°ì‚¬ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ 3-4ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."
                    },
                    {
                        "role": "user",
                        "content": text
                    }
                ],
                max_tokens=300,
                temperature=0.3
            )
            
            result["summary"] = response.choices[0].message.content.strip()
            
            # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
            if response.usage:
                result["usage"] = {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            
            return result
        except Exception as e:
            safe_log("ê¸°ì‚¬ ìš”ì•½ ì‹¤íŒ¨", level="warning", error=str(e))
            return result

    def _generate_overall_summary(self, articles: List[Dict], keyword: str, sentiment_distribution: Dict) -> Dict[str, Any]:
        """
        ì „ì²´ ê¸°ì‚¬ì— ëŒ€í•œ ì¢…í•© ìš”ì•½ ìƒì„±
        
        Returns:
            {"summary": "ìš”ì•½ í…ìŠ¤íŠ¸", "usage": {"prompt_tokens": N, "completion_tokens": N, "total_tokens": N}}
        """
        result = {"summary": "", "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}}
        
        if not OPENAI_AVAILABLE or not self.openai_api_key:
            return result
        
        try:
            client = OpenAI(api_key=self.openai_api_key)
            
            # ê¸°ì‚¬ ì œëª©ê³¼ ìš”ì•½ ìˆ˜ì§‘
            article_summaries = []
            for i, article in enumerate(articles[:10], 1):  # ìµœëŒ€ 10ê°œ ê¸°ì‚¬
                title = article.get('title', '')
                summary = article.get('summary', '')
                sentiment = article.get('sentiment', 'ì¤‘ë¦½')
                if title:
                    article_summaries.append(f"{i}. [{sentiment}] {title}")
                    if summary:
                        article_summaries.append(f"   ìš”ì•½: {summary[:100]}...")
            
            articles_text = "\n".join(article_summaries)
            
            # ê°ì • ë¶„í¬ ì •ë³´
            total = sum(sentiment_distribution.values())
            sentiment_info = f"""
ê°ì • ë¶„í¬:
- ê¸ì •: {sentiment_distribution.get('positive', 0)}ê°œ ({sentiment_distribution.get('positive', 0)/total*100:.1f}% if total > 0 else 0)
- ë¶€ì •: {sentiment_distribution.get('negative', 0)}ê°œ ({sentiment_distribution.get('negative', 0)/total*100:.1f}% if total > 0 else 0)
- ì¤‘ë¦½: {sentiment_distribution.get('neutral', 0)}ê°œ ({sentiment_distribution.get('neutral', 0)/total*100:.1f}% if total > 0 else 0)
"""
            
            prompt = f"""'{keyword}' í‚¤ì›Œë“œì— ëŒ€í•œ ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•© ìš”ì•½í•´ì£¼ì„¸ìš”.

ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(articles)}ê°œ

{sentiment_info}

ì£¼ìš” ê¸°ì‚¬ ëª©ë¡:
{articles_text}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ:
1. ì „ë°˜ì ì¸ ì—¬ë¡  ë™í–¥ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½)
2. ì£¼ìš” ìŸì  ë° ì´ìŠˆ
3. í–¥í›„ ì „ë§ ë˜ëŠ” ì‹œì‚¬ì 

ì„ 5-7ë¬¸ì¥ìœ¼ë¡œ ì¢…í•© ìš”ì•½í•´ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."""
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ì„í•˜ì—¬ ì¢…í•©ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=500,
                temperature=0.3
            )
            
            result["summary"] = response.choices[0].message.content.strip()
            
            # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
            if response.usage:
                result["usage"] = {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            
            return result
        except Exception as e:
            safe_log("ì¢…í•© ìš”ì•½ ìƒì„± ì‹¤íŒ¨", level="warning", error=str(e))
            return result

    def get_conversation_history(self) -> List[Dict]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        if self.memory:
            return [{"role": "user" if isinstance(m, HumanMessage) else "assistant", "content": m.content} for m in self.memory.messages]
        return []


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ë¹„ë™ê¸°)"""
    print("ğŸš€ News Analysis Agent í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)

    try:
        config = get_config()
        agent = NewsAnalysisAgent(config.get_openai_key())

        # í…ŒìŠ¤íŠ¸: ë¹„ë™ê¸° ë¶„ì„
        print("\nğŸ“ ë¹„ë™ê¸° ë‰´ìŠ¤ ë¶„ì„ í…ŒìŠ¤íŠ¸:")
        result = await agent.analyze_news_async(
            keyword="AI",
            sources=["ë„¤ì´ë²„", "êµ¬ê¸€"],
            max_articles=5
        )

        if "error" in result:
            print(f"âŒ ì˜¤ë¥˜: {result['error']}")
        else:
            print(f"âœ… ë¶„ì„ ì™„ë£Œ:")
            print(f"   - ì´ ê¸°ì‚¬ ìˆ˜: {result['total_articles']}")
            print(f"   - ê°ì„± ë¶„í¬: {result['sentiment_distribution']}")
            print(f"   - í‚¤ì›Œë“œ ìˆ˜: {len(result['keywords'])}")

        # í…ŒìŠ¤íŠ¸: ìì—°ì–´ ì§ˆì˜
        if agent.agent:
            print("\nğŸ“ ìì—°ì–´ ì§ˆì˜ í…ŒìŠ¤íŠ¸:")
            response = agent.analyze_news_sentiment("AI ê¸°ìˆ ì— ëŒ€í•œ ìµœê·¼ ë‰´ìŠ¤ì˜ ì—¬ë¡ ì„ ë¶„ì„í•´ì¤˜")
            print(f"âœ… ì‘ë‹µ: {response[:200]}...")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    asyncio.run(main())
