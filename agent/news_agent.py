"""
News Analysis Agent

ë‰´ìŠ¤ ê°ì„± ë¶„ì„ì„ ìœ„í•œ í†µí•© AI Agent
ë„¤ì´ë²„ ë‰´ìŠ¤ì™€ êµ¬ê¸€ ë‰´ìŠ¤ë¥¼ ì§€ì›í•˜ë©°, ì‹¤ì œ Toolsë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

import json
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime

# LangChain import (ìµœì‹  ë²„ì „ 1.2.0 í˜¸í™˜)
try:
    # LangGraph ê¸°ë°˜ ReAct Agent (ìµœì‹  ë°©ì‹)
    from langgraph.prebuilt import create_react_agent
    from langchain_openai import ChatOpenAI
    from langchain_core.chat_history import InMemoryChatMessageHistory
    from langchain_core.messages import HumanMessage, AIMessage
    AGENT_AVAILABLE = True
    USE_LANGGRAPH = True
except ImportError:
    try:
        # ëŒ€ì²´: LangChainì˜ ë‹¤ë¥¸ ë°©ì‹ ì‹œë„
        from langchain_openai import ChatOpenAI
        from langchain_core.chat_history import InMemoryChatMessageHistory
        AGENT_AVAILABLE = True
        USE_LANGGRAPH = False
    except ImportError:
        AGENT_AVAILABLE = False
        USE_LANGGRAPH = False

from common.config import get_config
from common.utils import safe_log, validate_input
from agent.tools import scrape_news, analyze_sentiment, analyze_sentiment_func, analyze_news_trend, analyze_news_trend_func
from agent.tools.news_scraper import NewsScraperTool


class NewsAnalysisAgent:
    """ë‰´ìŠ¤ ê°ì„± ë¶„ì„ì„ ìœ„í•œ í†µí•© AI Agent"""

    def __init__(self, api_key: Optional[str] = None):
        """
        Agent ì´ˆê¸°í™”

        Args:
            api_key: OpenAI API í‚¤ (Noneì´ë©´ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ìŒ)
        """
        config = get_config()
        self.openai_api_key = api_key or config.get_openai_key()

        if not self.openai_api_key:
            raise RuntimeError("OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        # LangChain AgentëŠ” ì„ íƒì  (analyze_news_sentimentì—ì„œë§Œ ì‚¬ìš©)
        # analyze_news_asyncëŠ” LangChain ì—†ì´ë„ ì‘ë™
        self.agent = None
        self.llm = None
        self.memory = None

        if AGENT_AVAILABLE:
            try:
                # LLM ì´ˆê¸°í™”
                self.llm = ChatOpenAI(
                    temperature=0.1,
                    openai_api_key=self.openai_api_key,
                    max_tokens=2000,
                    model="gpt-4o-mini"
                )

                # ë©”ëª¨ë¦¬ ì„¤ì • (ìµœì‹  ë°©ì‹)
                try:
                    self.memory = InMemoryChatMessageHistory()
                except Exception as e:
                    safe_log("ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨", level="warning", error=str(e))
                    self.memory = None

                # Tools ë“±ë¡ (ì‹¤ì œ Tools ì‚¬ìš©)
                self.tools = [
                    scrape_news,
                    analyze_sentiment,
                    analyze_news_trend,
                ]

                # Agent ì´ˆê¸°í™” (LangGraph ë°©ì‹)
                if USE_LANGGRAPH and create_react_agent:
                    try:
                        # create_react_agentëŠ” modelê³¼ toolsë§Œ í•„ìš”
                        self.agent = create_react_agent(
                            model=self.llm,
                            tools=self.tools
                        )
                        safe_log("NewsAnalysisAgent ì´ˆê¸°í™” ì™„ë£Œ (LangGraph Agent í¬í•¨)", level="info", tools_count=len(self.tools))
                    except Exception as e:
                        safe_log("LangGraph Agent ì´ˆê¸°í™” ì‹¤íŒ¨ (ê³„ì† ì§„í–‰)", level="warning", error=str(e))
                        self.agent = None
                else:
                    safe_log("LangGraphë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ (analyze_news_asyncë§Œ ì‚¬ìš© ê°€ëŠ¥)", level="warning")
                    self.agent = None

            except Exception as e:
                safe_log("LangChain ì´ˆê¸°í™” ì‹¤íŒ¨ (ê³„ì† ì§„í–‰)", level="warning", error=str(e))
                # LangChain ì—†ì´ë„ analyze_news_asyncëŠ” ì‘ë™ ê°€ëŠ¥
        else:
            safe_log("LangChainì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ (analyze_news_asyncë§Œ ì‚¬ìš© ê°€ëŠ¥)", level="warning")

    async def analyze_news_async(
        self,
        keyword: str,
        sources: List[str] = None,
        max_articles: int = 10
    ) -> Dict[str, Any]:
        """
        ë¹„ë™ê¸° ë‰´ìŠ¤ ë¶„ì„ ì‹¤í–‰

        Args:
            keyword: ê²€ìƒ‰í•  í‚¤ì›Œë“œ
            sources: ë‰´ìŠ¤ ì†ŒìŠ¤ ëª©ë¡ (["ë„¤ì´ë²„", "êµ¬ê¸€"])
            max_articles: ìµœëŒ€ ê¸°ì‚¬ ìˆ˜

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if sources is None:
            sources = ["ë„¤ì´ë²„"]

        # ì…ë ¥ ê²€ì¦
        if not validate_input(keyword, max_length=100):
            raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ í‚¤ì›Œë“œì…ë‹ˆë‹¤.")

        safe_log("ë‰´ìŠ¤ ë¶„ì„ ì‹œì‘", level="info", keyword=keyword, sources=sources)

        try:
            # 1ë‹¨ê³„: ë‰´ìŠ¤ ìˆ˜ì§‘
            # scrape_newsê°€ @tool ë°ì½”ë ˆì´í„°ë¡œ ì¥ì‹ë˜ì–´ ìˆì–´ ì§ì ‘ í˜¸ì¶œ ë¶ˆê°€
            # NewsScraperToolì„ ì§ì ‘ ì‚¬ìš©
            scraper = NewsScraperTool()
            articles_data = []
            
            try:
                # ì†ŒìŠ¤ í•„í„°ë§ ë° ë§¤í•‘ (ë‹¤ì–‘í•œ ì†ŒìŠ¤ ì´ë¦„ ì§€ì›)
                source_mapping = {
                    "ë„¤ì´ë²„": "ë„¤ì´ë²„",
                    "naver": "ë„¤ì´ë²„",
                    "êµ¬ê¸€": "êµ¬ê¸€",
                    "google": "êµ¬ê¸€",
                }
                
                # ì§€ì›í•˜ì§€ ì•ŠëŠ” ì†ŒìŠ¤ ëª©ë¡ (ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€ìš©)
                unsupported_sources = ["ë‹¤ìŒ", "Daum", "KBS", "SBS", "MBC", "YTN", "JTBC", "ì—°í•©ë‰´ìŠ¤"]
                
                valid_sources = []
                rejected_sources = []  # ì§€ì›í•˜ì§€ ì•ŠëŠ” ì†ŒìŠ¤ ì¶”ì 
                
                for source in (sources or ["ë„¤ì´ë²„"]):
                    # ì§€ì›í•˜ì§€ ì•ŠëŠ” ì†ŒìŠ¤ í™•ì¸
                    if source in unsupported_sources:
                        rejected_sources.append(source)
                        safe_log("ì§€ì›í•˜ì§€ ì•ŠëŠ” ë‰´ìŠ¤ ì†ŒìŠ¤", level="warning", source=source)
                        continue
                    
                    normalized_source = source_mapping.get(source, None)
                    if normalized_source:
                        if normalized_source not in valid_sources:
                            valid_sources.append(normalized_source)
                        if source != normalized_source:
                            safe_log(f"ì†ŒìŠ¤ ë§¤í•‘: {source} -> {normalized_source}", level="info")
                    else:
                        # ì•Œ ìˆ˜ ì—†ëŠ” ì†ŒìŠ¤
                        rejected_sources.append(source)
                        safe_log("ì•Œ ìˆ˜ ì—†ëŠ” ë‰´ìŠ¤ ì†ŒìŠ¤", level="warning", source=source)
                
                # ì§€ì›í•˜ì§€ ì•ŠëŠ” ì†ŒìŠ¤ë§Œ ì„ íƒí•œ ê²½ìš° ì—ëŸ¬ ë°˜í™˜
                if not valid_sources and rejected_sources:
                    return {
                        "error": f"ì„ íƒí•œ ë‰´ìŠ¤ ì†ŒìŠ¤({', '.join(rejected_sources)})ëŠ” í˜„ì¬ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë„¤ì´ë²„ ë˜ëŠ” êµ¬ê¸€ì„ ì„ íƒí•´ì£¼ì„¸ìš”.",
                        "keyword": keyword,
                        "rejected_sources": rejected_sources,
                        "supported_sources": ["ë„¤ì´ë²„", "êµ¬ê¸€"]
                    }
                
                if not valid_sources:
                    valid_sources = ["ë„¤ì´ë²„"]  # ê¸°ë³¸ê°’
                
                # ë‰´ìŠ¤ ê²€ìƒ‰ ë° í¬ë¡¤ë§ (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
                import asyncio
                try:
                    # ì „ì²´ í¬ë¡¤ë§ì— ìµœëŒ€ 2ë¶„ ì œí•œ (ê° ì†ŒìŠ¤ë³„ë¡œ 60ì´ˆ)
                    article_urls = await asyncio.wait_for(
                        asyncio.to_thread(scraper.search_news, keyword, valid_sources, max_articles),
                        timeout=120  # 2ë¶„
                    )
                except asyncio.TimeoutError:
                    safe_log("ë‰´ìŠ¤ ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ (2ë¶„ ì´ˆê³¼)", level="warning", keyword=keyword, sources=valid_sources)
                    return {
                        "error": f"'{keyword}' í‚¤ì›Œë“œë¡œ ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘ ì‹œê°„ ì´ˆê³¼ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                        "keyword": keyword,
                        "sources": valid_sources
                    }
                
                if not article_urls:
                    return {
                        "error": f"'{keyword}' í‚¤ì›Œë“œë¡œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        "keyword": keyword,
                        "sources": valid_sources
                    }
                
                # ê° ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                for i, url in enumerate(article_urls, 1):
                    safe_log(f"ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ({i}/{len(article_urls)})", level="info")
                    
                    # URLì—ì„œ ì†ŒìŠ¤ íŒë‹¨
                    source = "naver" if "naver.com" in url else "google"
                    
                    try:
                        article = scraper.scrape_article(url, source)
                        article_dict = article.to_dict()
                        article_dict["keyword"] = keyword
                        article_dict["source"] = "ë„¤ì´ë²„" if source == "naver" else "êµ¬ê¸€"
                        articles_data.append(article_dict)
                    except Exception as e:
                        safe_log(f"ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {url}", level="warning", error=str(e))
                        continue
                    
                    # Rate Limit ì¤€ìˆ˜
                    import time
                    time.sleep(1)
                    
            finally:
                scraper.cleanup()

            if not articles_data or (len(articles_data) == 1 and "error" in articles_data[0]):
                return {
                    "error": articles_data[0].get("error", "ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨") if articles_data else "ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨",
                    "keyword": keyword,
                    "sources": sources
                }

            # 2ë‹¨ê³„: ê° ê¸°ì‚¬ ë° ëŒ“ê¸€ ê°ì„± ë¶„ì„
            analyzed_articles = []
            all_comments = []

            for article in articles_data:
                if "error" in article:
                    safe_log("ê¸°ì‚¬ ìŠ¤í‚µ (ì—ëŸ¬ í¬í•¨)", level="warning", error=article.get("error"))
                    continue

                # ê¸°ì‚¬ ë³¸ë¬¸ ê°ì„± ë¶„ì„
                article_text = f"{article.get('title', '')} {article.get('content', '')}"
                
                try:
                    # analyze_sentiment_func ì‚¬ìš© (ì§ì ‘ í˜¸ì¶œ ê°€ëŠ¥í•œ í•¨ìˆ˜)
                    article_sentiment = analyze_sentiment_func(article_text[:500])  # ìµœëŒ€ 500ì
                except Exception as e:
                    safe_log("ê¸°ì‚¬ ê°ì„± ë¶„ì„ ì‹¤íŒ¨", level="error", error=str(e))
                    article_sentiment = {
                        "sentiment": "ì¤‘ë¦½",
                        "sentiment_score": 0.0,
                        "sentiment_label": "neutral",
                        "confidence": 0.0
                    }

                # ëŒ“ê¸€ ê°ì„± ë¶„ì„
                article_comments = article.get("comments", [])
                analyzed_comments = []

                for comment in article_comments[:10]:  # ìµœëŒ€ 10ê°œ ëŒ“ê¸€
                    comment_text = comment.get("text", "") if isinstance(comment, dict) else str(comment)
                    if comment_text:
                        try:
                            # analyze_sentiment_func ì‚¬ìš© (ì§ì ‘ í˜¸ì¶œ ê°€ëŠ¥í•œ í•¨ìˆ˜)
                            comment_sentiment = analyze_sentiment_func(comment_text)
                            # ëŒ“ê¸€ ë°ì´í„°ì™€ ê°ì„± ë¶„ì„ ê²°ê³¼ ë³‘í•©
                            comment_data = comment if isinstance(comment, dict) else {"text": comment}
                            analyzed_comments.append({
                                **comment_data,
                                **comment_sentiment
                            })
                            all_comments.append(comment_text)
                        except Exception as e:
                            safe_log("ëŒ“ê¸€ ê°ì„± ë¶„ì„ ì‹¤íŒ¨", level="warning", error=str(e))
                            continue

                analyzed_articles.append({
                    **article,
                    **article_sentiment,
                    "comments": analyzed_comments,
                    "comment_count": len(analyzed_comments)
                })

            # 3ë‹¨ê³„: ì „ì²´ ë™í–¥ ë¶„ì„
            if all_comments:
                # analyze_news_trend_func ì‚¬ìš© (ì§ì ‘ í˜¸ì¶œ ê°€ëŠ¥í•œ í•¨ìˆ˜)
                trend_result = analyze_news_trend_func(
                    [{"text": c} for c in all_comments],
                    keyword
                )
            else:
                trend_result = {
                    "keyword": keyword,
                    "overall_sentiment": "ì¤‘ë¦½",
                    "sentiment_distribution": {"ê¸ì •": 0.33, "ë¶€ì •": 0.33, "ì¤‘ë¦½": 0.34},
                    "key_topics": [],
                    "summary": "ëŒ“ê¸€ì´ ì—†ì–´ ë™í–¥ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "total_comments": 0
                }

            # 4ë‹¨ê³„: ê°ì„± ë¶„í¬ ê³„ì‚°
            sentiment_distribution = self._calculate_sentiment_distribution(analyzed_articles)

            # 5ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords(analyzed_articles, keyword)

            result = {
                "keyword": keyword,
                "sources": sources,
                "total_articles": len(analyzed_articles),
                "articles": analyzed_articles,
                "sentiment_distribution": sentiment_distribution,
                "trend_analysis": trend_result,
                "keywords": keywords,
                "analyzed_at": datetime.now().isoformat()
            }

            safe_log("ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ", level="info", total_articles=len(analyzed_articles))
            return result

        except Exception as e:
            safe_log("ë‰´ìŠ¤ ë¶„ì„ ì˜¤ë¥˜", level="error", error=str(e))
            return {
                "error": f"ë‰´ìŠ¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}",
                "keyword": keyword,
                "sources": sources
            }

    def analyze_news_sentiment(self, user_query: str) -> str:
        """
        ìì—°ì–´ ì§ˆì˜ë¥¼ ë°›ì•„ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ìˆ˜í–‰

        Args:
            user_query: ì‚¬ìš©ì ì§ˆì˜ (ì˜ˆ: "AI ê¸°ìˆ ì— ëŒ€í•œ ìµœê·¼ ë‰´ìŠ¤ì˜ ì—¬ë¡ ì„ ë¶„ì„í•´ì¤˜")

        Returns:
            Agent ì‘ë‹µ ë¬¸ìì—´
        """
        if not self.agent:
            return "LangChain Agentê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. analyze_news_asyncë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."

        if not validate_input(user_query, max_length=500):
            return "ìœ íš¨í•˜ì§€ ì•Šì€ ì§ˆì˜ì…ë‹ˆë‹¤."

        safe_log("Agent ì‹¤í–‰ ì‹œì‘", level="info", query=user_query[:50])

        try:
            # LangGraph Agent ì‹¤í–‰ (ìµœì‹  ë°©ì‹)
            if USE_LANGGRAPH:
                from langchain_core.messages import HumanMessage
                messages = [HumanMessage(content=user_query)]
                if self.memory:
                    messages = list(self.memory.messages) + messages
                
                response = self.agent.invoke({"messages": messages})
                
                # ë©”ëª¨ë¦¬ì— ì‘ë‹µ ì €ì¥
                if self.memory:
                    self.memory.add_message(HumanMessage(content=user_query))
                    if isinstance(response, dict) and "messages" in response:
                        self.memory.add_messages(response["messages"][-1:])
                
                # ì‘ë‹µ ì¶”ì¶œ
                if isinstance(response, dict) and "messages" in response:
                    last_message = response["messages"][-1]
                    result = last_message.content if hasattr(last_message, "content") else str(last_message)
                else:
                    result = str(response)
            else:
                # ëŒ€ì²´ ë°©ì‹ (ì—†ìœ¼ë©´ ì—ëŸ¬)
                result = "LangGraph Agentë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            safe_log("Agent ì‹¤í–‰ ì™„ë£Œ", level="info")
            return result
        except Exception as e:
            error_msg = f"Agent ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"
            safe_log("Agent ì‹¤í–‰ ì˜¤ë¥˜", level="error", error=str(e))
            return error_msg

    def _calculate_sentiment_distribution(self, articles: List[Dict]) -> Dict[str, int]:
        """ê°ì„± ë¶„í¬ ê³„ì‚°"""
        distribution = {"positive": 0, "negative": 0, "neutral": 0}

        for article in articles:
            sentiment = article.get("sentiment", "ì¤‘ë¦½")
            if sentiment == "ê¸ì •":
                distribution["positive"] += 1
            elif sentiment == "ë¶€ì •":
                distribution["negative"] += 1
            else:
                distribution["neutral"] += 1

        return distribution

    def _extract_keywords(self, articles: List[Dict], main_keyword: str) -> List[Dict]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¹ˆë„ ê³„ì‚°"""
        keyword_freq = {}

        for article in articles:
            text = f"{article.get('title', '')} {article.get('content', '')}"
            words = text.split()

            for word in words:
                if len(word) > 1 and word != main_keyword:
                    keyword_freq[word] = keyword_freq.get(word, 0) + 1

        # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ ë°˜í™˜
        sorted_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)[:10]

        return [
            {
                "keyword": keyword,
                "frequency": freq
            }
            for keyword, freq in sorted_keywords
        ]

    def get_conversation_history(self) -> List[Dict]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        if self.memory:
            return [{"role": "user" if isinstance(m, HumanMessage) else "assistant", "content": m.content} for m in self.memory.messages]
        return []


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ë¹„ë™ê¸°)"""
    print("ğŸš€ News Analysis Agent í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)

    try:
        config = get_config()
        agent = NewsAnalysisAgent(config.get_openai_key())

        # í…ŒìŠ¤íŠ¸: ë¹„ë™ê¸° ë¶„ì„
        print("\nğŸ“ ë¹„ë™ê¸° ë‰´ìŠ¤ ë¶„ì„ í…ŒìŠ¤íŠ¸:")
        result = await agent.analyze_news_async(
            keyword="AI",
            sources=["ë„¤ì´ë²„", "êµ¬ê¸€"],
            max_articles=5
        )

        if "error" in result:
            print(f"âŒ ì˜¤ë¥˜: {result['error']}")
        else:
            print(f"âœ… ë¶„ì„ ì™„ë£Œ:")
            print(f"   - ì´ ê¸°ì‚¬ ìˆ˜: {result['total_articles']}")
            print(f"   - ê°ì„± ë¶„í¬: {result['sentiment_distribution']}")
            print(f"   - í‚¤ì›Œë“œ ìˆ˜: {len(result['keywords'])}")

        # í…ŒìŠ¤íŠ¸: ìì—°ì–´ ì§ˆì˜
        if agent.agent:
            print("\nğŸ“ ìì—°ì–´ ì§ˆì˜ í…ŒìŠ¤íŠ¸:")
            response = agent.analyze_news_sentiment("AI ê¸°ìˆ ì— ëŒ€í•œ ìµœê·¼ ë‰´ìŠ¤ì˜ ì—¬ë¡ ì„ ë¶„ì„í•´ì¤˜")
            print(f"âœ… ì‘ë‹µ: {response[:200]}...")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    asyncio.run(main())
