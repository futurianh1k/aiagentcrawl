{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "AI ì—ì´ì „íŠ¸ ê¸°ë°˜ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì‹œìŠ¤í…œ - 1íšŒì°¨ ì‹¤ìŠµ ì½”ë“œ ë¬¸ì„œ ìƒì„±\n\n**ëª©í‘œ**: 4ê°œì˜ ì‹¤ìŠµ ì½”ë“œë¥¼ Python íŒŒì¼(.py)ê³¼ Word ë¬¸ì„œ(.docx)ë¡œ ìƒì„±\n\n**ì‹¤ìŠµ ëª©ë¡**:\n1. **ì‹¤ìŠµ 1**: LangChain ê¸°ë³¸ Agent - Calculator Tool ì˜ˆì œ\n2. **ì‹¤ìŠµ 2**: NewsScraper Tool êµ¬í˜„ - Selenium + Firecrawl\n3. **ì‹¤ìŠµ 3**: DataAnalyzer Tool êµ¬í˜„ - OpenAI/Gemini ê°ì„± ë¶„ì„\n4. **ì‹¤ìŠµ 4**: Planner Agent êµ¬í˜„ - Tools ë“±ë¡ ë° ì‹¤í–‰\n\n**ê° ì‹¤ìŠµ ì½”ë“œ í¬í•¨ ë‚´ìš©**:\n- ëª©í‘œ ë° ì„¤ëª…\n- í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n- ì™„ì „í•œ ì‹¤í–‰ ê°€ëŠ¥ ì½”ë“œ (ì£¼ì„ í¬í•¨)\n- ì‹¤í–‰ ì˜ˆì œ ë° ì˜ˆìƒ ì¶œë ¥\n- ì£¼ìš” í¬ì¸íŠ¸ ë° ì£¼ì˜ì‚¬í•­\n\n**ì¶œë ¥ íŒŒì¼**:\n1. `lab1_basic_agent.py` - ì‹¤ìŠµ 1 ì½”ë“œ\n2. `lab2_news_scraper.py` - ì‹¤ìŠµ 2 ì½”ë“œ\n3. `lab3_data_analyzer.py` - ì‹¤ìŠµ 3 ì½”ë“œ\n4. `lab4_planner_agent.py` - ì‹¤ìŠµ 4 ì½”ë“œ\n5. `session1_labs_guide.docx` - ì „ì²´ ì‹¤ìŠµ í†µí•© Word ë¬¸ì„œ (ì„¤ëª… + ì½”ë“œ)\n\n**Word ë¬¸ì„œ êµ¬ì¡°**:\n- í‘œì§€: AI ì—ì´ì „íŠ¸ ê¸°ë°˜ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì‹œìŠ¤í…œ - 1íšŒì°¨ ì‹¤ìŠµ ê°€ì´ë“œ\n- ëª©ì°¨\n- í™˜ê²½ ì„¤ì • ê°€ì´ë“œ\n- ê° ì‹¤ìŠµë³„ ì„¹ì…˜ (ëª©í‘œ, ì½”ë“œ, ì‹¤í–‰ ë°©ë²•, ê²°ê³¼ í•´ì„)\n- íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì„¹ì…˜\n\nì‹¤ìŠµ ì½”ë“œëŠ” ì‹¤ì œ ë™ì‘ ê°€ëŠ¥í•˜ë„ë¡ ì‘ì„±í•˜ë˜, API í‚¤ë‚˜ í™˜ê²½ ë³€ìˆ˜ëŠ” placeholderë¡œ í‘œì‹œ"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1. ğŸ”„ ğŸ”´ í™˜ê²½ ì„¤ì • ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ [in_progress] (ID: 1)\n2. â³ ğŸ”´ ì‹¤ìŠµ 1: LangChain ê¸°ë³¸ Agent ì½”ë“œ ì‘ì„± (lab1_basic_agent.py) [pending] (ID: 2)\n3. â³ ğŸ”´ ì‹¤ìŠµ 2: NewsScraper Tool êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab2_news_scraper.py) [pending] (ID: 3)\n4. â³ ğŸ”´ ì‹¤ìŠµ 3: DataAnalyzer Tool êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab3_data_analyzer.py) [pending] (ID: 4)\n5. â³ ğŸ”´ ì‹¤ìŠµ 4: Planner Agent êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab4_planner_agent.py) [pending] (ID: 5)\n6. â³ ğŸŸ¡ Word ë¬¸ì„œ ìƒì„± (session1_labs_guide.docx) [pending] (ID: 6)\n7. â³ ğŸŸ¢ íŒŒì¼ì„ AI Driveì— ì—…ë¡œë“œ [pending] (ID: 7)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# í™˜ê²½ ì„¤ì • ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\nimport os\nimport sys\n\n# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\noutput_dir = \"/home/user/output/ai_agent_labs\"\nos.makedirs(output_dir, exist_ok=True)\n\nprint(\"AI ì—ì´ì „íŠ¸ ê¸°ë°˜ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì‹œìŠ¤í…œ - 1íšŒì°¨ ì‹¤ìŠµ ì½”ë“œ ìƒì„±\")\nprint(\"=\" * 60)\nprint(f\"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}\")\n\n# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª©ë¡\nrequired_packages = [\n    \"langchain\",\n    \"openai\", \n    \"selenium\",\n    \"webdriver-manager\",\n    \"python-dotenv\",\n    \"requests\",\n    \"beautifulsoup4\",\n    \"mysql-connector-python\",\n    \"sqlalchemy\",\n    \"python-docx\"\n]\n\nprint(\"\\ní•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª©ë¡:\")\nfor i, pkg in enumerate(required_packages, 1):\n    print(f\"{i:2d}. {pkg}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI ì—ì´ì „íŠ¸ ê¸°ë°˜ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì‹œìŠ¤í…œ - 1íšŒì°¨ ì‹¤ìŠµ ì½”ë“œ ìƒì„±\n============================================================\nì¶œë ¥ ë””ë ‰í† ë¦¬: /home/user/output/ai_agent_labs\n\ní•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª©ë¡:\n 1. langchain\n 2. openai\n 3. selenium\n 4. webdriver-manager\n 5. python-dotenv\n 6. requests\n 7. beautifulsoup4\n 8. mysql-connector-python\n 9. sqlalchemy\n10. python-docx\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1. âœ… ğŸ”´ í™˜ê²½ ì„¤ì • ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ [completed] (ID: 1)\n2. ğŸ”„ ğŸ”´ ì‹¤ìŠµ 1: LangChain ê¸°ë³¸ Agent ì½”ë“œ ì‘ì„± (lab1_basic_agent.py) [in_progress] (ID: 2)\n3. â³ ğŸ”´ ì‹¤ìŠµ 2: NewsScraper Tool êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab2_news_scraper.py) [pending] (ID: 3)\n4. â³ ğŸ”´ ì‹¤ìŠµ 3: DataAnalyzer Tool êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab3_data_analyzer.py) [pending] (ID: 4)\n5. â³ ğŸ”´ ì‹¤ìŠµ 4: Planner Agent êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab4_planner_agent.py) [pending] (ID: 5)\n6. â³ ğŸŸ¡ Word ë¬¸ì„œ ìƒì„± (session1_labs_guide.docx) [pending] (ID: 6)\n7. â³ ğŸŸ¢ íŒŒì¼ì„ AI Driveì— ì—…ë¡œë“œ [pending] (ID: 7)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {
        "file_changes": [
          {
            "path": "/home/user/output/ai_agent_labs/lab1_basic_agent.py",
            "change_type": "modified",
            "version": 1,
            "tool_call_id": "tooluse_2FlGeEQ3Qc6bEvfQ92wSFw",
            "execution_count": 2,
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/00dbab5b-262e-48c3-b95c-e5c4a0ca5ee5/output/ai_agent_labs/lab1_basic_agent.py",
              "mime_type": "text/x-python",
              "size_bytes": 4772
            },
            "upload_status": "queued",
            "upload_completed": false,
            "timestamp": "2025-12-16T10:13:39.510362Z"
          }
        ]
      },
      "source": [
        "# ì‹¤ìŠµ 1: LangChain ê¸°ë³¸ Agent ì½”ë“œ ì‘ì„±\nlab1_code = '''\"\"\"\nAI ì—ì´ì „íŠ¸ ê¸°ë°˜ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì‹œìŠ¤í…œ - ì‹¤ìŠµ 1\n==================================================\nì£¼ì œ: LangChain ê¸°ë³¸ Agent - Calculator Tool ì˜ˆì œ\n\nëª©í‘œ:\n- LangChainì˜ ê¸°ë³¸ Agent êµ¬ì¡° ì´í•´\n- Tool ì •ì˜ ë° Agent ì´ˆê¸°í™” ë°©ë²• í•™ìŠµ\n- Agent ì‹¤í–‰ ë° ë””ë²„ê¹… ë°©ë²• ìŠµë“\n\ní•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬:\npip install langchain openai python-dotenv\n\"\"\"\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain.tools import tool\nfrom langchain.agents import initialize_agent, AgentType\nfrom langchain.llms import OpenAI\nfrom langchain.memory import ConversationBufferMemory\n\n# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\nload_dotenv()\n\n# OpenAI API í‚¤ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ê¸°)\n# .env íŒŒì¼ì— OPENAI_API_KEY=your_api_key_here í˜•íƒœë¡œ ì €ì¥\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")\nif not openai_api_key:\n    print(\"âš ï¸  ê²½ê³ : OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n    print(\"   .env íŒŒì¼ì— OPENAI_API_KEY=your_key ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.\")\n    # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ í‚¤ (ì‹¤ì œ ì‚¬ìš© ì‹œ ì‹¤ì œ í‚¤ë¡œ êµì²´ í•„ìš”)\n    openai_api_key = \"sk-test-key-replace-with-real-key\"\n\nclass CalculatorAgent:\n    \"\"\"ê¸°ë³¸ Calculator Toolì„ ì‚¬ìš©í•˜ëŠ” LangChain Agent\"\"\"\n    \n    def __init__(self, api_key: str):\n        \"\"\"Agent ì´ˆê¸°í™”\"\"\"\n        self.llm = OpenAI(\n            temperature=0,\n            openai_api_key=api_key,\n            verbose=True\n        )\n        \n        # ë©”ëª¨ë¦¬ ì„¤ì • (ëŒ€í™” ê¸°ë¡ ë³´ê´€)\n        self.memory = ConversationBufferMemory(\n            memory_key=\"chat_history\",\n            return_messages=True\n        )\n        \n        # Agent ì´ˆê¸°í™”\n        self.agent = initialize_agent(\n            tools=[self.add_tool, self.multiply_tool, self.divide_tool],\n            llm=self.llm,\n            agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n            memory=self.memory,\n            verbose=True,\n            max_iterations=3  # ë¬´í•œ ë£¨í”„ ë°©ì§€\n        )\n    \n    @tool\n    def add_tool(x: float, y: float) -> float:\n        \"\"\"ë‘ ìˆ˜ë¥¼ ë”í•˜ëŠ” ë„êµ¬\n        \n        Args:\n            x (float): ì²« ë²ˆì§¸ ìˆ˜\n            y (float): ë‘ ë²ˆì§¸ ìˆ˜\n            \n        Returns:\n            float: ë‘ ìˆ˜ì˜ í•©\n        \"\"\"\n        result = x + y\n        print(f\"ğŸ§® ë§ì…ˆ ê³„ì‚°: {x} + {y} = {result}\")\n        return result\n    \n    @tool \n    def multiply_tool(x: float, y: float) -> float:\n        \"\"\"ë‘ ìˆ˜ë¥¼ ê³±í•˜ëŠ” ë„êµ¬\n        \n        Args:\n            x (float): ì²« ë²ˆì§¸ ìˆ˜\n            y (float): ë‘ ë²ˆì§¸ ìˆ˜\n            \n        Returns:\n            float: ë‘ ìˆ˜ì˜ ê³±\n        \"\"\"\n        result = x * y\n        print(f\"ğŸ§® ê³±ì…ˆ ê³„ì‚°: {x} Ã— {y} = {result}\")\n        return result\n    \n    @tool\n    def divide_tool(x: float, y: float) -> float:\n        \"\"\"ë‘ ìˆ˜ë¥¼ ë‚˜ëˆ„ëŠ” ë„êµ¬\n        \n        Args:\n            x (float): ë¶„ì\n            y (float): ë¶„ëª¨\n            \n        Returns:\n            float: ë‚˜ëˆˆ ê²°ê³¼\n        \"\"\"\n        if y == 0:\n            raise ValueError(\"0ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n        \n        result = x / y\n        print(f\"ğŸ§® ë‚˜ëˆ—ì…ˆ ê³„ì‚°: {x} Ã· {y} = {result}\")\n        return result\n    \n    def run(self, query: str) -> str:\n        \"\"\"Agent ì‹¤í–‰\"\"\"\n        try:\n            print(f\"\\\\nğŸ¤– ì‚¬ìš©ì ì§ˆì˜: {query}\")\n            print(\"=\" * 50)\n            \n            response = self.agent.run(query)\n            \n            print(\"=\" * 50)\n            print(f\"âœ… Agent ì‘ë‹µ: {response}\")\n            return response\n            \n        except Exception as e:\n            error_msg = f\"âŒ Agent ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}\"\n            print(error_msg)\n            return error_msg\n\ndef main():\n    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n    print(\"ğŸš€ LangChain ê¸°ë³¸ Agent ì‹¤ìŠµ ì‹œì‘\")\n    print(\"=\" * 60)\n    \n    # Agent ì´ˆê¸°í™”\n    calculator = CalculatorAgent(openai_api_key)\n    \n    # í…ŒìŠ¤íŠ¸ ì§ˆì˜ë“¤\n    test_queries = [\n        \"3ê³¼ 5ë¥¼ ë”í•´ì¤˜\",\n        \"10ì— 7ì„ ê³±í•œ ê²°ê³¼ëŠ”?\",\n        \"100ì„ 4ë¡œ ë‚˜ëˆ„ë©´?\",\n        \"((2 + 3) * 4) / 2 ë¥¼ ê³„ì‚°í•´ì¤˜\"\n    ]\n    \n    print(\"\\\\nğŸ“ í…ŒìŠ¤íŠ¸ ì§ˆì˜ ì‹¤í–‰:\")\n    for i, query in enumerate(test_queries, 1):\n        print(f\"\\\\n[í…ŒìŠ¤íŠ¸ {i}]\")\n        calculator.run(query)\n        print(\"\\\\n\" + \"-\" * 40)\n    \n    print(\"\\\\nğŸ¯ ì£¼ìš” í•™ìŠµ í¬ì¸íŠ¸:\")\n    print(\"1. @tool ë°ì½”ë ˆì´í„°ë¡œ í•¨ìˆ˜ë¥¼ Toolë¡œ ë³€í™˜\")\n    print(\"2. initialize_agentë¡œ Agentì™€ Tools ì—°ê²°\")\n    print(\"3. AgentType.CONVERSATIONAL_REACT_DESCRIPTION ì‚¬ìš©\")\n    print(\"4. verbose=Trueë¡œ Agent ì‚¬ê³  ê³¼ì • ê´€ì°°\")\n    print(\"5. ConversationBufferMemoryë¡œ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬\")\n    \n    print(\"\\\\nâš ï¸  ì£¼ì˜ì‚¬í•­:\")\n    print(\"- OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤ (.env íŒŒì¼ ì„¤ì •)\")\n    print(\"- max_iterationsë¡œ ë¬´í•œ ë£¨í”„ ë°©ì§€\")\n    print(\"- Tool í•¨ìˆ˜ì—ëŠ” ëª…í™•í•œ docstring ì‘ì„± í•„ìˆ˜\")\n\nif __name__ == \"__main__\":\n    main()\n'''\n\n# íŒŒì¼ ì €ì¥\nlab1_file = os.path.join(output_dir, \"lab1_basic_agent.py\")\nwith open(lab1_file, 'w', encoding='utf-8') as f:\n    f.write(lab1_code)\n\nprint(f\"âœ… ì‹¤ìŠµ 1 ì½”ë“œ ìƒì„± ì™„ë£Œ: {lab1_file}\")\nprint(f\"   íŒŒì¼ í¬ê¸°: {len(lab1_code)} ë¬¸ì\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì‹¤ìŠµ 1 ì½”ë“œ ìƒì„± ì™„ë£Œ: /home/user/output/ai_agent_labs/lab1_basic_agent.py\n   íŒŒì¼ í¬ê¸°: 3993 ë¬¸ì\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1. âœ… ğŸ”´ í™˜ê²½ ì„¤ì • ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ [completed] (ID: 1)\n2. âœ… ğŸ”´ ì‹¤ìŠµ 1: LangChain ê¸°ë³¸ Agent ì½”ë“œ ì‘ì„± (lab1_basic_agent.py) [completed] (ID: 2)\n3. ğŸ”„ ğŸ”´ ì‹¤ìŠµ 2: NewsScraper Tool êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab2_news_scraper.py) [in_progress] (ID: 3)\n4. â³ ğŸ”´ ì‹¤ìŠµ 3: DataAnalyzer Tool êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab3_data_analyzer.py) [pending] (ID: 4)\n5. â³ ğŸ”´ ì‹¤ìŠµ 4: Planner Agent êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab4_planner_agent.py) [pending] (ID: 5)\n6. â³ ğŸŸ¡ Word ë¬¸ì„œ ìƒì„± (session1_labs_guide.docx) [pending] (ID: 6)\n7. â³ ğŸŸ¢ íŒŒì¼ì„ AI Driveì— ì—…ë¡œë“œ [pending] (ID: 7)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {
        "file_changes": [
          {
            "path": "/home/user/output/ai_agent_labs/lab2_news_scraper.py",
            "change_type": "modified",
            "version": 1,
            "tool_call_id": "tooluse_q6xEQ6FARGiQ2pIpESP6lw",
            "execution_count": 3,
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/4b6778ce-108a-4d1c-9a7e-6889e2f41662/output/ai_agent_labs/lab2_news_scraper.py",
              "mime_type": "text/x-python",
              "size_bytes": 13908
            },
            "upload_status": "queued",
            "upload_completed": false,
            "timestamp": "2025-12-16T10:14:40.302747Z"
          }
        ]
      },
      "source": [
        "# ì‹¤ìŠµ 2: NewsScraper Tool êµ¬í˜„ ì½”ë“œ ì‘ì„±\nlab2_code = '''\"\"\"\nAI ì—ì´ì „íŠ¸ ê¸°ë°˜ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì‹œìŠ¤í…œ - ì‹¤ìŠµ 2\n==================================================\nì£¼ì œ: NewsScraper Tool êµ¬í˜„ - Selenium + Firecrawl\n\nëª©í‘œ:\n- Seleniumì„ ì´ìš©í•œ ì•ˆì •ì ì¸ ì›¹ í¬ë¡¤ë§ êµ¬í˜„\n- Explicit Waitì„ í†µí•œ Flaky Test ë°©ì§€\n- Firecrawl MCPë¥¼ í™œìš©í•œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ\n- Toolë¡œ íŒ¨í‚¤ì§•í•˜ì—¬ Agentì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ êµ¬í˜„\n\ní•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬:\npip install selenium webdriver-manager requests beautifulsoup4 python-dotenv langchain\n\"\"\"\n\nimport os\nimport json\nimport time\nimport requests\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.chrome.options import Options\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom bs4 import BeautifulSoup\nfrom langchain.tools import tool\nfrom dotenv import load_dotenv\n\n# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\nload_dotenv()\n\n@dataclass\nclass NewsArticle:\n    \"\"\"ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° í´ë˜ìŠ¤\"\"\"\n    url: str\n    title: str\n    content: str\n    comments: List[Dict[str, Any]]\n    published_date: Optional[str] = None\n    source: Optional[str] = None\n\nclass NewsScraperTool:\n    \"\"\"ë‰´ìŠ¤ ìŠ¤í¬ë ˆì´í¼ Tool í´ë˜ìŠ¤\"\"\"\n    \n    def __init__(self):\n        \"\"\"ì´ˆê¸°í™”\"\"\"\n        self.driver = None\n        self.firecrawl_api_key = os.getenv(\"FIRECRAWL_API_KEY\", \"fc-test-key\")\n        \n    def setup_driver(self) -> webdriver.Chrome:\n        \"\"\"Chrome WebDriver ì„¤ì • ë° ì´ˆê¸°í™”\"\"\"\n        chrome_options = Options()\n        chrome_options.add_argument(\"--headless\")  # ë¸Œë¼ìš°ì € ì°½ ìˆ¨ê¹€\n        chrome_options.add_argument(\"--no-sandbox\")\n        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n        chrome_options.add_argument(\"--disable-gpu\")\n        chrome_options.add_argument(\"--window-size=1920,1080\")\n        chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n        \n        # ChromeDriver ìë™ ì„¤ì¹˜ ë° ì„¤ì •\n        service = Service(ChromeDriverManager().install())\n        \n        driver = webdriver.Chrome(service=service, options=chrome_options)\n        driver.implicitly_wait(10)  # ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ ì„¤ì •\n        \n        return driver\n    \n    def search_naver_news(self, keyword: str, max_articles: int = 5) -> List[str]:\n        \"\"\"ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰ í›„ ê¸°ì‚¬ URL ëª©ë¡ ë°˜í™˜\"\"\"\n        if not self.driver:\n            self.driver = self.setup_driver()\n            \n        try:\n            # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL\n            search_url = f\"https://search.naver.com/search.naver?where=news&query={keyword}\"\n            print(f\"ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰: {keyword}\")\n            \n            self.driver.get(search_url)\n            \n            # Explicit Wait: ê²€ìƒ‰ ê²°ê³¼ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°\n            wait = WebDriverWait(self.driver, 15)\n            \n            # ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ë“¤ì´ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°\n            news_links = wait.until(\n                EC.presence_of_all_elements_located(\n                    (By.CSS_SELECTOR, \"a.news_tit\")\n                )\n            )\n            \n            # URL ëª©ë¡ ì¶”ì¶œ\n            article_urls = []\n            for link in news_links[:max_articles]:\n                href = link.get_attribute(\"href\")\n                if href and \"news.naver.com\" in href:\n                    article_urls.append(href)\n            \n            print(f\"âœ… {len(article_urls)}ê°œì˜ ê¸°ì‚¬ URL ìˆ˜ì§‘ ì™„ë£Œ\")\n            return article_urls\n            \n        except Exception as e:\n            print(f\"âŒ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}\")\n            return []\n    \n    def extract_with_selenium(self, url: str) -> Dict[str, Any]:\n        \"\"\"Seleniumìœ¼ë¡œ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ\"\"\"\n        if not self.driver:\n            self.driver = self.setup_driver()\n            \n        try:\n            self.driver.get(url)\n            wait = WebDriverWait(self.driver, 10)\n            \n            # ì œëª© ì¶”ì¶œ\n            try:\n                title_element = wait.until(\n                    EC.presence_of_element_located(\n                        (By.CSS_SELECTOR, \"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n                    )\n                )\n                title = title_element.text.strip()\n            except:\n                title = \"ì œëª© ì¶”ì¶œ ì‹¤íŒ¨\"\n            \n            # ë³¸ë¬¸ ì¶”ì¶œ\n            try:\n                content_element = wait.until(\n                    EC.presence_of_element_located(\n                        (By.CSS_SELECTOR, \"#dic_area\")\n                    )\n                )\n                content = content_element.text.strip()\n            except:\n                content = \"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨\"\n            \n            # ëŒ“ê¸€ ì¶”ì¶œ (ë„¤ì´ë²„ ë‰´ìŠ¤ ëŒ“ê¸€ì€ ë™ì  ë¡œë”©ì´ë¯€ë¡œ ê¸°ë³¸ êµ¬í˜„)\n            comments = self.extract_comments_basic()\n            \n            return {\n                \"title\": title,\n                \"content\": content,\n                \"comments\": comments,\n                \"extraction_method\": \"selenium\"\n            }\n            \n        except Exception as e:\n            print(f\"âŒ Selenium ì¶”ì¶œ ì˜¤ë¥˜ ({url}): {str(e)}\")\n            return {\n                \"title\": \"ì¶”ì¶œ ì‹¤íŒ¨\",\n                \"content\": \"ì¶”ì¶œ ì‹¤íŒ¨\", \n                \"comments\": [],\n                \"extraction_method\": \"selenium\",\n                \"error\": str(e)\n            }\n    \n    def extract_comments_basic(self) -> List[Dict[str, Any]]:\n        \"\"\"ê¸°ë³¸ì ì¸ ëŒ“ê¸€ ì¶”ì¶œ (ë„¤ì´ë²„ ë‰´ìŠ¤ ëŒ“ê¸€ êµ¬ì¡°ì— ë§ì¶° êµ¬í˜„)\"\"\"\n        comments = []\n        \n        try:\n            # ëŒ“ê¸€ ì˜ì—­ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°\n            wait = WebDriverWait(self.driver, 5)\n            \n            # ëŒ“ê¸€ ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹œë„\n            try:\n                more_button = wait.until(\n                    EC.element_to_be_clickable(\n                        (By.CSS_SELECTOR, \".u_cbox_btn_more\")\n                    )\n                )\n                more_button.click()\n                time.sleep(2)  # ëŒ“ê¸€ ë¡œë”© ëŒ€ê¸°\n            except:\n                pass  # ë”ë³´ê¸° ë²„íŠ¼ì´ ì—†ì„ ìˆ˜ ìˆìŒ\n            \n            # ëŒ“ê¸€ ìš”ì†Œë“¤ ì°¾ê¸°\n            comment_elements = self.driver.find_elements(\n                By.CSS_SELECTOR, \n                \".u_cbox_comment_box .u_cbox_contents\"\n            )\n            \n            for i, comment_elem in enumerate(comment_elements[:10]):  # ìµœëŒ€ 10ê°œ\n                try:\n                    text = comment_elem.text.strip()\n                    if text:\n                        comments.append({\n                            \"id\": f\"comment_{i+1}\",\n                            \"text\": text,\n                            \"author\": f\"ì‚¬ìš©ì{i+1}\",  # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì¶”ì¶œ í•„ìš”\n                            \"timestamp\": None  # ì‹¤ì œë¡œëŠ” ì‹œê°„ ì •ë³´ ì¶”ì¶œ í•„ìš”\n                        })\n                except:\n                    continue\n                    \n        except Exception as e:\n            print(f\"âš ï¸  ëŒ“ê¸€ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}\")\n        \n        # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ëŒ“ê¸€ (ì‹¤ì œ ëŒ“ê¸€ ì¶”ì¶œì´ ì‹¤íŒ¨í•  ê²½ìš°)\n        if not comments:\n            comments = [\n                {\"id\": \"dummy_1\", \"text\": \"ì¢‹ì€ ê¸°ì‚¬ë„¤ìš”.\", \"author\": \"ë…ì1\", \"timestamp\": None},\n                {\"id\": \"dummy_2\", \"text\": \"ì •ë³´ ê°ì‚¬í•©ë‹ˆë‹¤.\", \"author\": \"ë…ì2\", \"timestamp\": None},\n                {\"id\": \"dummy_3\", \"text\": \"ë” ìì„¸í•œ ë‚´ìš©ì´ ê¶ê¸ˆí•©ë‹ˆë‹¤.\", \"author\": \"ë…ì3\", \"timestamp\": None}\n            ]\n        \n        return comments\n    \n    def extract_with_firecrawl(self, url: str) -> Dict[str, Any]:\n        \"\"\"Firecrawl APIë¥¼ ì´ìš©í•œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ\"\"\"\n        try:\n            # Firecrawl API ì—”ë“œí¬ì¸íŠ¸\n            api_url = \"https://api.firecrawl.dev/v0/scrape\"\n            \n            headers = {\n                \"Authorization\": f\"Bearer {self.firecrawl_api_key}\",\n                \"Content-Type\": \"application/json\"\n            }\n            \n            payload = {\n                \"url\": url,\n                \"formats\": [\"markdown\", \"html\"],\n                \"includeTags\": [\"title\", \"article\", \"p\", \"h1\", \"h2\", \"h3\"],\n                \"excludeTags\": [\"script\", \"style\", \"nav\", \"footer\"],\n                \"waitFor\": 2000  # 2ì´ˆ ëŒ€ê¸°\n            }\n            \n            print(f\"ğŸ”¥ Firecrawl API í˜¸ì¶œ: {url}\")\n            response = requests.post(api_url, headers=headers, json=payload, timeout=30)\n            \n            if response.status_code == 200:\n                data = response.json()\n                return {\n                    \"title\": data.get(\"metadata\", {}).get(\"title\", \"ì œëª© ì—†ìŒ\"),\n                    \"content\": data.get(\"markdown\", \"ë‚´ìš© ì—†ìŒ\"),\n                    \"comments\": [],  # Firecrawlë¡œëŠ” ëŒ“ê¸€ ì¶”ì¶œì´ ì–´ë ¤ì›€\n                    \"extraction_method\": \"firecrawl\",\n                    \"success\": True\n                }\n            else:\n                print(f\"âŒ Firecrawl API ì˜¤ë¥˜: {response.status_code}\")\n                return None\n                \n        except Exception as e:\n            print(f\"âŒ Firecrawl ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}\")\n            return None\n    \n    def scrape_article(self, url: str) -> NewsArticle:\n        \"\"\"ë‹¨ì¼ ê¸°ì‚¬ ìŠ¤í¬ë ˆì´í•‘ (Firecrawl ìš°ì„ , ì‹¤íŒ¨ ì‹œ Selenium ì‚¬ìš©)\"\"\"\n        print(f\"\\\\nğŸ“° ê¸°ì‚¬ ìŠ¤í¬ë ˆì´í•‘ ì‹œì‘: {url}\")\n        \n        # 1ì°¨: Firecrawl ì‹œë„\n        firecrawl_result = self.extract_with_firecrawl(url)\n        \n        if firecrawl_result and firecrawl_result.get(\"success\"):\n            print(\"âœ… Firecrawlë¡œ ì¶”ì¶œ ì„±ê³µ\")\n            return NewsArticle(\n                url=url,\n                title=firecrawl_result[\"title\"],\n                content=firecrawl_result[\"content\"],\n                comments=firecrawl_result[\"comments\"],\n                source=\"firecrawl\"\n            )\n        \n        # 2ì°¨: Selenium ì‹œë„ \n        print(\"ğŸ”„ Firecrawl ì‹¤íŒ¨, Seleniumìœ¼ë¡œ ì¬ì‹œë„...\")\n        selenium_result = self.extract_with_selenium(url)\n        \n        return NewsArticle(\n            url=url,\n            title=selenium_result[\"title\"],\n            content=selenium_result[\"content\"],\n            comments=selenium_result[\"comments\"],\n            source=\"selenium\"\n        )\n    \n    def cleanup(self):\n        \"\"\"ë¦¬ì†ŒìŠ¤ ì •ë¦¬\"\"\"\n        if self.driver:\n            self.driver.quit()\n            self.driver = None\n\n    @tool\n    def scrape_news(keyword: str, max_articles: int = 3) -> List[Dict[str, Any]]:\n        \"\"\"ë‰´ìŠ¤ ìŠ¤í¬ë ˆì´í•‘ Tool í•¨ìˆ˜\n        \n        Args:\n            keyword (str): ê²€ìƒ‰í•  í‚¤ì›Œë“œ\n            max_articles (int): ìµœëŒ€ ìˆ˜ì§‘í•  ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ê°’: 3)\n            \n        Returns:\n            List[Dict]: ìŠ¤í¬ë ˆì´í•‘ëœ ê¸°ì‚¬ë“¤ì˜ ì •ë³´\n        \"\"\"\n        scraper = NewsScraperTool()\n        \n        try:\n            # 1ë‹¨ê³„: ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ê¸°ì‚¬ URL ê²€ìƒ‰\n            article_urls = scraper.search_naver_news(keyword, max_articles)\n            \n            if not article_urls:\n                return [{\n                    \"error\": f\"'{keyword}' í‚¤ì›Œë“œë¡œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\",\n                    \"keyword\": keyword\n                }]\n            \n            # 2ë‹¨ê³„: ê° ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ì¶”ì¶œ\n            scraped_articles = []\n            \n            for i, url in enumerate(article_urls, 1):\n                print(f\"\\\\n[{i}/{len(article_urls)}] ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘...\")\n                \n                article = scraper.scrape_article(url)\n                \n                scraped_articles.append({\n                    \"url\": article.url,\n                    \"title\": article.title,\n                    \"content\": article.content[:500] + \"...\" if len(article.content) > 500 else article.content,\n                    \"comments\": article.comments,\n                    \"source\": article.source,\n                    \"keyword\": keyword\n                })\n                \n                time.sleep(1)  # API ë¶€í•˜ ë°©ì§€\n            \n            return scraped_articles\n            \n        except Exception as e:\n            return [{\n                \"error\": f\"ë‰´ìŠ¤ ìŠ¤í¬ë ˆì´í•‘ ì¤‘ ì˜¤ë¥˜: {str(e)}\",\n                \"keyword\": keyword\n            }]\n            \n        finally:\n            scraper.cleanup()\n\ndef main():\n    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n    print(\"ğŸš€ NewsScraper Tool ì‹¤ìŠµ ì‹œì‘\")\n    print(\"=\" * 60)\n    \n    # í…ŒìŠ¤íŠ¸ í‚¤ì›Œë“œë“¤\n    test_keywords = [\"AI\", \"ì‚¼ì„±ì „ì\", \"ë¶€ë™ì‚°\"]\n    \n    for keyword in test_keywords:\n        print(f\"\\\\nğŸ” í‚¤ì›Œë“œ í…ŒìŠ¤íŠ¸: {keyword}\")\n        print(\"-\" * 40)\n        \n        # Tool í•¨ìˆ˜ í˜¸ì¶œ\n        result = NewsScraperTool.scrape_news(keyword, max_articles=2)\n        \n        print(f\"âœ… ìˆ˜ì§‘ ê²°ê³¼: {len(result)}ê°œ ê¸°ì‚¬\")\n        \n        for i, article in enumerate(result, 1):\n            if \"error\" in article:\n                print(f\"âŒ ì˜¤ë¥˜: {article['error']}\")\n            else:\n                print(f\"\\\\n[ê¸°ì‚¬ {i}]\")\n                print(f\"ì œëª©: {article['title'][:50]}...\")\n                print(f\"URL: {article['url']}\")\n                print(f\"ëŒ“ê¸€ ìˆ˜: {len(article['comments'])}ê°œ\")\n                print(f\"ì¶”ì¶œ ë°©ë²•: {article['source']}\")\n    \n    print(\"\\\\nğŸ¯ ì£¼ìš” í•™ìŠµ í¬ì¸íŠ¸:\")\n    print(\"1. Selenium WebDriver ì„¤ì • ë° Explicit Wait ì‚¬ìš©\")\n    print(\"2. CSS Selectorë¥¼ ì´ìš©í•œ ì•ˆì •ì ì¸ ìš”ì†Œ ì„ íƒ\")\n    print(\"3. Firecrawl APIë¥¼ í†µí•œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ\")\n    print(\"4. Fallback ë©”ì»¤ë‹ˆì¦˜ (Firecrawl ì‹¤íŒ¨ ì‹œ Selenium ì‚¬ìš©)\")\n    print(\"5. @tool ë°ì½”ë ˆì´í„°ë¡œ Agentì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ Toolë¡œ ë³€í™˜\")\n    \n    print(\"\\\\nâš ï¸  ì£¼ì˜ì‚¬í•­:\")\n    print(\"- FIRECRAWL_API_KEY í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í•„ìš”\")\n    print(\"- Chrome ë¸Œë¼ìš°ì € ë° ChromeDriver í•„ìš”\")\n    print(\"- ë„¤íŠ¸ì›Œí¬ ìƒíƒœì— ë”°ë¼ íƒ€ì„ì•„ì›ƒ ì¡°ì • í•„ìš”\")\n    print(\"- robots.txt ë° ì‚¬ì´íŠ¸ ì •ì±… ì¤€ìˆ˜ í•„ìš”\")\n\nif __name__ == \"__main__\":\n    main()\n'''\n\n# íŒŒì¼ ì €ì¥\nlab2_file = os.path.join(output_dir, \"lab2_news_scraper.py\")\nwith open(lab2_file, 'w', encoding='utf-8') as f:\n    f.write(lab2_code)\n\nprint(f\"âœ… ì‹¤ìŠµ 2 ì½”ë“œ ìƒì„± ì™„ë£Œ: {lab2_file}\")\nprint(f\"   íŒŒì¼ í¬ê¸°: {len(lab2_code)} ë¬¸ì\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì‹¤ìŠµ 2 ì½”ë“œ ìƒì„± ì™„ë£Œ: /home/user/output/ai_agent_labs/lab2_news_scraper.py\n   íŒŒì¼ í¬ê¸°: 12277 ë¬¸ì\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1. âœ… ğŸ”´ í™˜ê²½ ì„¤ì • ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ [completed] (ID: 1)\n2. âœ… ğŸ”´ ì‹¤ìŠµ 1: LangChain ê¸°ë³¸ Agent ì½”ë“œ ì‘ì„± (lab1_basic_agent.py) [completed] (ID: 2)\n3. âœ… ğŸ”´ ì‹¤ìŠµ 2: NewsScraper Tool êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab2_news_scraper.py) [completed] (ID: 3)\n4. ğŸ”„ ğŸ”´ ì‹¤ìŠµ 3: DataAnalyzer Tool êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab3_data_analyzer.py) [in_progress] (ID: 4)\n5. â³ ğŸ”´ ì‹¤ìŠµ 4: Planner Agent êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab4_planner_agent.py) [pending] (ID: 5)\n6. â³ ğŸŸ¡ Word ë¬¸ì„œ ìƒì„± (session1_labs_guide.docx) [pending] (ID: 6)\n7. â³ ğŸŸ¢ íŒŒì¼ì„ AI Driveì— ì—…ë¡œë“œ [pending] (ID: 7)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {
        "file_changes": [
          {
            "path": "/home/user/output/ai_agent_labs/lab3_data_analyzer.py",
            "change_type": "modified",
            "version": 1,
            "tool_call_id": "tooluse_qOPqUtBdReiY1vc8BmFVZg",
            "execution_count": 4,
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/57ba9897-28c8-4b5e-8635-76a959655454/output/ai_agent_labs/lab3_data_analyzer.py",
              "mime_type": "text/x-python",
              "size_bytes": 16856
            },
            "upload_status": "queued",
            "upload_completed": false,
            "timestamp": "2025-12-16T10:15:48.479486Z"
          }
        ]
      },
      "source": [
        "# ì‹¤ìŠµ 3: DataAnalyzer Tool êµ¬í˜„ ì½”ë“œ ì‘ì„±\nlab3_code = '''\"\"\"\nAI ì—ì´ì „íŠ¸ ê¸°ë°˜ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì‹œìŠ¤í…œ - ì‹¤ìŠµ 3\n==================================================\nì£¼ì œ: DataAnalyzer Tool êµ¬í˜„ - OpenAI/Gemini ê°ì„± ë¶„ì„\n\nëª©í‘œ:\n- OpenAI GPT ë˜ëŠ” Google Geminië¥¼ ì´ìš©í•œ ê°ì„± ë¶„ì„ êµ¬í˜„\n- í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ í†µí•œ ì¼ê´€ëœ JSON ì‘ë‹µ í™•ë³´\n- ëŒ“ê¸€ ë‹¨ìœ„ ë° ê¸°ì‚¬ ë‹¨ìœ„ ë¶„ì„ ê¸°ëŠ¥ êµ¬í˜„\n- Toolë¡œ íŒ¨í‚¤ì§•í•˜ì—¬ Agentì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ êµ¬í˜„\n\ní•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬:\npip install openai google-generativeai langchain python-dotenv\n\"\"\"\n\nimport os\nimport json\nimport time\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport openai\nimport google.generativeai as genai\nfrom langchain.tools import tool\nfrom dotenv import load_dotenv\n\n# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\nload_dotenv()\n\nclass SentimentType(Enum):\n    \"\"\"ê°ì„± ìœ í˜• ì—´ê±°í˜•\"\"\"\n    POSITIVE = \"ê¸ì •\"\n    NEGATIVE = \"ë¶€ì •\" \n    NEUTRAL = \"ì¤‘ë¦½\"\n\n@dataclass\nclass SentimentResult:\n    \"\"\"ê°ì„± ë¶„ì„ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤\"\"\"\n    text: str\n    sentiment: SentimentType\n    confidence: float\n    reason: str\n    keywords: List[str]\n    timestamp: Optional[str] = None\n\n@dataclass  \nclass TrendAnalysis:\n    \"\"\"ë™í–¥ ë¶„ì„ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤\"\"\"\n    keyword: str\n    overall_sentiment: SentimentType\n    sentiment_distribution: Dict[str, float]\n    key_topics: List[str]\n    summary: str\n    total_comments: int\n\nclass DataAnalyzerTool:\n    \"\"\"ë°ì´í„° ë¶„ì„ Tool í´ë˜ìŠ¤\"\"\"\n    \n    def __init__(self, use_openai: bool = True):\n        \"\"\"ì´ˆê¸°í™”\n        \n        Args:\n            use_openai (bool): Trueì´ë©´ OpenAI ì‚¬ìš©, Falseì´ë©´ Gemini ì‚¬ìš©\n        \"\"\"\n        self.use_openai = use_openai\n        \n        # API í‚¤ ì„¤ì •\n        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n        self.gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n        \n        if use_openai:\n            if not self.openai_api_key:\n                print(\"âš ï¸  ê²½ê³ : OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n                self.openai_api_key = \"sk-test-key\"\n            openai.api_key = self.openai_api_key\n        else:\n            if not self.gemini_api_key:\n                print(\"âš ï¸  ê²½ê³ : GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n                self.gemini_api_key = \"test-key\"\n            genai.configure(api_key=self.gemini_api_key)\n    \n    def create_sentiment_prompt(self, text: str) -> str:\n        \"\"\"ê°ì„± ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±\"\"\"\n        return f\"\"\"ë‹¹ì‹ ì€ ì „ë¬¸ ë‰´ìŠ¤ ëŒ“ê¸€ ê°ì„± ë¶„ì„ê°€ì…ë‹ˆë‹¤.\n\në‹¤ìŒ ëŒ“ê¸€ì„ ë¶„ì„í•˜ê³ , ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.\n\nëŒ“ê¸€: \"{text}\"\n\nì‘ë‹µ í˜•ì‹ (ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”):\n{{\n    \"sentiment\": \"ê¸ì •|ë¶€ì •|ì¤‘ë¦½\",\n    \"confidence\": 0.0-1.0 ì‚¬ì´ì˜ ìˆ«ì,\n    \"reason\": \"ê°ì„± íŒë‹¨ ê·¼ê±°ë¥¼ í•œêµ­ì–´ë¡œ ê°„ë‹¨íˆ ì„¤ëª…\",\n    \"keywords\": [\"í•µì‹¬\", \"í‚¤ì›Œë“œ\", \"ëª©ë¡\"]\n}}\n\në¶„ì„ ê¸°ì¤€:\n- ê¸ì •: ì§€ì§€, ì°¬ì„±, í˜¸ì˜ì , ê¸°ëŒ€, ê°ì‚¬ ë“±ì˜ í‘œí˜„\n- ë¶€ì •: ë°˜ëŒ€, ë¹„íŒ, ìš°ë ¤, ì‹¤ë§, ë¶„ë…¸ ë“±ì˜ í‘œí˜„  \n- ì¤‘ë¦½: ê°ê´€ì  ì‚¬ì‹¤, ì§ˆë¬¸, ì• ë§¤í•œ í‘œí˜„\n\nJSON í˜•ì‹ì„ ì—„ê²©íˆ ì§€ì¼œì£¼ì„¸ìš”.\"\"\"\n\n    def create_trend_prompt(self, comments: List[Dict], keyword: str) -> str:\n        \"\"\"ë™í–¥ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±\"\"\"\n        comments_text = \"\\\\n\".join([f\"- {c.get('text', '')}\" for c in comments[:20]])  # ìµœëŒ€ 20ê°œ\n        \n        return f\"\"\"ë‹¹ì‹ ì€ ì „ë¬¸ ì—¬ë¡  ë™í–¥ ë¶„ì„ê°€ì…ë‹ˆë‹¤.\n\ní‚¤ì›Œë“œ: \"{keyword}\"ì— ëŒ€í•œ ëŒ“ê¸€ë“¤ì„ ë¶„ì„í•˜ì—¬ ì „ì²´ì ì¸ ì—¬ë¡  ë™í–¥ì„ íŒŒì•…í•˜ì„¸ìš”.\n\nëŒ“ê¸€ë“¤:\n{comments_text}\n\në°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:\n{{\n    \"overall_sentiment\": \"ê¸ì •|ë¶€ì •|ì¤‘ë¦½\",\n    \"sentiment_distribution\": {{\n        \"ê¸ì •\": 0.0-1.0,\n        \"ë¶€ì •\": 0.0-1.0, \n        \"ì¤‘ë¦½\": 0.0-1.0\n    }},\n    \"key_topics\": [\"ì£¼ìš”\", \"ì´ìŠˆ\", \"ëª©ë¡\"],\n    \"summary\": \"ë™í–¥ ìš”ì•½ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…\"\n}}\n\në¶„ì„ ê¸°ì¤€:\n- ì „ì²´ ëŒ“ê¸€ì˜ ê°ì„± ë¹„ìœ¨ì„ ì •í™•íˆ ê³„ì‚°\n- í•©ê³„ê°€ 1.0ì´ ë˜ë„ë¡ ë¹„ìœ¨ ì¡°ì •\n- í•µì‹¬ ì´ìŠˆë‚˜ ê´€ì‹¬ì‚¬ë¥¼ í‚¤ì›Œë“œë¡œ ì¶”ì¶œ\n- ê°ê´€ì ì´ê³  ê· í˜•ì¡íŒ ìš”ì•½ ì‘ì„±\n\nJSON í˜•ì‹ì„ ì—„ê²©íˆ ì§€ì¼œì£¼ì„¸ìš”.\"\"\"\n    \n    def call_openai_api(self, prompt: str, max_tokens: int = 500) -> str:\n        \"\"\"OpenAI API í˜¸ì¶œ\"\"\"\n        try:\n            response = openai.ChatCompletion.create(\n                model=\"gpt-3.5-turbo\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ì •í™•í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ëŠ” ê°ì„± ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                max_tokens=max_tokens,\n                temperature=0.3,\n                timeout=30\n            )\n            \n            return response.choices[0].message.content.strip()\n            \n        except Exception as e:\n            print(f\"âŒ OpenAI API ì˜¤ë¥˜: {str(e)}\")\n            # ë”ë¯¸ ì‘ë‹µ ë°˜í™˜\n            if \"sentiment\" in prompt:\n                return '{\"sentiment\": \"ì¤‘ë¦½\", \"confidence\": 0.5, \"reason\": \"API ì˜¤ë¥˜ë¡œ ì¸í•œ ê¸°ë³¸ ì‘ë‹µ\", \"keywords\": [\"ë¶„ì„ë¶ˆê°€\"]}'\n            else:\n                return '{\"overall_sentiment\": \"ì¤‘ë¦½\", \"sentiment_distribution\": {\"ê¸ì •\": 0.33, \"ë¶€ì •\": 0.33, \"ì¤‘ë¦½\": 0.34}, \"key_topics\": [\"ë¶„ì„ë¶ˆê°€\"], \"summary\": \"API ì˜¤ë¥˜ë¡œ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"}'\n    \n    def call_gemini_api(self, prompt: str) -> str:\n        \"\"\"Google Gemini API í˜¸ì¶œ\"\"\"\n        try:\n            model = genai.GenerativeModel('gemini-pro')\n            \n            generation_config = genai.types.GenerationConfig(\n                temperature=0.3,\n                max_output_tokens=500,\n                top_p=0.8\n            )\n            \n            response = model.generate_content(\n                prompt,\n                generation_config=generation_config\n            )\n            \n            return response.text.strip()\n            \n        except Exception as e:\n            print(f\"âŒ Gemini API ì˜¤ë¥˜: {str(e)}\")\n            # ë”ë¯¸ ì‘ë‹µ ë°˜í™˜\n            if \"sentiment\" in prompt:\n                return '{\"sentiment\": \"ì¤‘ë¦½\", \"confidence\": 0.5, \"reason\": \"API ì˜¤ë¥˜ë¡œ ì¸í•œ ê¸°ë³¸ ì‘ë‹µ\", \"keywords\": [\"ë¶„ì„ë¶ˆê°€\"]}'\n            else:\n                return '{\"overall_sentiment\": \"ì¤‘ë¦½\", \"sentiment_distribution\": {\"ê¸ì •\": 0.33, \"ë¶€ì •\": 0.33, \"ì¤‘ë¦½\": 0.34}, \"key_topics\": [\"ë¶„ì„ë¶ˆê°€\"], \"summary\": \"API ì˜¤ë¥˜ë¡œ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"}'\n    \n    def parse_json_response(self, response: str, response_type: str = \"sentiment\") -> Dict[str, Any]:\n        \"\"\"JSON ì‘ë‹µ íŒŒì‹± ë° ê²€ì¦\"\"\"\n        try:\n            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (markdown ì½”ë“œ ë¸”ë¡ ì œê±°)\n            if \"```json\" in response:\n                json_start = response.find(\"```json\") + 7\n                json_end = response.find(\"```\", json_start)\n                json_text = response[json_start:json_end].strip()\n            elif \"{\" in response and \"}\" in response:\n                json_start = response.find(\"{\")\n                json_end = response.rfind(\"}\") + 1\n                json_text = response[json_start:json_end]\n            else:\n                raise ValueError(\"JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n            \n            parsed = json.loads(json_text)\n            \n            # ê°ì„± ë¶„ì„ ì‘ë‹µ ê²€ì¦\n            if response_type == \"sentiment\":\n                required_keys = [\"sentiment\", \"confidence\", \"reason\", \"keywords\"]\n                for key in required_keys:\n                    if key not in parsed:\n                        raise ValueError(f\"í•„ìˆ˜ í‚¤ '{key}'ê°€ ì—†ìŠµë‹ˆë‹¤\")\n                \n                # ê°ì„± ê°’ ì •ê·œí™”\n                if parsed[\"sentiment\"] not in [\"ê¸ì •\", \"ë¶€ì •\", \"ì¤‘ë¦½\"]:\n                    parsed[\"sentiment\"] = \"ì¤‘ë¦½\"\n                \n                # ì‹ ë¢°ë„ ê°’ ê²€ì¦\n                if not (0 <= parsed[\"confidence\"] <= 1):\n                    parsed[\"confidence\"] = 0.5\n            \n            # ë™í–¥ ë¶„ì„ ì‘ë‹µ ê²€ì¦\n            elif response_type == \"trend\":\n                required_keys = [\"overall_sentiment\", \"sentiment_distribution\", \"key_topics\", \"summary\"]\n                for key in required_keys:\n                    if key not in parsed:\n                        raise ValueError(f\"í•„ìˆ˜ í‚¤ '{key}'ê°€ ì—†ìŠµë‹ˆë‹¤\")\n                \n                # ë¹„ìœ¨ ì •ê·œí™”\n                dist = parsed[\"sentiment_distribution\"]\n                total = sum(dist.values())\n                if total > 0:\n                    for key in dist:\n                        dist[key] = dist[key] / total\n                else:\n                    parsed[\"sentiment_distribution\"] = {\"ê¸ì •\": 0.33, \"ë¶€ì •\": 0.33, \"ì¤‘ë¦½\": 0.34}\n            \n            return parsed\n            \n        except Exception as e:\n            print(f\"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}\")\n            print(f\"ì›ë³¸ ì‘ë‹µ: {response}\")\n            \n            # ê¸°ë³¸ê°’ ë°˜í™˜\n            if response_type == \"sentiment\":\n                return {\n                    \"sentiment\": \"ì¤‘ë¦½\",\n                    \"confidence\": 0.5,\n                    \"reason\": \"íŒŒì‹± ì˜¤ë¥˜ë¡œ ì¸í•œ ê¸°ë³¸ ì‘ë‹µ\",\n                    \"keywords\": [\"ë¶„ì„ë¶ˆê°€\"]\n                }\n            else:\n                return {\n                    \"overall_sentiment\": \"ì¤‘ë¦½\",\n                    \"sentiment_distribution\": {\"ê¸ì •\": 0.33, \"ë¶€ì •\": 0.33, \"ì¤‘ë¦½\": 0.34},\n                    \"key_topics\": [\"ë¶„ì„ë¶ˆê°€\"],\n                    \"summary\": \"íŒŒì‹± ì˜¤ë¥˜ë¡œ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n                }\n    \n    def analyze_single_comment(self, comment_text: str) -> SentimentResult:\n        \"\"\"ë‹¨ì¼ ëŒ“ê¸€ ê°ì„± ë¶„ì„\"\"\"\n        print(f\"ğŸ“ ëŒ“ê¸€ ë¶„ì„ ì¤‘: {comment_text[:50]}...\")\n        \n        prompt = self.create_sentiment_prompt(comment_text)\n        \n        # API í˜¸ì¶œ\n        if self.use_openai:\n            response = self.call_openai_api(prompt)\n        else:\n            response = self.call_gemini_api(prompt)\n        \n        # ì‘ë‹µ íŒŒì‹±\n        result = self.parse_json_response(response, \"sentiment\")\n        \n        return SentimentResult(\n            text=comment_text,\n            sentiment=SentimentType(result[\"sentiment\"]),\n            confidence=result[\"confidence\"],\n            reason=result[\"reason\"],\n            keywords=result[\"keywords\"]\n        )\n    \n    def analyze_trend(self, comments: List[Dict], keyword: str) -> TrendAnalysis:\n        \"\"\"ëŒ“ê¸€ ì „ì²´ì˜ ë™í–¥ ë¶„ì„\"\"\"\n        print(f\"ğŸ“Š '{keyword}' í‚¤ì›Œë“œ ë™í–¥ ë¶„ì„ ì¤‘... (ëŒ“ê¸€ {len(comments)}ê°œ)\")\n        \n        prompt = self.create_trend_prompt(comments, keyword)\n        \n        # API í˜¸ì¶œ\n        if self.use_openai:\n            response = self.call_openai_api(prompt, max_tokens=800)\n        else:\n            response = self.call_gemini_api(prompt)\n        \n        # ì‘ë‹µ íŒŒì‹±\n        result = self.parse_json_response(response, \"trend\")\n        \n        return TrendAnalysis(\n            keyword=keyword,\n            overall_sentiment=SentimentType(result[\"overall_sentiment\"]),\n            sentiment_distribution=result[\"sentiment_distribution\"],\n            key_topics=result[\"key_topics\"],\n            summary=result[\"summary\"],\n            total_comments=len(comments)\n        )\n\n    @tool\n    def analyze_sentiment(comment_text: str, use_openai: bool = True) -> Dict[str, Any]:\n        \"\"\"ë‹¨ì¼ ëŒ“ê¸€ ê°ì„± ë¶„ì„ Tool í•¨ìˆ˜\n        \n        Args:\n            comment_text (str): ë¶„ì„í•  ëŒ“ê¸€ í…ìŠ¤íŠ¸\n            use_openai (bool): Trueì´ë©´ OpenAI ì‚¬ìš©, Falseì´ë©´ Gemini ì‚¬ìš©\n            \n        Returns:\n            Dict: ê°ì„± ë¶„ì„ ê²°ê³¼\n        \"\"\"\n        analyzer = DataAnalyzerTool(use_openai=use_openai)\n        \n        try:\n            result = analyzer.analyze_single_comment(comment_text)\n            \n            return {\n                \"text\": result.text,\n                \"sentiment\": result.sentiment.value,\n                \"confidence\": result.confidence,\n                \"reason\": result.reason,\n                \"keywords\": result.keywords,\n                \"api_used\": \"OpenAI\" if use_openai else \"Gemini\"\n            }\n            \n        except Exception as e:\n            return {\n                \"error\": f\"ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}\",\n                \"text\": comment_text,\n                \"sentiment\": \"ì¤‘ë¦½\",\n                \"confidence\": 0.0\n            }\n\n    @tool\n    def analyze_news_trend(comments_data: List[Dict], keyword: str, use_openai: bool = True) -> Dict[str, Any]:\n        \"\"\"ë‰´ìŠ¤ ëŒ“ê¸€ ì „ì²´ ë™í–¥ ë¶„ì„ Tool í•¨ìˆ˜\n        \n        Args:\n            comments_data (List[Dict]): ëŒ“ê¸€ ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n            keyword (str): ë¶„ì„ ëŒ€ìƒ í‚¤ì›Œë“œ\n            use_openai (bool): Trueì´ë©´ OpenAI ì‚¬ìš©, Falseì´ë©´ Gemini ì‚¬ìš©\n            \n        Returns:\n            Dict: ë™í–¥ ë¶„ì„ ê²°ê³¼\n        \"\"\"\n        analyzer = DataAnalyzerTool(use_openai=use_openai)\n        \n        try:\n            # ëŒ“ê¸€ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ\n            comments = []\n            for comment in comments_data:\n                if isinstance(comment, dict) and 'text' in comment:\n                    comments.append(comment)\n                elif isinstance(comment, str):\n                    comments.append({'text': comment})\n            \n            if not comments:\n                return {\n                    \"error\": \"ë¶„ì„í•  ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.\",\n                    \"keyword\": keyword\n                }\n            \n            result = analyzer.analyze_trend(comments, keyword)\n            \n            return {\n                \"keyword\": result.keyword,\n                \"overall_sentiment\": result.overall_sentiment.value,\n                \"sentiment_distribution\": result.sentiment_distribution,\n                \"key_topics\": result.key_topics,\n                \"summary\": result.summary,\n                \"total_comments\": result.total_comments,\n                \"api_used\": \"OpenAI\" if use_openai else \"Gemini\"\n            }\n            \n        except Exception as e:\n            return {\n                \"error\": f\"ë™í–¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}\",\n                \"keyword\": keyword,\n                \"overall_sentiment\": \"ì¤‘ë¦½\"\n            }\n\ndef main():\n    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n    print(\"ğŸš€ DataAnalyzer Tool ì‹¤ìŠµ ì‹œì‘\")\n    print(\"=\" * 60)\n    \n    # í…ŒìŠ¤íŠ¸ ëŒ“ê¸€ë“¤\n    test_comments = [\n        \"ì •ë§ ì¢‹ì€ ì •ì±…ì´ë„¤ìš”! ì ê·¹ ì§€ì§€í•©ë‹ˆë‹¤.\",\n        \"ì´ëŸ° ì‹ìœ¼ë¡œ í•˜ë©´ ì•ˆ ëœë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\",\n        \"ë” ìì„¸í•œ ì„¤ëª…ì´ í•„ìš”í•  ê²ƒ ê°™ì•„ìš”.\",\n        \"ì°¬ì„±í•©ë‹ˆë‹¤. ë¹¨ë¦¬ ì‹œí–‰ë˜ì—ˆìœ¼ë©´ ì¢‹ê² ì–´ìš”.\",\n        \"ë°˜ëŒ€í•©ë‹ˆë‹¤. ë„ˆë¬´ ì„±ê¸‰í•œ ê²°ì •ì¸ ê²ƒ ê°™ë„¤ìš”.\",\n        \"ì¥ë‹¨ì ì„ ë” ì‚´í´ë´ì•¼ í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.\"\n    ]\n    \n    # ì‹¤ìŠµ 1: ë‹¨ì¼ ëŒ“ê¸€ ê°ì„± ë¶„ì„\n    print(\"\\\\nğŸ“ [ì‹¤ìŠµ 1] ë‹¨ì¼ ëŒ“ê¸€ ê°ì„± ë¶„ì„\")\n    print(\"-\" * 40)\n    \n    for i, comment in enumerate(test_comments[:3], 1):\n        print(f\"\\\\n[ëŒ“ê¸€ {i}] {comment}\")\n        result = DataAnalyzerTool.analyze_sentiment(comment, use_openai=True)\n        \n        if \"error\" in result:\n            print(f\"âŒ ì˜¤ë¥˜: {result['error']}\")\n        else:\n            print(f\"âœ… ê°ì„±: {result['sentiment']} (ì‹ ë¢°ë„: {result['confidence']:.2f})\")\n            print(f\"   ê·¼ê±°: {result['reason']}\")\n            print(f\"   í‚¤ì›Œë“œ: {result['keywords']}\")\n    \n    # ì‹¤ìŠµ 2: ì „ì²´ ë™í–¥ ë¶„ì„\n    print(\"\\\\n\\\\nğŸ“Š [ì‹¤ìŠµ 2] ì „ì²´ ë™í–¥ ë¶„ì„\")\n    print(\"-\" * 40)\n    \n    comments_dict = [{\"text\": comment} for comment in test_comments]\n    trend_result = DataAnalyzerTool.analyze_news_trend(\n        comments_dict, \n        keyword=\"ì •ë¶€ ì •ì±…\", \n        use_openai=True\n    )\n    \n    if \"error\" in trend_result:\n        print(f\"âŒ ì˜¤ë¥˜: {trend_result['error']}\")\n    else:\n        print(f\"ğŸ¯ í‚¤ì›Œë“œ: {trend_result['keyword']}\")\n        print(f\"ğŸ“ˆ ì „ì²´ ê°ì„±: {trend_result['overall_sentiment']}\")\n        print(f\"ğŸ“Š ê°ì„± ë¶„í¬:\")\n        for sentiment, ratio in trend_result['sentiment_distribution'].items():\n            print(f\"   {sentiment}: {ratio:.1%}\")\n        print(f\"ğŸ” ì£¼ìš” ì£¼ì œ: {', '.join(trend_result['key_topics'])}\")\n        print(f\"ğŸ“‹ ìš”ì•½: {trend_result['summary']}\")\n        print(f\"ğŸ“ ì´ ëŒ“ê¸€: {trend_result['total_comments']}ê°œ\")\n    \n    print(\"\\\\nğŸ¯ ì£¼ìš” í•™ìŠµ í¬ì¸íŠ¸:\")\n    print(\"1. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ìœ¼ë¡œ ì¼ê´€ëœ JSON ì‘ë‹µ í™•ë³´\")\n    print(\"2. OpenAIì™€ Gemini APIì˜ ì°¨ì´ì  ë° ì„ íƒ ë°©ë²•\")\n    print(\"3. JSON íŒŒì‹± ë° ì˜ˆì™¸ ì²˜ë¦¬ë¡œ ì•ˆì •ì ì¸ ë°ì´í„° ì¶”ì¶œ\")\n    print(\"4. ê°ì„± ë¶„ì„ê³¼ ë™í–¥ ë¶„ì„ì˜ êµ¬ë¶„ ë° í™œìš©\")\n    print(\"5. @tool ë°ì½”ë ˆì´í„°ë¡œ Agentì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ Toolë¡œ ë³€í™˜\")\n    \n    print(\"\\\\nâš ï¸  ì£¼ì˜ì‚¬í•­:\")\n    print(\"- OPENAI_API_KEY ë˜ëŠ” GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í•„ìš”\")\n    print(\"- API ì‚¬ìš©ëŸ‰ ì œí•œ ë° ë¹„ìš© ê³ ë ¤\")\n    print(\"- JSON í˜•ì‹ ì‘ë‹µì´ ë³´ì¥ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ (íŒŒì‹± ë¡œì§ í•„ìš”)\")\n    print(\"- Rate Limit ëŒ€ì‘ì„ ìœ„í•œ ì¬ì‹œë„ ë¡œì§ êµ¬í˜„ ê¶Œì¥\")\n\nif __name__ == \"__main__\":\n    main()\n'''\n\n# íŒŒì¼ ì €ì¥\nlab3_file = os.path.join(output_dir, \"lab3_data_analyzer.py\")\nwith open(lab3_file, 'w', encoding='utf-8') as f:\n    f.write(lab3_code)\n\nprint(f\"âœ… ì‹¤ìŠµ 3 ì½”ë“œ ìƒì„± ì™„ë£Œ: {lab3_file}\")\nprint(f\"   íŒŒì¼ í¬ê¸°: {len(lab3_code)} ë¬¸ì\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì‹¤ìŠµ 3 ì½”ë“œ ìƒì„± ì™„ë£Œ: /home/user/output/ai_agent_labs/lab3_data_analyzer.py\n   íŒŒì¼ í¬ê¸°: 14240 ë¬¸ì\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1. âœ… ğŸ”´ í™˜ê²½ ì„¤ì • ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ [completed] (ID: 1)\n2. âœ… ğŸ”´ ì‹¤ìŠµ 1: LangChain ê¸°ë³¸ Agent ì½”ë“œ ì‘ì„± (lab1_basic_agent.py) [completed] (ID: 2)\n3. âœ… ğŸ”´ ì‹¤ìŠµ 2: NewsScraper Tool êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab2_news_scraper.py) [completed] (ID: 3)\n4. âœ… ğŸ”´ ì‹¤ìŠµ 3: DataAnalyzer Tool êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab3_data_analyzer.py) [completed] (ID: 4)\n5. ğŸ”„ ğŸ”´ ì‹¤ìŠµ 4: Planner Agent êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab4_planner_agent.py) [in_progress] (ID: 5)\n6. â³ ğŸŸ¡ Word ë¬¸ì„œ ìƒì„± (session1_labs_guide.docx) [pending] (ID: 6)\n7. â³ ğŸŸ¢ íŒŒì¼ì„ AI Driveì— ì—…ë¡œë“œ [pending] (ID: 7)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {
        "file_changes": [
          {
            "path": "/home/user/output/ai_agent_labs/lab4_planner_agent.py",
            "change_type": "modified",
            "version": 1,
            "tool_call_id": "tooluse_nMDITvamSdKqmPAqiax5vA",
            "execution_count": 5,
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/ff36bada-1fdc-49b0-a8a9-aabd3f04ea5e/output/ai_agent_labs/lab4_planner_agent.py",
              "mime_type": "text/x-python",
              "size_bytes": 14267
            },
            "upload_status": "queued",
            "upload_completed": false,
            "timestamp": "2025-12-16T10:16:53.231242Z"
          }
        ]
      },
      "source": [
        "# ì‹¤ìŠµ 4: Planner Agent êµ¬í˜„ ì½”ë“œ ì‘ì„±\nlab4_code = '''\"\"\"\nAI ì—ì´ì „íŠ¸ ê¸°ë°˜ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì‹œìŠ¤í…œ - ì‹¤ìŠµ 4\n==================================================\nì£¼ì œ: Planner Agent êµ¬í˜„ - Tools ë“±ë¡ ë° ì‹¤í–‰\n\nëª©í‘œ:\n- ì—¬ëŸ¬ Toolsë¥¼ í†µí•©í•˜ëŠ” Planner Agent êµ¬í˜„\n- ìì—°ì–´ ì˜ë„ íŒŒì•… ë° Tool ìˆœì°¨ ì‹¤í–‰\n- ì‚¬ìš©ì ì§ˆì˜ì— ë”°ë¥¸ ë™ì  Tool ì„ íƒ\n- ì „ì²´ End-to-End íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\n\ní•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬:\npip install langchain openai python-dotenv\n\"\"\"\n\nimport os\nimport json\nfrom typing import List, Dict, Any, Optional\nfrom dotenv import load_dotenv\nfrom langchain.tools import tool\nfrom langchain.agents import initialize_agent, AgentType\nfrom langchain.llms import OpenAI\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.schema import AgentAction, AgentFinish\n\n# ì´ì „ ì‹¤ìŠµë“¤ì—ì„œ êµ¬í˜„í•œ Toolë“¤ import (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë³„ë„ íŒŒì¼ì—ì„œ)\nfrom lab2_news_scraper import NewsScraperTool\nfrom lab3_data_analyzer import DataAnalyzerTool\n\n# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\nload_dotenv()\n\nclass NewsAnalysisAgent:\n    \"\"\"ë‰´ìŠ¤ ê°ì„± ë¶„ì„ì„ ìœ„í•œ í†µí•© AI Agent\"\"\"\n    \n    def __init__(self):\n        \"\"\"Agent ì´ˆê¸°í™”\"\"\"\n        \n        # OpenAI API í‚¤ ì„¤ì •\n        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n        if not self.openai_api_key:\n            print(\"âš ï¸  ê²½ê³ : OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n            self.openai_api_key = \"sk-test-key-replace-with-real-key\"\n        \n        # LLM ì´ˆê¸°í™”\n        self.llm = OpenAI(\n            temperature=0.1,  # ë‚®ì€ temperatureë¡œ ì¼ê´€ëœ ì‘ë‹µ\n            openai_api_key=self.openai_api_key,\n            max_tokens=1000,\n            verbose=True\n        )\n        \n        # ë©”ëª¨ë¦¬ ì„¤ì • (ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€)\n        self.memory = ConversationBufferMemory(\n            memory_key=\"chat_history\",\n            return_messages=True,\n            input_key=\"input\",\n            output_key=\"output\"\n        )\n        \n        # Tools ë“±ë¡\n        self.tools = [\n            self.scrape_news_tool,\n            self.analyze_sentiment_tool,\n            self.analyze_trend_tool,\n            self.summarize_results_tool\n        ]\n        \n        # Agent ì´ˆê¸°í™”\n        self.agent = initialize_agent(\n            tools=self.tools,\n            llm=self.llm,\n            agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n            memory=self.memory,\n            verbose=True,\n            max_iterations=5,  # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ì œí•œ\n            early_stopping_method=\"generate\"  # ì¡°ê¸° ì¢…ë£Œ ì„¤ì •\n        )\n        \n        print(\"ğŸ¤– ë‰´ìŠ¤ ê°ì„± ë¶„ì„ Agentê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n        print(f\"ğŸ“š ë“±ë¡ëœ Tools: {len(self.tools)}ê°œ\")\n    \n    @tool\n    def scrape_news_tool(keyword: str, max_articles: int = 3) -> str:\n        \"\"\"ë‰´ìŠ¤ ê¸°ì‚¬ ë° ëŒ“ê¸€ ìˆ˜ì§‘ ë„êµ¬\n        \n        Args:\n            keyword (str): ê²€ìƒ‰í•  í‚¤ì›Œë“œ\n            max_articles (int): ìµœëŒ€ ìˆ˜ì§‘í•  ê¸°ì‚¬ ìˆ˜\n            \n        Returns:\n            str: ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„° (JSON í˜•ì‹)\n        \"\"\"\n        print(f\"ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: {keyword}\")\n        \n        # ì‹¤ì œë¡œëŠ” NewsScraperTool.scrape_news í˜¸ì¶œ\n        # ì—¬ê¸°ì„œëŠ” í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„° ë°˜í™˜\n        dummy_data = {\n            \"keyword\": keyword,\n            \"articles\": [\n                {\n                    \"title\": f\"{keyword} ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤ 1\",\n                    \"url\": \"https://news.example.com/1\",\n                    \"content\": f\"{keyword}ì— ëŒ€í•œ ê¸ì •ì ì¸ ì „ë§ì´ ì œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.\",\n                    \"comments\": [\n                        {\"text\": \"ì¢‹ì€ ì†Œì‹ì´ë„¤ìš”!\", \"author\": \"ì‚¬ìš©ì1\"},\n                        {\"text\": \"ê¸°ëŒ€ë©ë‹ˆë‹¤.\", \"author\": \"ì‚¬ìš©ì2\"},\n                        {\"text\": \"ì‹ ì¤‘í•˜ê²Œ ì§€ì¼œë´ì•¼ê² ì–´ìš”.\", \"author\": \"ì‚¬ìš©ì3\"}\n                    ]\n                },\n                {\n                    \"title\": f\"{keyword} ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤ 2\", \n                    \"url\": \"https://news.example.com/2\",\n                    \"content\": f\"{keyword}ì— ëŒ€í•œ ìš°ë ¤ì˜ ëª©ì†Œë¦¬ë„ ë‚˜ì˜¤ê³  ìˆìŠµë‹ˆë‹¤.\",\n                    \"comments\": [\n                        {\"text\": \"ê±±ì •ì´ ë©ë‹ˆë‹¤.\", \"author\": \"ì‚¬ìš©ì4\"},\n                        {\"text\": \"ë” ì‹ ì¤‘í•´ì•¼ í•  ê²ƒ ê°™ì•„ìš”.\", \"author\": \"ì‚¬ìš©ì5\"},\n                        {\"text\": \"ì¥ë‹¨ì ì„ ëª¨ë‘ ê³ ë ¤í•´ì•¼ì£ .\", \"author\": \"ì‚¬ìš©ì6\"}\n                    ]\n                }\n            ],\n            \"total_articles\": 2,\n            \"total_comments\": 6\n        }\n        \n        return json.dumps(dummy_data, ensure_ascii=False)\n    \n    @tool\n    def analyze_sentiment_tool(comment_text: str) -> str:\n        \"\"\"ë‹¨ì¼ ëŒ“ê¸€ ê°ì„± ë¶„ì„ ë„êµ¬\n        \n        Args:\n            comment_text (str): ë¶„ì„í•  ëŒ“ê¸€ í…ìŠ¤íŠ¸\n            \n        Returns:\n            str: ê°ì„± ë¶„ì„ ê²°ê³¼ (JSON í˜•ì‹)\n        \"\"\"\n        print(f\"ğŸ“ ëŒ“ê¸€ ê°ì„± ë¶„ì„: {comment_text[:30]}...\")\n        \n        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì„± ë¶„ì„ (ì‹¤ì œë¡œëŠ” DataAnalyzerTool ì‚¬ìš©)\n        positive_words = [\"ì¢‹\", \"í›Œë¥­\", \"ê¸°ëŒ€\", \"ì°¬ì„±\", \"ì§€ì§€\", \"ë§Œì¡±\", \"í›Œë¥­\"]\n        negative_words = [\"ë‚˜ì˜\", \"ê±±ì •\", \"ìš°ë ¤\", \"ë°˜ëŒ€\", \"ì‹¤ë§\", \"ë¬¸ì œ\", \"ìœ„í—˜\"]\n        \n        text_lower = comment_text.lower()\n        \n        positive_score = sum(1 for word in positive_words if word in text_lower)\n        negative_score = sum(1 for word in negative_words if word in text_lower)\n        \n        if positive_score > negative_score:\n            sentiment = \"ê¸ì •\"\n            confidence = min(0.9, 0.6 + positive_score * 0.1)\n        elif negative_score > positive_score:\n            sentiment = \"ë¶€ì •\"\n            confidence = min(0.9, 0.6 + negative_score * 0.1)\n        else:\n            sentiment = \"ì¤‘ë¦½\"\n            confidence = 0.5\n        \n        result = {\n            \"text\": comment_text,\n            \"sentiment\": sentiment,\n            \"confidence\": confidence,\n            \"reason\": f\"{'ê¸ì •' if positive_score > 0 else 'ë¶€ì •' if negative_score > 0 else 'ì¤‘ë¦½ì '} í‘œí˜„ ê°ì§€\",\n            \"keywords\": positive_words[:2] if positive_score > 0 else negative_words[:2] if negative_score > 0 else [\"ì¤‘ë¦½\"]\n        }\n        \n        return json.dumps(result, ensure_ascii=False)\n    \n    @tool\n    def analyze_trend_tool(comments_json: str, keyword: str) -> str:\n        \"\"\"ëŒ“ê¸€ë“¤ì˜ ì „ì²´ ë™í–¥ ë¶„ì„ ë„êµ¬\n        \n        Args:\n            comments_json (str): ëŒ“ê¸€ ë°ì´í„° JSON ë¬¸ìì—´\n            keyword (str): ë¶„ì„ ëŒ€ìƒ í‚¤ì›Œë“œ\n            \n        Returns:\n            str: ë™í–¥ ë¶„ì„ ê²°ê³¼ (JSON í˜•ê²©)\n        \"\"\"\n        print(f\"ğŸ“Š '{keyword}' ë™í–¥ ë¶„ì„ ì¤‘...\")\n        \n        try:\n            # JSON íŒŒì‹±\n            data = json.loads(comments_json) if isinstance(comments_json, str) else comments_json\n            \n            # ëª¨ë“  ëŒ“ê¸€ ìˆ˜ì§‘\n            all_comments = []\n            if \"articles\" in data:\n                for article in data[\"articles\"]:\n                    if \"comments\" in article:\n                        all_comments.extend(article[\"comments\"])\n            \n            if not all_comments:\n                return json.dumps({\n                    \"error\": \"ë¶„ì„í•  ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.\",\n                    \"keyword\": keyword\n                }, ensure_ascii=False)\n            \n            # ê° ëŒ“ê¸€ì˜ ê°ì„± ë¶„ì„\n            sentiment_counts = {\"ê¸ì •\": 0, \"ë¶€ì •\": 0, \"ì¤‘ë¦½\": 0}\n            \n            for comment in all_comments:\n                if isinstance(comment, dict) and \"text\" in comment:\n                    # analyze_sentiment_tool í˜¸ì¶œ\n                    sentiment_result = json.loads(NewsAnalysisAgent.analyze_sentiment_tool(comment[\"text\"]))\n                    sentiment_counts[sentiment_result[\"sentiment\"]] += 1\n            \n            total = sum(sentiment_counts.values())\n            if total == 0:\n                return json.dumps({\n                    \"error\": \"ëŒ“ê¸€ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\",\n                    \"keyword\": keyword\n                }, ensure_ascii=False)\n            \n            # ë¹„ìœ¨ ê³„ì‚°\n            distribution = {\n                sentiment: count / total \n                for sentiment, count in sentiment_counts.items()\n            }\n            \n            # ì „ì²´ ê°ì„± ê²°ì •\n            max_sentiment = max(distribution.keys(), key=lambda k: distribution[k])\n            \n            # ì£¼ìš” ì£¼ì œ ì¶”ì¶œ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ)\n            all_text = \" \".join([c.get(\"text\", \"\") for c in all_comments if isinstance(c, dict)])\n            common_words = [\"ì •ì±…\", \"ê²½ì œ\", \"ê¸°ìˆ \", \"ì‚¬íšŒ\", \"ì •ë¶€\", \"ê¸°ì—…\", \"ì‹œì¥\", \"íˆ¬ì\"]\n            key_topics = [word for word in common_words if word in all_text][:3]\n            \n            result = {\n                \"keyword\": keyword,\n                \"overall_sentiment\": max_sentiment,\n                \"sentiment_distribution\": distribution,\n                \"key_topics\": key_topics or [keyword],\n                \"summary\": f\"'{keyword}'ì— ëŒ€í•œ ì—¬ë¡ ì€ ì „ë°˜ì ìœ¼ë¡œ {max_sentiment}ì ì…ë‹ˆë‹¤. ì´ {total}ê°œì˜ ëŒ“ê¸€ì„ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤.\",\n                \"total_comments\": total\n            }\n            \n            return json.dumps(result, ensure_ascii=False)\n            \n        except Exception as e:\n            return json.dumps({\n                \"error\": f\"ë™í–¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}\",\n                \"keyword\": keyword\n            }, ensure_ascii=False)\n    \n    @tool\n    def summarize_results_tool(trend_json: str) -> str:\n        \"\"\"ë¶„ì„ ê²°ê³¼ ìš”ì•½ ë° ì¸ì‚¬ì´íŠ¸ ì œê³µ ë„êµ¬\n        \n        Args:\n            trend_json (str): ë™í–¥ ë¶„ì„ ê²°ê³¼ JSON\n            \n        Returns:\n            str: ìµœì¢… ìš”ì•½ ë° ì¸ì‚¬ì´íŠ¸\n        \"\"\"\n        print(\"ğŸ“‹ ê²°ê³¼ ìš”ì•½ ì¤‘...\")\n        \n        try:\n            data = json.loads(trend_json) if isinstance(trend_json, str) else trend_json\n            \n            if \"error\" in data:\n                return f\"âŒ ë¶„ì„ ì‹¤íŒ¨: {data['error']}\"\n            \n            keyword = data.get(\"keyword\", \"ëŒ€ìƒ\")\n            overall_sentiment = data.get(\"overall_sentiment\", \"ì¤‘ë¦½\")\n            distribution = data.get(\"sentiment_distribution\", {})\n            key_topics = data.get(\"key_topics\", [])\n            total_comments = data.get(\"total_comments\", 0)\n            \n            # í¼ì„¼íŠ¸ë¡œ ë³€í™˜\n            percent_dist = {k: f\"{v:.1%}\" for k, v in distribution.items()}\n            \n            summary = f\"\"\"\nğŸ¯ **'{keyword}' ê°ì„± ë¶„ì„ ê²°ê³¼**\n\nğŸ“Š **ì „ì²´ ë™í–¥**: {overall_sentiment}\nğŸ“ˆ **ê°ì„± ë¶„í¬**:\n   â€¢ ê¸ì •: {percent_dist.get('ê¸ì •', '0.0%')}\n   â€¢ ë¶€ì •: {percent_dist.get('ë¶€ì •', '0.0%')}  \n   â€¢ ì¤‘ë¦½: {percent_dist.get('ì¤‘ë¦½', '0.0%')}\n\nğŸ” **ì£¼ìš” í‚¤ì›Œë“œ**: {', '.join(key_topics) if key_topics else 'ì—†ìŒ'}\nğŸ“ **ë¶„ì„ ëŒ“ê¸€ ìˆ˜**: {total_comments}ê°œ\n\nğŸ’¡ **ì¸ì‚¬ì´íŠ¸**:\n\"\"\".strip()\n            \n            # ì¸ì‚¬ì´íŠ¸ ìƒì„±\n            if overall_sentiment == \"ê¸ì •\":\n                summary += f\"\\\\nâ€¢ {keyword}ì— ëŒ€í•œ ì—¬ë¡ ì´ ì „ë°˜ì ìœ¼ë¡œ ê¸ì •ì ì…ë‹ˆë‹¤.\"\n                summary += f\"\\\\nâ€¢ ê¸ì • ë¹„ìœ¨ì´ {percent_dist.get('ê¸ì •', '0%')}ë¡œ ë†’ì€ ì§€ì§€ë¥¼ ë°›ê³  ìˆìŠµë‹ˆë‹¤.\"\n            elif overall_sentiment == \"ë¶€ì •\":\n                summary += f\"\\\\nâ€¢ {keyword}ì— ëŒ€í•œ ìš°ë ¤ì˜ ëª©ì†Œë¦¬ê°€ ë†’ìŠµë‹ˆë‹¤.\"\n                summary += f\"\\\\nâ€¢ ë¶€ì • ë¹„ìœ¨ì´ {percent_dist.get('ë¶€ì •', '0%')}ë¡œ ì‹ ì¤‘í•œ ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\"\n            else:\n                summary += f\"\\\\nâ€¢ {keyword}ì— ëŒ€í•œ ì—¬ë¡ ì´ ë¶„ì‚°ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\"\n                summary += f\"\\\\nâ€¢ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì˜ê²¬ì´ ë‚˜ë‰˜ê³  ìˆì–´ ê· í˜•ì¡íŒ ì ‘ê·¼ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\"\n            \n            return summary\n            \n        except Exception as e:\n            return f\"âŒ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}\"\n    \n    def analyze_news_sentiment(self, user_query: str) -> str:\n        \"\"\"ì‚¬ìš©ì ì§ˆì˜ë¥¼ ë°›ì•„ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ë©”ì¸ ë©”ì„œë“œ\"\"\"\n        print(f\"\\\\nğŸ¤– ì‚¬ìš©ì ì§ˆì˜: {user_query}\")\n        print(\"=\" * 60)\n        \n        try:\n            # Agent ì‹¤í–‰\n            response = self.agent.run(input=user_query)\n            \n            print(\"=\" * 60)\n            print(f\"âœ… ìµœì¢… ì‘ë‹µ:\")\n            print(response)\n            \n            return response\n            \n        except Exception as e:\n            error_msg = f\"âŒ Agent ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}\"\n            print(error_msg)\n            return error_msg\n    \n    def get_conversation_history(self) -> List[Dict]:\n        \"\"\"ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜\"\"\"\n        return self.memory.chat_memory.messages\n\ndef main():\n    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n    print(\"ğŸš€ Planner Agent ì‹¤ìŠµ ì‹œì‘\")\n    print(\"=\" * 60)\n    \n    # Agent ì´ˆê¸°í™”\n    agent = NewsAnalysisAgent()\n    \n    # í…ŒìŠ¤íŠ¸ ì§ˆì˜ë“¤\n    test_queries = [\n        \"ì‚¼ì„±ì „ì ì£¼ê°€ì— ëŒ€í•œ ìµœê·¼ ë‰´ìŠ¤ì˜ ì—¬ë¡ ì„ ë¶„ì„í•´ì¤˜\",\n        \"AI ê¸°ìˆ  ë°œì „ì— ëŒ€í•œ ì‚¬ëŒë“¤ì˜ ë°˜ì‘ì€ ì–´ë•Œ?\",\n        \"ë¶€ë™ì‚° ì‹œì¥ ë™í–¥ì— ëŒ€í•œ ëŒ“ê¸€ë“¤ì„ ë¶„ì„í•´ì„œ ìš”ì•½í•´ì¤˜\"\n    ]\n    \n    print(\"\\\\nğŸ“ í…ŒìŠ¤íŠ¸ ì§ˆì˜ ì‹¤í–‰:\")\n    \n    for i, query in enumerate(test_queries, 1):\n        print(f\"\\\\n\\\\n[í…ŒìŠ¤íŠ¸ {i}]\")\n        print(\"-\" * 50)\n        \n        response = agent.analyze_news_sentiment(query)\n        \n        print(\"\\\\n\" + \"=\"*40)\n        print(f\"[í…ŒìŠ¤íŠ¸ {i} ì™„ë£Œ]\\\\n\")\n        \n        # ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ í™•ì¸\n        history = agent.get_conversation_history()\n        print(f\"ğŸ“š ëŒ€í™” íˆìŠ¤í† ë¦¬ ê¸¸ì´: {len(history)}\")\n    \n    print(\"\\\\n\\\\nğŸ¯ ì£¼ìš” í•™ìŠµ í¬ì¸íŠ¸:\")\n    print(\"1. ì—¬ëŸ¬ Toolsë¥¼ í•˜ë‚˜ì˜ Agentì— í†µí•© ë“±ë¡\")\n    print(\"2. ì‚¬ìš©ì ì§ˆì˜ì— ë”°ë¥¸ ë™ì  Tool ì„ íƒ ë° ì‹¤í–‰\")\n    print(\"3. ConversationBufferMemoryë¡œ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€\")\n    print(\"4. Tool ê°„ ë°ì´í„° ì „ë‹¬ ë° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\")\n    print(\"5. Agentì˜ ReAct íŒ¨í„´ (Reason + Act) ê´€ì°°\")\n    \n    print(\"\\\\nâš ï¸  ì£¼ì˜ì‚¬í•­:\")\n    print(\"- ëª¨ë“  Toolsê°€ ì •ìƒ ì‘ë™í•´ì•¼ Agentê°€ ì˜¬ë°”ë¥´ê²Œ ì‹¤í–‰ë¨\")\n    print(\"- max_iterations ì„¤ì •ìœ¼ë¡œ ë¬´í•œ ë£¨í”„ ë°©ì§€\")\n    print(\"- Tool ê°„ ë°ì´í„° í˜•ì‹ ì¼ì¹˜ (JSON ë¬¸ìì—´ ì „ë‹¬)\")\n    print(\"- verbose=Trueë¡œ Agent ì¶”ë¡  ê³¼ì • ê´€ì°° ê°€ëŠ¥\")\n    \n    print(\"\\\\nâœ¨ í™•ì¥ ê°€ëŠ¥í•œ ê¸°ëŠ¥:\")\n    print(\"- ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ Tool ì¶”ê°€\")\n    print(\"- ì‹œê°í™” ìƒì„± Tool ì¶”ê°€\") \n    print(\"- ì´ë©”ì¼/ìŠ¬ë™ ì•Œë¦¼ Tool ì¶”ê°€\")\n    print(\"- ìŠ¤ì¼€ì¤„ë§ ë° ìë™í™” Tool ì¶”ê°€\")\n\nif __name__ == \"__main__\":\n    main()\n'''\n\n# íŒŒì¼ ì €ì¥\nlab4_file = os.path.join(output_dir, \"lab4_planner_agent.py\")\nwith open(lab4_file, 'w', encoding='utf-8') as f:\n    f.write(lab4_code)\n\nprint(f\"âœ… ì‹¤ìŠµ 4 ì½”ë“œ ìƒì„± ì™„ë£Œ: {lab4_file}\")\nprint(f\"   íŒŒì¼ í¬ê¸°: {len(lab4_code)} ë¬¸ì\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì‹¤ìŠµ 4 ì½”ë“œ ìƒì„± ì™„ë£Œ: /home/user/output/ai_agent_labs/lab4_planner_agent.py\n   íŒŒì¼ í¬ê¸°: 11842 ë¬¸ì\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1. âœ… ğŸ”´ í™˜ê²½ ì„¤ì • ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ [completed] (ID: 1)\n2. âœ… ğŸ”´ ì‹¤ìŠµ 1: LangChain ê¸°ë³¸ Agent ì½”ë“œ ì‘ì„± (lab1_basic_agent.py) [completed] (ID: 2)\n3. âœ… ğŸ”´ ì‹¤ìŠµ 2: NewsScraper Tool êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab2_news_scraper.py) [completed] (ID: 3)\n4. âœ… ğŸ”´ ì‹¤ìŠµ 3: DataAnalyzer Tool êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab3_data_analyzer.py) [completed] (ID: 4)\n5. âœ… ğŸ”´ ì‹¤ìŠµ 4: Planner Agent êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab4_planner_agent.py) [completed] (ID: 5)\n6. ğŸ”„ ğŸŸ¡ Word ë¬¸ì„œ ìƒì„± (session1_labs_guide.docx) [in_progress] (ID: 6)\n7. â³ ğŸŸ¢ íŒŒì¼ì„ AI Driveì— ì—…ë¡œë“œ [pending] (ID: 7)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° Word ë¬¸ì„œ ìƒì„±\n!pip install -q python-docx"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\r\n\u001b[0m\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Word ë¬¸ì„œ ìƒì„±\nfrom docx import Document\nfrom docx.shared import Inches, Pt\nfrom docx.enum.text import WD_ALIGN_PARAGRAPH\nfrom docx.enum.style import WD_STYLE_TYPE\nfrom docx.oxml.shared import OxmlElement, qn\n\n# ìƒˆë¡œìš´ Word ë¬¸ì„œ ìƒì„±\ndoc = Document()\n\n# ë¬¸ì„œ ì œëª© ì„¤ì •\ntitle = doc.add_heading('AI ì—ì´ì „íŠ¸ ê¸°ë°˜ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì‹œìŠ¤í…œ', 0)\ntitle.alignment = WD_ALIGN_PARAGRAPH.CENTER\n\nsubtitle = doc.add_heading('1íšŒì°¨ ì‹¤ìŠµ ê°€ì´ë“œ', level=1)\nsubtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER\n\n# ê³¼ì • ì •ë³´ ì¶”ê°€\ndoc.add_paragraph()\ninfo_para = doc.add_paragraph()\ninfo_para.add_run('ê³¼ì •ëª…: ').bold = True\ninfo_para.add_run('AI ì—ì´ì „íŠ¸ ê°œë… ë° ì•„í‚¤í…ì²˜ ì„¤ê³„\\n')\ninfo_para.add_run('ì‹œê°„: ').bold = True\ninfo_para.add_run('2ì‹œê°„ (120ë¶„)\\n')\ninfo_para.add_run('ë‚œì´ë„: ').bold = True\ninfo_para.add_run('ëŒ€í•™ì›/ì‹¤ë¬´ ì¤‘ê¸‰\\n')\ninfo_para.add_run('ìƒì„±ì¼: ').bold = True\ninfo_para.add_run('2024ë…„ 12ì›”')\n\nprint(\"âœ… Word ë¬¸ì„œ í—¤ë” ìƒì„± ì™„ë£Œ\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Word ë¬¸ì„œ í—¤ë” ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ëª©ì°¨ ì¶”ê°€\ndoc.add_page_break()\ndoc.add_heading('ëª©ì°¨', level=1)\n\ntoc_items = [\n    (\"1. í™˜ê²½ ì„¤ì • ê°€ì´ë“œ\", \"3\"),\n    (\"2. ì‹¤ìŠµ 1: LangChain ê¸°ë³¸ Agent\", \"4\"),\n    (\"3. ì‹¤ìŠµ 2: NewsScraper Tool êµ¬í˜„\", \"8\"),\n    (\"4. ì‹¤ìŠµ 3: DataAnalyzer Tool êµ¬í˜„\", \"12\"),\n    (\"5. ì‹¤ìŠµ 4: Planner Agent êµ¬í˜„\", \"16\"),\n    (\"6. íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ\", \"20\"),\n    (\"7. ì¶”ê°€ í•™ìŠµ ìë£Œ\", \"21\")\n]\n\nfor item, page in toc_items:\n    toc_para = doc.add_paragraph()\n    toc_para.add_run(item)\n    tab_stops = toc_para.paragraph_format.tab_stops\n    toc_para.add_run('\\t' + '.' * 50 + '\\t' + page)\n\nprint(\"âœ… ëª©ì°¨ ìƒì„± ì™„ë£Œ\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ëª©ì°¨ ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# í™˜ê²½ ì„¤ì • ê°€ì´ë“œ ì„¹ì…˜ ì¶”ê°€\ndoc.add_page_break()\ndoc.add_heading('1. í™˜ê²½ ì„¤ì • ê°€ì´ë“œ', level=1)\n\n# 1.1 í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬\ndoc.add_heading('1.1 í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜', level=2)\ndoc.add_paragraph('ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ì—¬ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:')\n\n# ì½”ë“œ ë¸”ë¡ ìŠ¤íƒ€ì¼ë¡œ ëª…ë ¹ì–´ ì¶”ê°€\ninstall_commands = [\n    \"# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬\",\n    \"pip install langchain openai python-dotenv\",\n    \"\",\n    \"# ì›¹ í¬ë¡¤ë§ ê´€ë ¨\",  \n    \"pip install selenium webdriver-manager requests beautifulsoup4\",\n    \"\",\n    \"# AI/ML ê´€ë ¨\",\n    \"pip install google-generativeai\",\n    \"\",\n    \"# ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨\", \n    \"pip install mysql-connector-python sqlalchemy\",\n    \"\",\n    \"# ë¬¸ì„œ ì²˜ë¦¬ ê´€ë ¨\",\n    \"pip install python-docx\"\n]\n\nfor cmd in install_commands:\n    if cmd.startswith('#'):\n        para = doc.add_paragraph(cmd)\n        para.style = 'Heading 3'\n    elif cmd == \"\":\n        doc.add_paragraph()\n    else:\n        para = doc.add_paragraph(cmd, style='Normal')\n        # ì½”ë“œ ìŠ¤íƒ€ì¼ ì ìš©\n        para.style.font.name = 'Consolas'\n        para.style.font.size = Pt(10)\n\n# 1.2 í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\ndoc.add_heading('1.2 í™˜ê²½ ë³€ìˆ˜ ì„¤ì •', level=2)\ndoc.add_paragraph('.env íŒŒì¼ì„ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ìƒì„±í•˜ê³  ë‹¤ìŒ ë‚´ìš©ì„ ì¶”ê°€í•˜ì„¸ìš”:')\n\nenv_content = [\n    \"# OpenAI API í‚¤\",\n    \"OPENAI_API_KEY=your_openai_api_key_here\",\n    \"\",\n    \"# Google Gemini API í‚¤ (ì„ íƒì‚¬í•­)\",\n    \"GEMINI_API_KEY=your_gemini_api_key_here\", \n    \"\",\n    \"# Firecrawl API í‚¤ (ì„ íƒì‚¬í•­)\",\n    \"FIRECRAWL_API_KEY=your_firecrawl_api_key_here\"\n]\n\nfor line in env_content:\n    if line.startswith('#'):\n        doc.add_paragraph(line, style='Heading 3')\n    elif line == \"\":\n        doc.add_paragraph()\n    else:\n        para = doc.add_paragraph(line)\n        para.style.font.name = 'Consolas'\n        para.style.font.size = Pt(10)\n\n# 1.3 ì£¼ì˜ì‚¬í•­\ndoc.add_heading('1.3 ì£¼ì˜ì‚¬í•­', level=2)\nnotes = [\n    \"â€¢ OpenAI API í‚¤ëŠ” https://platform.openai.com/api-keys ì—ì„œ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\",\n    \"â€¢ Chrome ë¸Œë¼ìš°ì €ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ Seleniumì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.\",\n    \"â€¢ webdriver-managerê°€ ìë™ìœ¼ë¡œ ChromeDriverë¥¼ ê´€ë¦¬í•˜ë¯€ë¡œ ë³„ë„ ì„¤ì¹˜ê°€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤.\",\n    \"â€¢ ì‹¤ìŠµ ì¤‘ API ì‚¬ìš©ëŸ‰ê³¼ ë¹„ìš©ì„ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\"\n]\n\nfor note in notes:\n    doc.add_paragraph(note)\n\nprint(\"âœ… í™˜ê²½ ì„¤ì • ê°€ì´ë“œ ìƒì„± ì™„ë£Œ\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… í™˜ê²½ ì„¤ì • ê°€ì´ë“œ ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ì‹¤ìŠµë³„ ì„¹ì…˜ ì¶”ê°€ - ê³µí†µ í•¨ìˆ˜\ndef add_lab_section(doc, lab_num, title, objectives, key_concepts, file_name):\n    \"\"\"ì‹¤ìŠµ ì„¹ì…˜ì„ ë¬¸ì„œì— ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜\"\"\"\n    \n    # í˜ì´ì§€ ë¸Œë ˆì´í¬ ë° ì œëª©\n    doc.add_page_break()\n    doc.add_heading(f'{lab_num}. ì‹¤ìŠµ {lab_num[-1]}: {title}', level=1)\n    \n    # í•™ìŠµ ëª©í‘œ\n    doc.add_heading(f'{lab_num[-1]}.1 í•™ìŠµ ëª©í‘œ', level=2)\n    for obj in objectives:\n        doc.add_paragraph(f'â€¢ {obj}')\n    \n    # í•µì‹¬ ê°œë…\n    doc.add_heading(f'{lab_num[-1]}.2 í•µì‹¬ ê°œë…', level=2)\n    for concept in key_concepts:\n        doc.add_paragraph(f'â€¢ {concept}')\n    \n    # íŒŒì¼ ì •ë³´\n    doc.add_heading(f'{lab_num[-1]}.3 ì‹¤ìŠµ íŒŒì¼', level=2)\n    doc.add_paragraph(f'íŒŒì¼ëª…: {file_name}')\n    doc.add_paragraph('ìœ„ì¹˜: ê°•ì˜ ìë£Œ í´ë” ë˜ëŠ” AI Driveì—ì„œ ë‹¤ìš´ë¡œë“œ')\n    \n    # ì‹¤í–‰ ë°©ë²•\n    doc.add_heading(f'{lab_num[-1]}.4 ì‹¤í–‰ ë°©ë²•', level=2)\n    exec_steps = [\n        f\"1. {file_name} íŒŒì¼ì„ í”„ë¡œì íŠ¸ í´ë”ì— ì €ì¥\",\n        \"2. í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ ì‹¤í–‰:\",\n        f\"   python {file_name}\",\n        \"3. ì‹¤í–‰ ê²°ê³¼ í™•ì¸ ë° ì½”ë“œ ë¶„ì„\"\n    ]\n    \n    for step in exec_steps:\n        if step.startswith('   '):\n            para = doc.add_paragraph(step)\n            para.style.font.name = 'Consolas'\n        else:\n            doc.add_paragraph(step)\n    \n    return doc\n\n# ì‹¤ìŠµ 1: LangChain ê¸°ë³¸ Agent\nlab1_objectives = [\n    \"LangChainì˜ ê¸°ë³¸ Agent êµ¬ì¡° ì´í•´\",\n    \"Tool ì •ì˜ ë° Agent ì´ˆê¸°í™” ë°©ë²• í•™ìŠµ\", \n    \"@tool ë°ì½”ë ˆì´í„° ì‚¬ìš©ë²• ìŠµë“\",\n    \"Agent ì‹¤í–‰ ë° ë””ë²„ê¹… ë°©ë²• ì´í•´\"\n]\n\nlab1_concepts = [\n    \"@tool ë°ì½”ë ˆì´í„°ë¡œ í•¨ìˆ˜ë¥¼ Toolë¡œ ë³€í™˜\",\n    \"initialize_agentë¡œ Agentì™€ Tools ì—°ê²°\",\n    \"AgentType.CONVERSATIONAL_REACT_DESCRIPTION ì‚¬ìš©\",\n    \"ConversationBufferMemoryë¡œ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬\",\n    \"verbose=Trueë¡œ Agent ì‚¬ê³  ê³¼ì • ê´€ì°°\"\n]\n\ndoc = add_lab_section(doc, \"2\", \"LangChain ê¸°ë³¸ Agent\", lab1_objectives, lab1_concepts, \"lab1_basic_agent.py\")\n\nprint(\"âœ… ì‹¤ìŠµ 1 ì„¹ì…˜ ìƒì„± ì™„ë£Œ\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì‹¤ìŠµ 1 ì„¹ì…˜ ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ì‹¤ìŠµ 2: NewsScraper Tool êµ¬í˜„\nlab2_objectives = [\n    \"Seleniumì„ ì´ìš©í•œ ì•ˆì •ì ì¸ ì›¹ í¬ë¡¤ë§ êµ¬í˜„\",\n    \"Explicit Waitì„ í†µí•œ Flaky Test ë°©ì§€\",\n    \"Firecrawl MCPë¥¼ í™œìš©í•œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ\",\n    \"Toolë¡œ íŒ¨í‚¤ì§•í•˜ì—¬ Agentì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ êµ¬í˜„\"\n]\n\nlab2_concepts = [\n    \"Selenium WebDriver ì„¤ì • ë° Explicit Wait ì‚¬ìš©\",\n    \"CSS Selectorë¥¼ ì´ìš©í•œ ì•ˆì •ì ì¸ ìš”ì†Œ ì„ íƒ\",\n    \"Firecrawl APIë¥¼ í†µí•œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ\",\n    \"Fallback ë©”ì»¤ë‹ˆì¦˜ (Firecrawl ì‹¤íŒ¨ ì‹œ Selenium ì‚¬ìš©)\",\n    \"@tool ë°ì½”ë ˆì´í„°ë¡œ Agentì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ Toolë¡œ ë³€í™˜\"\n]\n\ndoc = add_lab_section(doc, \"3\", \"NewsScraper Tool êµ¬í˜„\", lab2_objectives, lab2_concepts, \"lab2_news_scraper.py\")\n\n# ì‹¤ìŠµ 3: DataAnalyzer Tool êµ¬í˜„ \nlab3_objectives = [\n    \"OpenAI GPT ë˜ëŠ” Google Geminië¥¼ ì´ìš©í•œ ê°ì„± ë¶„ì„ êµ¬í˜„\",\n    \"í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ í†µí•œ ì¼ê´€ëœ JSON ì‘ë‹µ í™•ë³´\",\n    \"ëŒ“ê¸€ ë‹¨ìœ„ ë° ê¸°ì‚¬ ë‹¨ìœ„ ë¶„ì„ ê¸°ëŠ¥ êµ¬í˜„\",\n    \"Toolë¡œ íŒ¨í‚¤ì§•í•˜ì—¬ Agentì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ êµ¬í˜„\"\n]\n\nlab3_concepts = [\n    \"í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ìœ¼ë¡œ ì¼ê´€ëœ JSON ì‘ë‹µ í™•ë³´\",\n    \"OpenAIì™€ Gemini APIì˜ ì°¨ì´ì  ë° ì„ íƒ ë°©ë²•\",\n    \"JSON íŒŒì‹± ë° ì˜ˆì™¸ ì²˜ë¦¬ë¡œ ì•ˆì •ì ì¸ ë°ì´í„° ì¶”ì¶œ\",\n    \"ê°ì„± ë¶„ì„ê³¼ ë™í–¥ ë¶„ì„ì˜ êµ¬ë¶„ ë° í™œìš©\",\n    \"@tool ë°ì½”ë ˆì´í„°ë¡œ Agentì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ Toolë¡œ ë³€í™˜\"\n]\n\ndoc = add_lab_section(doc, \"4\", \"DataAnalyzer Tool êµ¬í˜„\", lab3_objectives, lab3_concepts, \"lab3_data_analyzer.py\")\n\n# ì‹¤ìŠµ 4: Planner Agent êµ¬í˜„\nlab4_objectives = [\n    \"ì—¬ëŸ¬ Toolsë¥¼ í†µí•©í•˜ëŠ” Planner Agent êµ¬í˜„\",\n    \"ìì—°ì–´ ì˜ë„ íŒŒì•… ë° Tool ìˆœì°¨ ì‹¤í–‰\",\n    \"ì‚¬ìš©ì ì§ˆì˜ì— ë”°ë¥¸ ë™ì  Tool ì„ íƒ\",\n    \"ì „ì²´ End-to-End íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\"\n]\n\nlab4_concepts = [\n    \"ì—¬ëŸ¬ Toolsë¥¼ í•˜ë‚˜ì˜ Agentì— í†µí•© ë“±ë¡\",\n    \"ì‚¬ìš©ì ì§ˆì˜ì— ë”°ë¥¸ ë™ì  Tool ì„ íƒ ë° ì‹¤í–‰\",\n    \"ConversationBufferMemoryë¡œ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€\",\n    \"Tool ê°„ ë°ì´í„° ì „ë‹¬ ë° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\",\n    \"Agentì˜ ReAct íŒ¨í„´ (Reason + Act) ê´€ì°°\"\n]\n\ndoc = add_lab_section(doc, \"5\", \"Planner Agent êµ¬í˜„\", lab4_objectives, lab4_concepts, \"lab4_planner_agent.py\")\n\nprint(\"âœ… ì‹¤ìŠµ 2, 3, 4 ì„¹ì…˜ ìƒì„± ì™„ë£Œ\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì‹¤ìŠµ 2, 3, 4 ì„¹ì…˜ ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ ì„¹ì…˜ ì¶”ê°€\ndoc.add_page_break()\ndoc.add_heading('6. íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ', level=1)\n\n# 6.1 ì¼ë°˜ì ì¸ ì˜¤ë¥˜ë“¤\ndoc.add_heading('6.1 ì¼ë°˜ì ì¸ ì˜¤ë¥˜ ë° í•´ê²°ì±…', level=2)\n\ntroubleshooting = [\n    {\n        \"ë¬¸ì œ\": \"ImportError: No module named 'langchain'\",\n        \"í•´ê²°ì±…\": \"pip install langchain ëª…ë ¹ì–´ë¡œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.\"\n    },\n    {\n        \"ë¬¸ì œ\": \"OpenAI API í‚¤ ì˜¤ë¥˜\",\n        \"í•´ê²°ì±…\": \".env íŒŒì¼ì— ì˜¬ë°”ë¥¸ API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\"\n    },\n    {\n        \"ë¬¸ì œ\": \"ChromeDriver ì˜¤ë¥˜\",\n        \"í•´ê²°ì±…\": \"Chrome ë¸Œë¼ìš°ì €ë¥¼ ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ê³  webdriver-managerë¥¼ ì¬ì„¤ì¹˜í•˜ì„¸ìš”.\"\n    },\n    {\n        \"ë¬¸ì œ\": \"Selenium íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜\",\n        \"í•´ê²°ì±…\": \"Explicit Wait ì‹œê°„ì„ ëŠ˜ë¦¬ê±°ë‚˜ ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”.\"\n    },\n    {\n        \"ë¬¸ì œ\": \"JSON íŒŒì‹± ì˜¤ë¥˜\",\n        \"í•´ê²°ì±…\": \"API ì‘ë‹µ í˜•ì‹ì„ í™•ì¸í•˜ê³  ì˜ˆì™¸ ì²˜ë¦¬ ë¡œì§ì„ ì ê²€í•˜ì„¸ìš”.\"\n    }\n]\n\nfor item in troubleshooting:\n    doc.add_paragraph(f\"â€¢ ë¬¸ì œ: {item['ë¬¸ì œ']}\")\n    doc.add_paragraph(f\"  í•´ê²°ì±…: {item['í•´ê²°ì±…']}\")\n    doc.add_paragraph()\n\n# 6.2 ì„±ëŠ¥ ìµœì í™” íŒ\ndoc.add_heading('6.2 ì„±ëŠ¥ ìµœì í™” íŒ', level=2)\noptimization_tips = [\n    \"Selenium WebDriverëŠ” headless ëª¨ë“œë¡œ ì‹¤í–‰í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ì„¸ìš”.\",\n    \"API í˜¸ì¶œ ì‹œ ì ì ˆí•œ Rate Limitì„ ì„¤ì •í•˜ì—¬ ì œí•œì„ í”¼í•˜ì„¸ìš”.\",\n    \"ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í™œìš©í•˜ì„¸ìš”.\",\n    \"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  í•„ìš” ì‹œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì„ ìˆ˜í–‰í•˜ì„¸ìš”.\"\n]\n\nfor tip in optimization_tips:\n    doc.add_paragraph(f\"â€¢ {tip}\")\n\nprint(\"âœ… íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ ìƒì„± ì™„ë£Œ\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ì¶”ê°€ í•™ìŠµ ìë£Œ ì„¹ì…˜ ì¶”ê°€\ndoc.add_page_break()\ndoc.add_heading('7. ì¶”ê°€ í•™ìŠµ ìë£Œ', level=1)\n\n# 7.1 ê³µì‹ ë¬¸ì„œ\ndoc.add_heading('7.1 ê³µì‹ ë¬¸ì„œ', level=2)\nofficial_docs = [\n    \"LangChain ê³µì‹ ë¬¸ì„œ: https://docs.langchain.com/\",\n    \"LangGraph ê³µì‹ ë¬¸ì„œ: https://langchain-ai.github.io/langgraph/\",\n    \"OpenAI API ë¬¸ì„œ: https://platform.openai.com/docs/\",\n    \"Selenium ê³µì‹ ë¬¸ì„œ: https://selenium-python.readthedocs.io/\",\n    \"Google Generative AI ë¬¸ì„œ: https://ai.google.dev/docs/\"\n]\n\nfor doc_link in official_docs:\n    doc.add_paragraph(f\"â€¢ {doc_link}\")\n\n# 7.2 ì˜¨ë¼ì¸ ê°•ì˜ ë° íŠœí† ë¦¬ì–¼\ndoc.add_heading('7.2 ì˜¨ë¼ì¸ ê°•ì˜ ë° íŠœí† ë¦¬ì–¼', level=2)\nonline_resources = [\n    \"DeepLearning.AI - LangChain for LLM Application Development\",\n    \"LangChain Academy (ë¬´ë£Œ ì˜¨ë¼ì¸ ì½”ìŠ¤)\",\n    \"YouTube - LangChain íŠœí† ë¦¬ì–¼ ì‹œë¦¬ì¦ˆ\",\n    \"Coursera - AI Agents ì „ë¬¸ ê³¼ì •\",\n    \"Udemy - Selenium WebDriver ì™„ì „ ì •ë³µ\"\n]\n\nfor resource in online_resources:\n    doc.add_paragraph(f\"â€¢ {resource}\")\n\n# 7.3 ì‹¤ìŠµ í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´\ndoc.add_heading('7.3 ì‹¤ìŠµ í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´', level=2)\nproject_ideas = [\n    \"ì£¼ì‹ ì‹œì¥ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ë° íˆ¬ì ì¸ì‚¬ì´íŠ¸ ì œê³µ ì‹œìŠ¤í…œ\",\n    \"ì†Œì…œ ë¯¸ë””ì–´ íŠ¸ë Œë“œ ë¶„ì„ ë° ë§ˆì¼€íŒ… ì¸ì‚¬ì´íŠ¸ ë„êµ¬\",\n    \"ê³ ê° ë¦¬ë·° ìë™ ë¶„ì„ ë° ì œí’ˆ ê°œì„  ì œì•ˆ ì‹œìŠ¤í…œ\",\n    \"ë‰´ìŠ¤ ê¸°ì‚¬ ìë™ ìš”ì•½ ë° í‚¤ì›Œë“œ ì¶”ì¶œ ë„êµ¬\",\n    \"ë‹¤êµ­ì–´ ëŒ“ê¸€ ë²ˆì—­ ë° ê°ì„± ë¶„ì„ í”Œë«í¼\"\n]\n\nfor idea in project_ideas:\n    doc.add_paragraph(f\"â€¢ {idea}\")\n\n# 7.4 ì»¤ë®¤ë‹ˆí‹° ë° ì§€ì›\ndoc.add_heading('7.4 ì»¤ë®¤ë‹ˆí‹° ë° ì§€ì›', level=2)\ncommunity_resources = [\n    \"LangChain Discord ì»¤ë®¤ë‹ˆí‹°\",\n    \"GitHub Discussions - LangChain ì €ì¥ì†Œ\",\n    \"Stack Overflow - langchain íƒœê·¸\",\n    \"Reddit - r/MachineLearning ì»¤ë®¤ë‹ˆí‹°\",\n    \"êµ­ë‚´ AI ê°œë°œì ì»¤ë®¤ë‹ˆí‹° ë° ë°‹ì—…\"\n]\n\nfor community in community_resources:\n    doc.add_paragraph(f\"â€¢ {community}\")\n\nprint(\"âœ… ì¶”ê°€ í•™ìŠµ ìë£Œ ì„¹ì…˜ ìƒì„± ì™„ë£Œ\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì¶”ê°€ í•™ìŠµ ìë£Œ ì„¹ì…˜ ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {
        "file_changes": [
          {
            "path": "/home/user/output/ai_agent_labs/session1_labs_guide.docx",
            "change_type": "modified",
            "version": 1,
            "tool_call_id": "tooluse_dcsHMOrkS8-L6f9lgCzrww",
            "execution_count": 14,
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/699572e3-3054-4594-ab35-ec14c1a205f3/output/ai_agent_labs/session1_labs_guide.docx",
              "mime_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
              "size_bytes": 39990
            },
            "upload_status": "queued",
            "upload_completed": false,
            "timestamp": "2025-12-16T10:18:23.848466Z"
          }
        ]
      },
      "source": [
        "# Word ë¬¸ì„œ ì €ì¥\ndocx_file = os.path.join(output_dir, \"session1_labs_guide.docx\")\ndoc.save(docx_file)\n\nprint(f\"âœ… Word ë¬¸ì„œ ìƒì„± ì™„ë£Œ: {docx_file}\")\n\n# ìƒì„±ëœ íŒŒì¼ ëª©ë¡ í™•ì¸\nprint(\"\\nğŸ“ ìƒì„±ëœ ì‹¤ìŠµ íŒŒì¼ ëª©ë¡:\")\nimport glob\n\npy_files = glob.glob(os.path.join(output_dir, \"*.py\"))\ndocx_files = glob.glob(os.path.join(output_dir, \"*.docx\"))\n\nfor i, py_file in enumerate(py_files, 1):\n    file_name = os.path.basename(py_file)\n    file_size = os.path.getsize(py_file)\n    print(f\"  {i}. {file_name} ({file_size:,} bytes)\")\n\nfor docx_file in docx_files:\n    file_name = os.path.basename(docx_file)\n    file_size = os.path.getsize(docx_file)\n    print(f\"  ğŸ“„ {file_name} ({file_size:,} bytes)\")\n\nprint(f\"\\nğŸ“Š ì´ íŒŒì¼ ìˆ˜: {len(py_files) + len(docx_files)}ê°œ\")\nprint(f\"   Python ì½”ë“œ íŒŒì¼: {len(py_files)}ê°œ\")\nprint(f\"   Word ë¬¸ì„œ: {len(docx_files)}ê°œ\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Word ë¬¸ì„œ ìƒì„± ì™„ë£Œ: /home/user/output/ai_agent_labs/session1_labs_guide.docx\n\nğŸ“ ìƒì„±ëœ ì‹¤ìŠµ íŒŒì¼ ëª©ë¡:\n  1. lab1_basic_agent.py (4,772 bytes)\n  2. lab2_news_scraper.py (13,908 bytes)\n  3. lab3_data_analyzer.py (16,856 bytes)\n  4. lab4_planner_agent.py (14,267 bytes)\n  ğŸ“„ session1_labs_guide.docx (39,990 bytes)\n\nğŸ“Š ì´ íŒŒì¼ ìˆ˜: 5ê°œ\n   Python ì½”ë“œ íŒŒì¼: 4ê°œ\n   Word ë¬¸ì„œ: 1ê°œ\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1. âœ… ğŸ”´ í™˜ê²½ ì„¤ì • ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ [completed] (ID: 1)\n2. âœ… ğŸ”´ ì‹¤ìŠµ 1: LangChain ê¸°ë³¸ Agent ì½”ë“œ ì‘ì„± (lab1_basic_agent.py) [completed] (ID: 2)\n3. âœ… ğŸ”´ ì‹¤ìŠµ 2: NewsScraper Tool êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab2_news_scraper.py) [completed] (ID: 3)\n4. âœ… ğŸ”´ ì‹¤ìŠµ 3: DataAnalyzer Tool êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab3_data_analyzer.py) [completed] (ID: 4)\n5. âœ… ğŸ”´ ì‹¤ìŠµ 4: Planner Agent êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab4_planner_agent.py) [completed] (ID: 5)\n6. âœ… ğŸŸ¡ Word ë¬¸ì„œ ìƒì„± (session1_labs_guide.docx) [completed] (ID: 6)\n7. ğŸ”„ ğŸŸ¢ íŒŒì¼ì„ AI Driveì— ì—…ë¡œë“œ [in_progress] (ID: 7)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check if AI Drive is mounted with retry handling\nimport time\nmax_retries = 3\nretry_delay = 5\n\nfor attempt in range(max_retries):\n    # Check if AI Drive is mounted\n    mount_output = !mount | grep /mnt/aidrive\n    # Expected output format: ['AIDriveFUSE on /mnt/aidrive type fuse (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)']\n    if mount_output and any('/mnt/aidrive' in line and 'AIDriveFUSE' in line for line in mount_output):\n        print(f\"AI Drive is mounted (attempt {attempt + 1})\")\n        print(f\"Mount info: {mount_output}\")\n        break\n    elif attempt < max_retries - 1:\n        print(f\"AI Drive not mounted (attempt {attempt + 1}), waiting {retry_delay} seconds for mount to complete...\")\n        time.sleep(retry_delay)\n    else:\n        print(f\"AI Drive not mounted after {max_retries * retry_delay} seconds\")\n        if mount_output:\n            print(f\"Current mount output: {mount_output}\")\n        raise RuntimeError(\"AI Drive is not mounted. Please ensure AI Drive is properly configured.\")\n\n# Create parent directory and upload the file\nprint(\"Starting upload process...\")\n!mkdir -p /mnt/aidrive/ai_agent_labs && cp /home/user/output/ai_agent_labs/lab1_basic_agent.py /mnt/aidrive/ai_agent_labs/lab1_basic_agent.py\nprint(\"Upload process finished\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Drive is mounted (attempt 1)\nMount info: ['AIDriveFUSE on /mnt/aidrive type fuse (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)']\nStarting upload process...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload process finished\n"
          ]
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check if AI Drive is mounted with retry handling\nimport time\nmax_retries = 3\nretry_delay = 5\n\nfor attempt in range(max_retries):\n    # Check if AI Drive is mounted\n    mount_output = !mount | grep /mnt/aidrive\n    # Expected output format: ['AIDriveFUSE on /mnt/aidrive type fuse (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)']\n    if mount_output and any('/mnt/aidrive' in line and 'AIDriveFUSE' in line for line in mount_output):\n        print(f\"AI Drive is mounted (attempt {attempt + 1})\")\n        print(f\"Mount info: {mount_output}\")\n        break\n    elif attempt < max_retries - 1:\n        print(f\"AI Drive not mounted (attempt {attempt + 1}), waiting {retry_delay} seconds for mount to complete...\")\n        time.sleep(retry_delay)\n    else:\n        print(f\"AI Drive not mounted after {max_retries * retry_delay} seconds\")\n        if mount_output:\n            print(f\"Current mount output: {mount_output}\")\n        raise RuntimeError(\"AI Drive is not mounted. Please ensure AI Drive is properly configured.\")\n\n# Create parent directory and upload the file\nprint(\"Starting upload process...\")\n!mkdir -p /mnt/aidrive/ai_agent_labs && cp /home/user/output/ai_agent_labs/lab2_news_scraper.py /mnt/aidrive/ai_agent_labs/lab2_news_scraper.py\nprint(\"Upload process finished\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Drive is mounted (attempt 1)\nMount info: ['AIDriveFUSE on /mnt/aidrive type fuse (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)']\nStarting upload process...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload process finished\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check if AI Drive is mounted with retry handling\nimport time\nmax_retries = 3\nretry_delay = 5\n\nfor attempt in range(max_retries):\n    # Check if AI Drive is mounted\n    mount_output = !mount | grep /mnt/aidrive\n    # Expected output format: ['AIDriveFUSE on /mnt/aidrive type fuse (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)']\n    if mount_output and any('/mnt/aidrive' in line and 'AIDriveFUSE' in line for line in mount_output):\n        print(f\"AI Drive is mounted (attempt {attempt + 1})\")\n        print(f\"Mount info: {mount_output}\")\n        break\n    elif attempt < max_retries - 1:\n        print(f\"AI Drive not mounted (attempt {attempt + 1}), waiting {retry_delay} seconds for mount to complete...\")\n        time.sleep(retry_delay)\n    else:\n        print(f\"AI Drive not mounted after {max_retries * retry_delay} seconds\")\n        if mount_output:\n            print(f\"Current mount output: {mount_output}\")\n        raise RuntimeError(\"AI Drive is not mounted. Please ensure AI Drive is properly configured.\")\n\n# Create parent directory and upload the file\nprint(\"Starting upload process...\")\n!mkdir -p /mnt/aidrive/ai_agent_labs && cp /home/user/output/ai_agent_labs/lab3_data_analyzer.py /mnt/aidrive/ai_agent_labs/lab3_data_analyzer.py\nprint(\"Upload process finished\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Drive is mounted (attempt 1)\nMount info: ['AIDriveFUSE on /mnt/aidrive type fuse (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)']\nStarting upload process...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload process finished\n"
          ]
        }
      ],
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check if AI Drive is mounted with retry handling\nimport time\nmax_retries = 3\nretry_delay = 5\n\nfor attempt in range(max_retries):\n    # Check if AI Drive is mounted\n    mount_output = !mount | grep /mnt/aidrive\n    # Expected output format: ['AIDriveFUSE on /mnt/aidrive type fuse (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)']\n    if mount_output and any('/mnt/aidrive' in line and 'AIDriveFUSE' in line for line in mount_output):\n        print(f\"AI Drive is mounted (attempt {attempt + 1})\")\n        print(f\"Mount info: {mount_output}\")\n        break\n    elif attempt < max_retries - 1:\n        print(f\"AI Drive not mounted (attempt {attempt + 1}), waiting {retry_delay} seconds for mount to complete...\")\n        time.sleep(retry_delay)\n    else:\n        print(f\"AI Drive not mounted after {max_retries * retry_delay} seconds\")\n        if mount_output:\n            print(f\"Current mount output: {mount_output}\")\n        raise RuntimeError(\"AI Drive is not mounted. Please ensure AI Drive is properly configured.\")\n\n# Create parent directory and upload the file\nprint(\"Starting upload process...\")\n!mkdir -p /mnt/aidrive/ai_agent_labs && cp /home/user/output/ai_agent_labs/lab4_planner_agent.py /mnt/aidrive/ai_agent_labs/lab4_planner_agent.py\nprint(\"Upload process finished\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Drive is mounted (attempt 1)\nMount info: ['AIDriveFUSE on /mnt/aidrive type fuse (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)']\nStarting upload process...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload process finished\n"
          ]
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check if AI Drive is mounted with retry handling\nimport time\nmax_retries = 3\nretry_delay = 5\n\nfor attempt in range(max_retries):\n    # Check if AI Drive is mounted\n    mount_output = !mount | grep /mnt/aidrive\n    # Expected output format: ['AIDriveFUSE on /mnt/aidrive type fuse (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)']\n    if mount_output and any('/mnt/aidrive' in line and 'AIDriveFUSE' in line for line in mount_output):\n        print(f\"AI Drive is mounted (attempt {attempt + 1})\")\n        print(f\"Mount info: {mount_output}\")\n        break\n    elif attempt < max_retries - 1:\n        print(f\"AI Drive not mounted (attempt {attempt + 1}), waiting {retry_delay} seconds for mount to complete...\")\n        time.sleep(retry_delay)\n    else:\n        print(f\"AI Drive not mounted after {max_retries * retry_delay} seconds\")\n        if mount_output:\n            print(f\"Current mount output: {mount_output}\")\n        raise RuntimeError(\"AI Drive is not mounted. Please ensure AI Drive is properly configured.\")\n\n# Create parent directory and upload the file\nprint(\"Starting upload process...\")\n!mkdir -p /mnt/aidrive/ai_agent_labs && cp /home/user/output/ai_agent_labs/session1_labs_guide.docx /mnt/aidrive/ai_agent_labs/session1_labs_guide.docx\nprint(\"Upload process finished\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Drive is mounted (attempt 1)\nMount info: ['AIDriveFUSE on /mnt/aidrive type fuse (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)']\nStarting upload process...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload process finished\n"
          ]
        }
      ],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1. âœ… ğŸ”´ í™˜ê²½ ì„¤ì • ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ [completed] (ID: 1)\n2. âœ… ğŸ”´ ì‹¤ìŠµ 1: LangChain ê¸°ë³¸ Agent ì½”ë“œ ì‘ì„± (lab1_basic_agent.py) [completed] (ID: 2)\n3. âœ… ğŸ”´ ì‹¤ìŠµ 2: NewsScraper Tool êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab2_news_scraper.py) [completed] (ID: 3)\n4. âœ… ğŸ”´ ì‹¤ìŠµ 3: DataAnalyzer Tool êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab3_data_analyzer.py) [completed] (ID: 4)\n5. âœ… ğŸ”´ ì‹¤ìŠµ 4: Planner Agent êµ¬í˜„ ì½”ë“œ ì‘ì„± (lab4_planner_agent.py) [completed] (ID: 5)\n6. âœ… ğŸŸ¡ Word ë¬¸ì„œ ìƒì„± (session1_labs_guide.docx) [completed] (ID: 6)\n7. âœ… ğŸŸ¢ íŒŒì¼ì„ AI Driveì— ì—…ë¡œë“œ [completed] (ID: 7)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ìµœì¢… ê²°ê³¼ ìš”ì•½\nprint(\"ğŸ‰ AI ì—ì´ì „íŠ¸ ê¸°ë°˜ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì‹œìŠ¤í…œ - 1íšŒì°¨ ì‹¤ìŠµ ì½”ë“œ ìƒì„± ì™„ë£Œ!\")\nprint(\"=\" * 80)\n\nprint(\"\\nğŸ“ ìƒì„±ëœ íŒŒì¼ ëª©ë¡:\")\nprint(\"   1ï¸âƒ£ lab1_basic_agent.py - LangChain ê¸°ë³¸ Agent (4,772 bytes)\")\nprint(\"   2ï¸âƒ£ lab2_news_scraper.py - NewsScraper Tool êµ¬í˜„ (13,908 bytes)\")  \nprint(\"   3ï¸âƒ£ lab3_data_analyzer.py - DataAnalyzer Tool êµ¬í˜„ (16,856 bytes)\")\nprint(\"   4ï¸âƒ£ lab4_planner_agent.py - Planner Agent êµ¬í˜„ (14,267 bytes)\")\nprint(\"   ğŸ“„ session1_labs_guide.docx - í†µí•© ì‹¤ìŠµ ê°€ì´ë“œ (39,990 bytes)\")\n\nprint(\"\\nğŸ“Š ìƒì„± í†µê³„:\")\nprint(f\"   ì´ Python ì½”ë“œ ë¼ì¸: ì•½ 1,500ì¤„\")\nprint(f\"   ì´ íŒŒì¼ í¬ê¸°: ì•½ 90KB\")\nprint(f\"   ì‹¤ìŠµ ì†Œìš” ì˜ˆìƒ ì‹œê°„: 2ì‹œê°„\")\nprint(f\"   ë‚œì´ë„: ëŒ€í•™ì›/ì‹¤ë¬´ ì¤‘ê¸‰\")\n\nprint(\"\\nğŸ”— íŒŒì¼ ìœ„ì¹˜:\")\nprint(\"   ë¡œì»¬: /home/user/output/ai_agent_labs/\")\nprint(\"   AI Drive: /ai_agent_labs/\")\n\nprint(\"\\nğŸ’¡ ê° ì‹¤ìŠµì˜ í•µì‹¬ í¬ì¸íŠ¸:\")\nprint(\"   ì‹¤ìŠµ 1: @tool ë°ì½”ë ˆì´í„°, initialize_agent, ConversationBufferMemory\")\nprint(\"   ì‹¤ìŠµ 2: Selenium Explicit Wait, Firecrawl API, Fallback ë©”ì»¤ë‹ˆì¦˜\")\nprint(\"   ì‹¤ìŠµ 3: í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§, JSON íŒŒì‹±, OpenAI/Gemini API\")\nprint(\"   ì‹¤ìŠµ 4: ë©€í‹° Tool í†µí•©, ReAct íŒ¨í„´, ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€\")\n\nprint(\"\\nâš ï¸  ì‹¤ìŠµ ì „ ì¤€ë¹„ì‚¬í•­:\")\nprint(\"   â€¢ OpenAI API í‚¤ ì¤€ë¹„ ë° .env íŒŒì¼ ì„¤ì •\")\nprint(\"   â€¢ Chrome ë¸Œë¼ìš°ì € ì„¤ì¹˜\")\nprint(\"   â€¢ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (pip install langchain openai selenium ë“±)\")\nprint(\"   â€¢ ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸ (API í˜¸ì¶œ ë° ì›¹ í¬ë¡¤ë§)\")\n\nprint(\"\\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:\")\nprint(\"   1. Word ë¬¸ì„œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì‹¤ìŠµ ê°€ì´ë“œ í™•ì¸\")\nprint(\"   2. Python íŒŒì¼ë“¤ì„ ë¡œì»¬ì— ì €ì¥\")\nprint(\"   3. í™˜ê²½ ì„¤ì • í›„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤ìŠµ ì§„í–‰\")\nprint(\"   4. ê° ì‹¤ìŠµ ì™„ë£Œ í›„ ì½”ë“œ ë¶„ì„ ë° ì´í•´\")\n\nprint(\"\\nâœ¨ ì™„ë£Œ! ì„±ê³µì ì¸ ì‹¤ìŠµë˜ì„¸ìš”!\")\nprint(\"=\" * 80)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ‰ AI ì—ì´ì „íŠ¸ ê¸°ë°˜ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì‹œìŠ¤í…œ - 1íšŒì°¨ ì‹¤ìŠµ ì½”ë“œ ìƒì„± ì™„ë£Œ!\n================================================================================\n\nğŸ“ ìƒì„±ëœ íŒŒì¼ ëª©ë¡:\n   1ï¸âƒ£ lab1_basic_agent.py - LangChain ê¸°ë³¸ Agent (4,772 bytes)\n   2ï¸âƒ£ lab2_news_scraper.py - NewsScraper Tool êµ¬í˜„ (13,908 bytes)\n   3ï¸âƒ£ lab3_data_analyzer.py - DataAnalyzer Tool êµ¬í˜„ (16,856 bytes)\n   4ï¸âƒ£ lab4_planner_agent.py - Planner Agent êµ¬í˜„ (14,267 bytes)\n   ğŸ“„ session1_labs_guide.docx - í†µí•© ì‹¤ìŠµ ê°€ì´ë“œ (39,990 bytes)\n\nğŸ“Š ìƒì„± í†µê³„:\n   ì´ Python ì½”ë“œ ë¼ì¸: ì•½ 1,500ì¤„\n   ì´ íŒŒì¼ í¬ê¸°: ì•½ 90KB\n   ì‹¤ìŠµ ì†Œìš” ì˜ˆìƒ ì‹œê°„: 2ì‹œê°„\n   ë‚œì´ë„: ëŒ€í•™ì›/ì‹¤ë¬´ ì¤‘ê¸‰\n\nğŸ”— íŒŒì¼ ìœ„ì¹˜:\n   ë¡œì»¬: /home/user/output/ai_agent_labs/\n   AI Drive: /ai_agent_labs/\n\nğŸ’¡ ê° ì‹¤ìŠµì˜ í•µì‹¬ í¬ì¸íŠ¸:\n   ì‹¤ìŠµ 1: @tool ë°ì½”ë ˆì´í„°, initialize_agent, ConversationBufferMemory\n   ì‹¤ìŠµ 2: Selenium Explicit Wait, Firecrawl API, Fallback ë©”ì»¤ë‹ˆì¦˜\n   ì‹¤ìŠµ 3: í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§, JSON íŒŒì‹±, OpenAI/Gemini API\n   ì‹¤ìŠµ 4: ë©€í‹° Tool í†µí•©, ReAct íŒ¨í„´, ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€\n\nâš ï¸  ì‹¤ìŠµ ì „ ì¤€ë¹„ì‚¬í•­:\n   â€¢ OpenAI API í‚¤ ì¤€ë¹„ ë° .env íŒŒì¼ ì„¤ì •\n   â€¢ Chrome ë¸Œë¼ìš°ì € ì„¤ì¹˜\n   â€¢ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (pip install langchain openai selenium ë“±)\n   â€¢ ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸ (API í˜¸ì¶œ ë° ì›¹ í¬ë¡¤ë§)\n\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:\n   1. Word ë¬¸ì„œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì‹¤ìŠµ ê°€ì´ë“œ í™•ì¸\n   2. Python íŒŒì¼ë“¤ì„ ë¡œì»¬ì— ì €ì¥\n   3. í™˜ê²½ ì„¤ì • í›„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤ìŠµ ì§„í–‰\n   4. ê° ì‹¤ìŠµ ì™„ë£Œ í›„ ì½”ë“œ ë¶„ì„ ë° ì´í•´\n\nâœ¨ ì™„ë£Œ! ì„±ê³µì ì¸ ì‹¤ìŠµë˜ì„¸ìš”!\n================================================================================\n"
          ]
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### [SUCCESS] Execution Complete\n\n**Task Completion Summary:** AI ì—ì´ì „íŠ¸ ê¸°ë°˜ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì‹œìŠ¤í…œ 1íšŒì°¨ ì‹¤ìŠµ ì½”ë“œ ìƒì„± ì‘ì—…ì„ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. 4ê°œì˜ Python ì‹¤ìŠµ íŒŒì¼(lab1~lab4)ê³¼ 1ê°œì˜ Word ë¬¸ì„œ(ì‹¤ìŠµ ê°€ì´ë“œ)ë¥¼ ìƒì„±í•˜ì—¬ ë¡œì»¬ ë° AI Driveì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n\n**Approaches Used:**\n- Python ì‹¤ìŠµ íŒŒì¼ ìƒì„± (LangChain Agent, NewsScraper Tool, DataAnalyzer Tool, Planner Agent)\n- python-docxë¥¼ ì´ìš©í•œ Word ë¬¸ì„œ ìƒì„±\n- AI Drive ì—…ë¡œë“œë¥¼ í†µí•œ íŒŒì¼ ê³µìœ \n- ìƒì„¸í•œ ì½”ë“œ ì£¼ì„ ë° ì‹¤í–‰ ì˜ˆì œ í¬í•¨\n- í™˜ê²½ ì„¤ì • ê°€ì´ë“œ ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì„¹ì…˜ ì‘ì„±\n\n\n**Files Saved to AI Drive:**\n- `/ai_agent_labs/session1_labs_guide.docx` (uploaded at 2025-12-16T10:18:57.571726)\n- `/ai_agent_labs/lab4_planner_agent.py` (uploaded at 2025-12-16T10:18:52.124523)\n- `/ai_agent_labs/lab3_data_analyzer.py` (uploaded at 2025-12-16T10:18:47.918680)\n- `/ai_agent_labs/lab2_news_scraper.py` (uploaded at 2025-12-16T10:18:43.477092)\n- `/ai_agent_labs/lab1_basic_agent.py` (uploaded at 2025-12-16T10:18:36.696564)\n\n\n\n**Files Saved:**\n- `output/ai_agent_labs/session1_labs_guide.docx` -> [Download](https://www.genspark.ai/api/files/s/e3Rbccc1) (application/vnd.openxmlformats-officedocument.wordprocessingml.document)\n- `output/ai_agent_labs/lab4_planner_agent.py` -> [Download](https://www.genspark.ai/api/files/s/8k1J2Ejv) (text/x-python)\n- `output/ai_agent_labs/lab3_data_analyzer.py` -> [Download](https://www.genspark.ai/api/files/s/NJU6RitM) (text/x-python)\n- `output/ai_agent_labs/lab2_news_scraper.py` -> [Download](https://www.genspark.ai/api/files/s/Qs6onSWT) (text/x-python)\n- `output/ai_agent_labs/lab1_basic_agent.py` -> [Download](https://www.genspark.ai/api/files/s/6aMS83aA) (text/x-python)\n\n\n**Challenges Overcome:** No significant challenges encountered\n\n**Next Steps/Suggestions:** Task completed as requested"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}