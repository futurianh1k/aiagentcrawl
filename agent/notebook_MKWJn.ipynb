{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "í˜ì´ì§€ 6, 7, 8, 10, 11, 12, 20, 21, 29ì˜ ì‹¤ìŠµ ì½”ë“œë¥¼ í¬í•¨í•œ ì‹¤í–‰ ê°€ëŠ¥í•œ íŒ¨í‚¤ì§€ ìƒì„± ë° ZIP ì••ì¶•\n\n**ì‘ì—… ëª©í‘œ**:\n1. ì‹¤ìŠµ ì½”ë“œ íŒ¨í‚¤ì§€ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±\n2. 9ê°œì˜ Python ì‹¤ìŠµ íŒŒì¼ ì‘ì„± (ê° í˜ì´ì§€ì˜ í•µì‹¬ ì½”ë“œ)\n3. ì„¤ì • íŒŒì¼ ì‘ì„± (requirements.txt, .env.example)\n4. README.md ì‘ì„± (ì‹¤í–‰ ê°€ì´ë“œ)\n5. í†µí•© ë°ëª¨ íŒŒì¼ ì‘ì„±\n6. ZIP íŒŒì¼ë¡œ ì••ì¶•\n7. AI Driveì— ì—…ë¡œë“œ\n\n**íŒŒì¼ êµ¬ì¡°**:\n```\nsession3_lab_codes/\nâ”œâ”€â”€ README.md                    # ì‹¤í–‰ ê°€ì´ë“œ\nâ”œâ”€â”€ requirements.txt             # ì˜ì¡´ì„±\nâ”œâ”€â”€ .env.example                 # API í‚¤ í…œí”Œë¦¿\nâ”œâ”€â”€ 01_openai_basic.py          # í˜ì´ì§€ 6\nâ”œâ”€â”€ 02_gemini_basic.py          # í˜ì´ì§€ 7\nâ”œâ”€â”€ 03_prompt_engineering.py    # í˜ì´ì§€ 8\nâ”œâ”€â”€ 04_structured_output.py     # í˜ì´ì§€ 10\nâ”œâ”€â”€ 05_function_calling.py      # í˜ì´ì§€ 11\nâ”œâ”€â”€ 06_data_analyzer_tool.py    # í˜ì´ì§€ 12\nâ”œâ”€â”€ 07_langgraph_sequential.py  # í˜ì´ì§€ 20\nâ”œâ”€â”€ 08_langgraph_conditional.py # í˜ì´ì§€ 21\nâ”œâ”€â”€ 09_langchain_memory.py      # í˜ì´ì§€ 29\nâ””â”€â”€ 10_integrated_demo.py       # í†µí•© ì˜ˆì œ\n```\n\n**ê° íŒŒì¼ ìš”êµ¬ì‚¬í•­**:\n- ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥\n- ìƒì„¸í•œ ì£¼ì„ (í•œê¸€)\n- ì—ëŸ¬ í•¸ë“¤ë§ í¬í•¨\n- ì‹¤í–‰ ì˜ˆì œ í¬í•¨\n- if __name__ == \"__main__\": ë¸”ë¡ í¬í•¨"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1. ğŸ”„ ğŸ”´ ì‹¤ìŠµ íŒ¨í‚¤ì§€ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± [in_progress] (ID: 1)\n2. â³ ğŸ”´ 9ê°œ Python ì‹¤ìŠµ íŒŒì¼ ì‘ì„± (01-09) [pending] (ID: 2)\n3. â³ ğŸ”´ ì„¤ì • íŒŒì¼ ì‘ì„± (requirements.txt, .env.example) [pending] (ID: 3)\n4. â³ ğŸ”´ README.md ì‹¤í–‰ ê°€ì´ë“œ ì‘ì„± [pending] (ID: 4)\n5. â³ ğŸ”´ í†µí•© ë°ëª¨ íŒŒì¼ ì‘ì„± (10_integrated_demo.py) [pending] (ID: 5)\n6. â³ ğŸ”´ ZIP íŒŒì¼ ì••ì¶• ë° AI Drive ì—…ë¡œë“œ [pending] (ID: 6)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n\n# ì‹¤ìŠµ íŒ¨í‚¤ì§€ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±\npackage_name = \"session3_lab_codes\"\noutput_dir = f\"/home/user/output/{package_name}\"\n\n# ë””ë ‰í† ë¦¬ ìƒì„±\nos.makedirs(output_dir, exist_ok=True)\n\nprint(f\"âœ… ì‹¤ìŠµ íŒ¨í‚¤ì§€ ë””ë ‰í† ë¦¬ ìƒì„±: {output_dir}\")\nprint(f\"ğŸ“‚ íŒŒì¼ êµ¬ì¡°:\")\nprint(f\"   {package_name}/\")\nprint(f\"   â”œâ”€â”€ README.md\")\nprint(f\"   â”œâ”€â”€ requirements.txt\") \nprint(f\"   â”œâ”€â”€ .env.example\")\nprint(f\"   â”œâ”€â”€ 01_openai_basic.py\")\nprint(f\"   â”œâ”€â”€ 02_gemini_basic.py\")\nprint(f\"   â”œâ”€â”€ 03_prompt_engineering.py\")\nprint(f\"   â”œâ”€â”€ 04_structured_output.py\")\nprint(f\"   â”œâ”€â”€ 05_function_calling.py\")\nprint(f\"   â”œâ”€â”€ 06_data_analyzer_tool.py\")\nprint(f\"   â”œâ”€â”€ 07_langgraph_sequential.py\")\nprint(f\"   â”œâ”€â”€ 08_langgraph_conditional.py\")\nprint(f\"   â”œâ”€â”€ 09_langchain_memory.py\")\nprint(f\"   â””â”€â”€ 10_integrated_demo.py\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì‹¤ìŠµ íŒ¨í‚¤ì§€ ë””ë ‰í† ë¦¬ ìƒì„±: /home/user/output/session3_lab_codes\nğŸ“‚ íŒŒì¼ êµ¬ì¡°:\n   session3_lab_codes/\n   â”œâ”€â”€ README.md\n   â”œâ”€â”€ requirements.txt\n   â”œâ”€â”€ .env.example\n   â”œâ”€â”€ 01_openai_basic.py\n   â”œâ”€â”€ 02_gemini_basic.py\n   â”œâ”€â”€ 03_prompt_engineering.py\n   â”œâ”€â”€ 04_structured_output.py\n   â”œâ”€â”€ 05_function_calling.py\n   â”œâ”€â”€ 06_data_analyzer_tool.py\n   â”œâ”€â”€ 07_langgraph_sequential.py\n   â”œâ”€â”€ 08_langgraph_conditional.py\n   â”œâ”€â”€ 09_langchain_memory.py\n   â””â”€â”€ 10_integrated_demo.py\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1. âœ… ğŸ”´ ì‹¤ìŠµ íŒ¨í‚¤ì§€ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± [completed] (ID: 1)\n2. ğŸ”„ ğŸ”´ 01_openai_basic.py ì‘ì„± (í˜ì´ì§€ 6) [in_progress] (ID: 2)\n3. â³ ğŸ”´ 02_gemini_basic.py ì‘ì„± (í˜ì´ì§€ 7) [pending] (ID: 3)\n4. â³ ğŸ”´ 03-09 Python ì‹¤ìŠµ íŒŒì¼ ì‘ì„± [pending] (ID: 4)\n5. â³ ğŸ”´ ì„¤ì • íŒŒì¼ ì‘ì„± [pending] (ID: 5)\n6. â³ ğŸ”´ README.md ì‘ì„± [pending] (ID: 6)\n7. â³ ğŸ”´ í†µí•© ë°ëª¨ ì‘ì„± [pending] (ID: 7)\n8. â³ ğŸ”´ ZIP ì••ì¶• ë° ì—…ë¡œë“œ [pending] (ID: 8)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {
        "file_changes": [
          {
            "path": "/home/user/output/session3_lab_codes/01_openai_basic.py",
            "change_type": "modified",
            "version": 1,
            "tool_call_id": "tooluse_G3STfpvnTmOQgjWwXjFjJQ",
            "execution_count": 2,
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/e4fc7fe4-aa99-4ffb-8bbf-faf411e242fa/output/session3_lab_codes/01_openai_basic.py",
              "mime_type": "text/x-python",
              "size_bytes": 4961
            },
            "upload_status": "queued",
            "upload_completed": false,
            "timestamp": "2025-12-18T07:07:11.524840Z"
          }
        ]
      },
      "source": [
        "# 01_openai_basic.py - í˜ì´ì§€ 6: OpenAI API ê¸°ì´ˆ\nopenai_basic_code = '''\"\"\"\n3íšŒì°¨ ì‹¤ìŠµ 01: OpenAI API ê¸°ì´ˆ\ní˜ì´ì§€ 6 - Chat Completions API ì‚¬ìš©ë²•\n\nì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” OpenAI Chat Completions APIì˜ ê¸°ë³¸ ì‚¬ìš©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n- API í‚¤ ì„¤ì •\n- ê¸°ë³¸ Chat Completions í˜¸ì¶œ\n- ëª¨ë¸ ì„ íƒ (GPT-4 vs GPT-3.5-turbo)\n- ì˜¨ë„, ë§¥ìŠ¤ í† í° ë“± íŒŒë¼ë¯¸í„° ì¡°ì •\n\"\"\"\n\nimport os\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\n\n# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\nload_dotenv()\n\ndef setup_openai_client():\n    \"\"\"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\"\"\"\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"OPENAI_API_KEYê°€ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\")\n    \n    client = OpenAI(api_key=api_key)\n    print(\"âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n    return client\n\ndef basic_chat_completion(client, user_message, model=\"gpt-4\"):\n    \"\"\"ê¸°ë³¸ Chat Completions API í˜¸ì¶œ\"\"\"\n    try:\n        response = client.chat.completions.create(\n            model=model,\n            messages=[\n                {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\"},\n                {\"role\": \"user\", \"content\": user_message}\n            ],\n            temperature=0.7,  # ì°½ì˜ì„± ì¡°ì ˆ (0.0-2.0)\n            max_tokens=1000,  # ìµœëŒ€ í† í° ìˆ˜\n        )\n        \n        return response.choices[0].message.content\n    \n    except Exception as e:\n        print(f\"âŒ API í˜¸ì¶œ ì˜¤ë¥˜: {e}\")\n        return None\n\ndef sentiment_analysis_example(client):\n    \"\"\"ê°ì„± ë¶„ì„ ì˜ˆì œ\"\"\"\n    comment = \"ì´ ì •ì±…ì€ ì •ë§ ìµœì•…ì´ì—ìš”. ì™„ì „íˆ ì‹¤ë§í–ˆìŠµë‹ˆë‹¤.\"\n    \n    system_prompt = \"\"\"ë‹¹ì‹ ì€ ì „ë¬¸ ê°ì„± ë¶„ì„ê°€ì…ë‹ˆë‹¤. \n    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ ê°ì„±ì„ 'ê¸ì •', 'ë¶€ì •', 'ì¤‘ë¦½' ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ê³  \n    ê·¸ ì´ìœ ë¥¼ ê°„ëµíˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\"\"\n    \n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": f\"ë‹¤ìŒ ëŒ“ê¸€ì„ ë¶„ì„í•´ì£¼ì„¸ìš”: {comment}\"}\n            ],\n            temperature=0.3,  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„\n        )\n        \n        return response.choices[0].message.content\n    \n    except Exception as e:\n        print(f\"âŒ ê°ì„± ë¶„ì„ ì˜¤ë¥˜: {e}\")\n        return None\n\ndef compare_models(client, prompt):\n    \"\"\"GPT-4ì™€ GPT-3.5-turbo ë¹„êµ\"\"\"\n    models = [\"gpt-4\", \"gpt-3.5-turbo\"]\n    results = {}\n    \n    for model in models:\n        print(f\"\\\\nğŸ”„ {model} ì‘ë‹µ ìƒì„± ì¤‘...\")\n        try:\n            response = client.chat.completions.create(\n                model=model,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.7,\n                max_tokens=200\n            )\n            \n            results[model] = response.choices[0].message.content\n            \n            # í† í° ì‚¬ìš©ëŸ‰ í‘œì‹œ (ê°€ëŠ¥í•œ ê²½ìš°)\n            if hasattr(response, 'usage'):\n                usage = response.usage\n                print(f\"  ğŸ“Š í† í° ì‚¬ìš©ëŸ‰: {usage.prompt_tokens} + {usage.completion_tokens} = {usage.total_tokens}\")\n        \n        except Exception as e:\n            results[model] = f\"ì˜¤ë¥˜: {e}\"\n    \n    return results\n\nif __name__ == \"__main__\":\n    print(\"ğŸš€ OpenAI API ê¸°ì´ˆ ì‹¤ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!\")\n    print(\"=\" * 50)\n    \n    try:\n        # 1. OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n        client = setup_openai_client()\n        \n        # 2. ê¸°ë³¸ ì±„íŒ… ì˜ˆì œ\n        print(\"\\\\n1ï¸âƒ£ ê¸°ë³¸ ì±„íŒ… ì˜ˆì œ\")\n        print(\"-\" * 30)\n        user_input = \"AI ì—ì´ì „íŠ¸ë€ ë¬´ì—‡ì¸ê°€ìš”?\"\n        response = basic_chat_completion(client, user_input)\n        if response:\n            print(f\"ğŸ‘¤ ì‚¬ìš©ì: {user_input}\")\n            print(f\"ğŸ¤– GPT-4: {response}\")\n        \n        # 3. ê°ì„± ë¶„ì„ ì˜ˆì œ\n        print(\"\\\\n2ï¸âƒ£ ê°ì„± ë¶„ì„ ì˜ˆì œ\")\n        print(\"-\" * 30)\n        sentiment_result = sentiment_analysis_example(client)\n        if sentiment_result:\n            print(f\"ğŸ“Š ë¶„ì„ ê²°ê³¼:\\\\n{sentiment_result}\")\n        \n        # 4. ëª¨ë¸ ë¹„êµ ì˜ˆì œ\n        print(\"\\\\n3ï¸âƒ£ GPT-4 vs GPT-3.5-turbo ë¹„êµ\")\n        print(\"-\" * 30)\n        comparison_prompt = \"Multi-Agent ì‹œìŠ¤í…œì˜ ì¥ì ì„ 3ê°€ì§€ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.\"\n        comparison_results = compare_models(client, comparison_prompt)\n        \n        for model, result in comparison_results.items():\n            print(f\"\\\\nğŸ”¹ {model.upper()}:\")\n            print(result)\n        \n        print(\"\\\\nâœ… ì‹¤ìŠµ ì™„ë£Œ!\")\n        print(\"\\\\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:\")\n        print(\"   - 02_gemini_basic.py: Google Gemini API ì‹¤ìŠµ\")\n        print(\"   - 03_prompt_engineering.py: í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§\")\n    \n    except Exception as e:\n        print(f\"âŒ ì „ì²´ ì‹¤ìŠµ ì˜¤ë¥˜: {e}\")\n        print(\"\\\\nğŸ”§ í•´ê²° ë°©ë²•:\")\n        print(\"   1. .env íŒŒì¼ì— OPENAI_API_KEY ì„¤ì • í™•ì¸\")\n        print(\"   2. pip install openai python-dotenv\")\n        print(\"   3. API í‚¤ ìœ íš¨ì„± ë° í¬ë ˆë”§ ì”ì•¡ í™•ì¸\")\n'''\n\n# íŒŒì¼ ì €ì¥\nwith open(f\"{output_dir}/01_openai_basic.py\", \"w\", encoding=\"utf-8\") as f:\n    f.write(openai_basic_code)\n\nprint(\"âœ… 01_openai_basic.py ìƒì„± ì™„ë£Œ\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… 01_openai_basic.py ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1. âœ… ğŸ”´ ì‹¤ìŠµ íŒ¨í‚¤ì§€ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± [completed] (ID: 1)\n2. âœ… ğŸ”´ 01_openai_basic.py ì‘ì„± (í˜ì´ì§€ 6) [completed] (ID: 2)\n3. ğŸ”„ ğŸ”´ 02_gemini_basic.py ì‘ì„± (í˜ì´ì§€ 7) [in_progress] (ID: 3)\n4. â³ ğŸ”´ 03-09 Python ì‹¤ìŠµ íŒŒì¼ ì‘ì„± [pending] (ID: 4)\n5. â³ ğŸ”´ ì„¤ì • íŒŒì¼ ì‘ì„± [pending] (ID: 5)\n6. â³ ğŸ”´ README.md ì‘ì„± [pending] (ID: 6)\n7. â³ ğŸ”´ í†µí•© ë°ëª¨ ì‘ì„± [pending] (ID: 7)\n8. â³ ğŸ”´ ZIP ì••ì¶• ë° ì—…ë¡œë“œ [pending] (ID: 8)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {
        "file_changes": [
          {
            "path": "/home/user/output/session3_lab_codes/02_gemini_basic.py",
            "change_type": "modified",
            "version": 1,
            "tool_call_id": "tooluse__l44fHA6SH21s7ifd81qbQ",
            "execution_count": 3,
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/1832fc7b-ea31-4833-bca9-96a2b92115bf/output/session3_lab_codes/02_gemini_basic.py",
              "mime_type": "text/x-python",
              "size_bytes": 6640
            },
            "upload_status": "queued",
            "upload_completed": false,
            "timestamp": "2025-12-18T07:07:48.869626Z"
          }
        ]
      },
      "source": [
        "# 02_gemini_basic.py - í˜ì´ì§€ 7: Google Gemini API ê¸°ì´ˆ\ngemini_basic_code = '''\"\"\"\n3íšŒì°¨ ì‹¤ìŠµ 02: Google Gemini API ê¸°ì´ˆ\ní˜ì´ì§€ 7 - Gemini Pro ì‚¬ìš©ë²•\n\nì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Google Gemini APIì˜ ê¸°ë³¸ ì‚¬ìš©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n- Gemini API í‚¤ ì„¤ì •\n- Gemini Pro ëª¨ë¸ ì‚¬ìš©\n- OpenAI APIì™€ì˜ ë¹„êµ\n- ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ ì†Œê°œ\n\"\"\"\n\nimport os\nimport google.generativeai as genai\nfrom dotenv import load_dotenv\n\n# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\nload_dotenv()\n\ndef setup_gemini_client():\n    \"\"\"Gemini API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\"\"\"\n    api_key = os.getenv(\"GEMINI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"GEMINI_API_KEYê°€ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\")\n    \n    genai.configure(api_key=api_key)\n    model = genai.GenerativeModel('gemini-pro')\n    print(\"âœ… Gemini Pro í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n    return model\n\ndef basic_gemini_generation(model, prompt):\n    \"\"\"ê¸°ë³¸ Gemini í…ìŠ¤íŠ¸ ìƒì„±\"\"\"\n    try:\n        response = model.generate_content(prompt)\n        return response.text\n    except Exception as e:\n        print(f\"âŒ Gemini API í˜¸ì¶œ ì˜¤ë¥˜: {e}\")\n        return None\n\ndef gemini_sentiment_analysis(model, comment):\n    \"\"\"Geminië¥¼ ì´ìš©í•œ ê°ì„± ë¶„ì„\"\"\"\n    prompt = f\"\"\"ë‹¹ì‹ ì€ ì „ë¬¸ ë‰´ìŠ¤ ëŒ“ê¸€ ê°ì„± ë¶„ì„ê°€ì…ë‹ˆë‹¤.\n    \n    ë‹¤ìŒ ëŒ“ê¸€ì˜ ê°ì„±ì„ ë¶„ì„í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:\n    ëŒ“ê¸€: \"{comment}\"\n    \n    ì‘ë‹µ í˜•ì‹:\n    {{\n        \"sentiment\": \"ê¸ì •|ë¶€ì •|ì¤‘ë¦½\",\n        \"confidence\": 0.0-1.0,\n        \"reason\": \"ë¶„ì„ ê·¼ê±°\"\n    }}\"\"\"\n    \n    try:\n        response = model.generate_content(prompt)\n        return response.text\n    except Exception as e:\n        print(f\"âŒ Gemini ê°ì„± ë¶„ì„ ì˜¤ë¥˜: {e}\")\n        return None\n\ndef compare_gemini_features():\n    \"\"\"Geminiì˜ íŠ¹ì§• ì„¤ëª…\"\"\"\n    features = {\n        \"ì¥ì \": [\n            \"ğŸŒ ë©€í‹°ëª¨ë‹¬ ì§€ì› (í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤)\",\n            \"ğŸ“„ ê¸´ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° (32K tokens, ì¼ë¶€ ëª¨ë¸ì€ 1M+ tokens)\",\n            \"ğŸ’° ê²½ìŸë ¥ ìˆëŠ” ê°€ê²© (OpenAI ëŒ€ë¹„ ì €ë ´)\",\n            \"âš¡ ë¹ ë¥¸ ì‘ë‹µ ì†ë„\",\n            \"ğŸ”’ Googleì˜ ì•ˆì „ì„± í•„í„°ë§\"\n        ],\n        \"OpenAI vs Gemini\": {\n            \"OpenAI GPT-4\": {\n                \"ì¥ì \": \"ì„±ìˆ™í•œ ìƒíƒœê³„, í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ìë£Œ í’ë¶€\",\n                \"ë‹¨ì \": \"ë†’ì€ ë¹„ìš©, ì»¨í…ìŠ¤íŠ¸ ì œí•œ\"\n            },\n            \"Google Gemini\": {\n                \"ì¥ì \": \"ë©€í‹°ëª¨ë‹¬, ê¸´ ì»¨í…ìŠ¤íŠ¸, ì €ë ´í•œ ë¹„ìš©\",\n                \"ë‹¨ì \": \"ìƒëŒ€ì ìœ¼ë¡œ ìƒˆë¡œìš´ í”Œë«í¼, ì ì€ ìë£Œ\"\n            }\n        }\n    }\n    return features\n\ndef multi_modal_example_info():\n    \"\"\"ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ ì˜ˆì œ ì •ë³´ (ì‹¤ì œ ì´ë¯¸ì§€ ì—†ì´ ì„¤ëª…ë§Œ)\"\"\"\n    info = \"\"\"\n    ğŸ–¼ï¸ Gemini ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ ì˜ˆì œ (ì°¸ê³ ìš©):\n    \n    # ì´ë¯¸ì§€ ë¶„ì„ ì˜ˆì œ (ì‹¤ì œ ì´ë¯¸ì§€ê°€ ìˆì„ ë•Œ)\n    import PIL.Image\n    \n    # ì´ë¯¸ì§€ ë¡œë“œ\n    img = PIL.Image.open('screenshot.jpg')\n    \n    # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ë¶„ì„\n    response = model.generate_content([\n        \"ì´ ìŠ¤í¬ë¦°ìƒ·ì—ì„œ UI ìš”ì†Œë“¤ì„ ë¶„ì„í•˜ê³  ê°œì„ ì ì„ ì œì•ˆí•´ì£¼ì„¸ìš”.\",\n        img\n    ])\n    \n    print(response.text)\n    \n    ğŸ¯ í™œìš© ì‚¬ë¡€:\n    - ë‰´ìŠ¤ ê¸°ì‚¬ ì´ë¯¸ì§€ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n    - ì°¨íŠ¸/ê·¸ë˜í”„ ë°ì´í„° ë¶„ì„\n    - UI/UX ìŠ¤í¬ë¦°ìƒ· ë¶„ì„\n    - ë¬¸ì„œ ì´ë¯¸ì§€ì—ì„œ ì •ë³´ ì¶”ì¶œ\n    \"\"\"\n    return info\n\ndef gemini_prompt_engineering_tips():\n    \"\"\"Gemini í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ íŒ\"\"\"\n    tips = \"\"\"\n    ğŸ’¡ Gemini í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ íŒ:\n    \n    1. ëª…í™•í•œ ì§€ì‹œì‚¬í•­:\n       - \"JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”\" âœ…\n       - \"ê²°ê³¼ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”\" âŒ\n    \n    2. ì˜ˆì‹œ ì œê³µ:\n       - Few-shot í”„ë¡¬í”„íŠ¸ íš¨ê³¼ì \n       - ì›í•˜ëŠ” ì¶œë ¥ í˜•ì‹ ëª…ì‹œ\n    \n    3. ì»¨í…ìŠ¤íŠ¸ í™œìš©:\n       - ê¸´ ë¬¸ì„œ ì²˜ë¦¬ì— ê°•ì \n       - ì „ì²´ ë§¥ë½ì„ ê³ ë ¤í•œ ë¶„ì„ ê°€ëŠ¥\n    \n    4. ì•ˆì „ í•„í„°ë§:\n       - Googleì˜ ì—„ê²©í•œ ì•ˆì „ ì •ì±…\n       - ë¯¼ê°í•œ ë‚´ìš© í•„í„°ë§ë¨\n    \"\"\"\n    return tips\n\nif __name__ == \"__main__\":\n    print(\"ğŸš€ Google Gemini API ê¸°ì´ˆ ì‹¤ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!\")\n    print(\"=\" * 60)\n    \n    try:\n        # 1. Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n        model = setup_gemini_client()\n        \n        # 2. ê¸°ë³¸ í…ìŠ¤íŠ¸ ìƒì„± ì˜ˆì œ\n        print(\"\\\\n1ï¸âƒ£ ê¸°ë³¸ í…ìŠ¤íŠ¸ ìƒì„± ì˜ˆì œ\")\n        print(\"-\" * 40)\n        basic_prompt = \"AI ì—ì´ì „íŠ¸ì˜ ì •ì˜ì™€ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n        basic_response = basic_gemini_generation(model, basic_prompt)\n        if basic_response:\n            print(f\"ğŸ“ ì§ˆë¬¸: {basic_prompt}\")\n            print(f\"ğŸ¤– Gemini: {basic_response}\")\n        \n        # 3. ê°ì„± ë¶„ì„ ì˜ˆì œ\n        print(\"\\\\n2ï¸âƒ£ Gemini ê°ì„± ë¶„ì„ ì˜ˆì œ\")\n        print(\"-\" * 40)\n        test_comment = \"ìƒˆë¡œìš´ AI ê¸°ìˆ ì´ ì •ë§ í˜ì‹ ì ì´ë„¤ìš”! ì•ìœ¼ë¡œê°€ ê¸°ëŒ€ë©ë‹ˆë‹¤.\"\n        sentiment_result = gemini_sentiment_analysis(model, test_comment)\n        if sentiment_result:\n            print(f\"ğŸ’¬ ëŒ“ê¸€: {test_comment}\")\n            print(f\"ğŸ“Š ë¶„ì„ ê²°ê³¼:\\\\n{sentiment_result}\")\n        \n        # 4. Gemini íŠ¹ì§• ë¹„êµ\n        print(\"\\\\n3ï¸âƒ£ Gemini vs OpenAI íŠ¹ì§• ë¹„êµ\")\n        print(\"-\" * 40)\n        features = compare_gemini_features()\n        \n        print(\"ğŸ”¥ Geminiì˜ ì£¼ìš” ì¥ì :\")\n        for advantage in features[\"ì¥ì \"]:\n            print(f\"   {advantage}\")\n        \n        print(\"\\\\nâš–ï¸ í”Œë«í¼ ë¹„êµ:\")\n        for platform, details in features[\"OpenAI vs Gemini\"].items():\n            print(f\"\\\\nğŸ“± {platform}:\")\n            for key, value in details.items():\n                print(f\"   {key}: {value}\")\n        \n        # 5. ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ ì†Œê°œ\n        print(\"\\\\n4ï¸âƒ£ ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ ì†Œê°œ\")\n        print(\"-\" * 40)\n        multimodal_info = multi_modal_example_info()\n        print(multimodal_info)\n        \n        # 6. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ íŒ\n        print(\"\\\\n5ï¸âƒ£ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ íŒ\")\n        print(\"-\" * 40)\n        tips = gemini_prompt_engineering_tips()\n        print(tips)\n        \n        print(\"\\\\nâœ… Gemini API ì‹¤ìŠµ ì™„ë£Œ!\")\n        print(\"\\\\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:\")\n        print(\"   - 03_prompt_engineering.py: í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì‹¬í™”\")\n        print(\"   - OpenAIì™€ Gemini ê²°ê³¼ ë¹„êµí•´ë³´ê¸°\")\n    \n    except Exception as e:\n        print(f\"âŒ ì „ì²´ ì‹¤ìŠµ ì˜¤ë¥˜: {e}\")\n        print(\"\\\\nğŸ”§ í•´ê²° ë°©ë²•:\")\n        print(\"   1. .env íŒŒì¼ì— GEMINI_API_KEY ì„¤ì • í™•ì¸\")\n        print(\"   2. pip install google-generativeai\")\n        print(\"   3. Google AI Studioì—ì„œ API í‚¤ ë°œê¸‰\")\n        print(\"   4. https://makersuite.google.com/app/apikey\")\n'''\n\n# íŒŒì¼ ì €ì¥\nwith open(f\"{output_dir}/02_gemini_basic.py\", \"w\", encoding=\"utf-8\") as f:\n    f.write(gemini_basic_code)\n\nprint(\"âœ… 02_gemini_basic.py ìƒì„± ì™„ë£Œ\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… 02_gemini_basic.py ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1. âœ… ğŸ”´ ì‹¤ìŠµ íŒ¨í‚¤ì§€ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± [completed] (ID: 1)\n2. âœ… ğŸ”´ 01_openai_basic.py ì‘ì„± (í˜ì´ì§€ 6) [completed] (ID: 2)\n3. âœ… ğŸ”´ 02_gemini_basic.py ì‘ì„± (í˜ì´ì§€ 7) [completed] (ID: 3)\n4. ğŸ”„ ğŸ”´ 03_prompt_engineering.py ì‘ì„± (í˜ì´ì§€ 8) [in_progress] (ID: 4)\n5. â³ ğŸ”´ 04-09 Python ì‹¤ìŠµ íŒŒì¼ ì‘ì„± [pending] (ID: 5)\n6. â³ ğŸ”´ ì„¤ì • íŒŒì¼ ì‘ì„± [pending] (ID: 6)\n7. â³ ğŸ”´ README.md ì‘ì„± [pending] (ID: 7)\n8. â³ ğŸ”´ í†µí•© ë°ëª¨ ì‘ì„± [pending] (ID: 8)\n9. â³ ğŸ”´ ZIP ì••ì¶• ë° ì—…ë¡œë“œ [pending] (ID: 9)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {
        "file_changes": [
          {
            "path": "/home/user/output/session3_lab_codes/03_prompt_engineering.py",
            "change_type": "modified",
            "version": 1,
            "tool_call_id": "tooluse_Mvlt_ILdSUGJT_PDfExf7g",
            "execution_count": 4,
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/805fda98-8763-437d-9b79-6645e190c4fc/output/session3_lab_codes/03_prompt_engineering.py",
              "mime_type": "text/x-python",
              "size_bytes": 8586
            },
            "upload_status": "queued",
            "upload_completed": false,
            "timestamp": "2025-12-18T07:08:36.166160Z"
          }
        ]
      },
      "source": [
        "# 03_prompt_engineering.py - í˜ì´ì§€ 8: í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ì´ˆ\nprompt_engineering_code = '''\"\"\"\n3íšŒì°¨ ì‹¤ìŠµ 03: í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ì´ˆ\ní˜ì´ì§€ 8 - ë‚˜ìœ/ì¢‹ì€ í”„ë¡¬í”„íŠ¸ ë¹„êµ\n\nì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì˜ í•µì‹¬ ì›ì¹™ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n- ë‚˜ìœ í”„ë¡¬í”„íŠ¸ vs ì¢‹ì€ í”„ë¡¬í”„íŠ¸ ë¹„êµ\n- ì¼ê´€ì„± í™•ë³´ë¥¼ ìœ„í•œ ì²´í¬ë¦¬ìŠ¤íŠ¸\n- ì‹¤ì œ ê°ì„± ë¶„ì„ í”„ë¡¬í”„íŠ¸ ê°œì„ \n- ì¶œë ¥ í˜•ì‹ ì œì–´ ë°©ë²•\n\"\"\"\n\nimport os\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nimport json\n\n# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\nload_dotenv()\n\ndef setup_openai_client():\n    \"\"\"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\"\"\"\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"OPENAI_API_KEYê°€ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n    \n    return OpenAI(api_key=api_key)\n\n# ë‚˜ìœ í”„ë¡¬í”„íŠ¸ ì˜ˆì œ\nBAD_PROMPT = \"ì´ ëŒ“ê¸€ì˜ ê°ì„±ì„ ë¶„ì„í•´ì¤˜: {comment}\"\n\n# ì¢‹ì€ í”„ë¡¬í”„íŠ¸ ì˜ˆì œ\nGOOD_PROMPT = \"\"\"ë‹¹ì‹ ì€ ì „ë¬¸ ë‰´ìŠ¤ ëŒ“ê¸€ ê°ì„± ë¶„ì„ê°€ì…ë‹ˆë‹¤.\n\në‹¤ìŒ ëŒ“ê¸€ì˜ ê°ì„±ì„ ë¶„ì„í•˜ê³ , ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.\n\nëŒ“ê¸€: {comment}\n\në¶„ë¥˜ ê¸°ì¤€:\n- ê¸ì •: ì§€ì§€, ì¹­ì°¬, ê¸°ëŒ€ê°, ë§Œì¡±ê°\n- ë¶€ì •: ë¹„íŒ, ë¶„ë…¸, ì‹¤ë§, ìš°ë ¤\n- ì¤‘ë¦½: ë‹¨ìˆœ ì‚¬ì‹¤ ì „ë‹¬, ì§ˆë¬¸, ê· í˜•ì¡íŒ ì˜ê²¬\n\nì‘ë‹µ í˜•ì‹ (JSON):\n{{\n  \"sentiment\": \"ê¸ì •|ë¶€ì •|ì¤‘ë¦½\",\n  \"confidence\": 0.0-1.0,\n  \"reason\": \"ë¶„ì„ ê·¼ê±° (í•œ ë¬¸ì¥)\",\n  \"keywords\": [\"í•µì‹¬\", \"í‚¤ì›Œë“œ\", \"ëª©ë¡\"]\n}}\n\nì£¼ì˜ì‚¬í•­:\n- ê°ì •ì  ë‹¨ì–´ì— ì£¼ëª©í•˜ì„¸ìš”\n- ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì„¸ìš”\n- í™•ì‹ ì´ ì—†ìœ¼ë©´ confidenceë¥¼ ë‚®ê²Œ ì„¤ì •í•˜ì„¸ìš”\"\"\"\n\ndef test_bad_prompt(client, comment):\n    \"\"\"ë‚˜ìœ í”„ë¡¬í”„íŠ¸ë¡œ í…ŒìŠ¤íŠ¸\"\"\"\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": BAD_PROMPT.format(comment=comment)}],\n            temperature=0.3\n        )\n        return response.choices[0].message.content\n    except Exception as e:\n        return f\"ì˜¤ë¥˜: {e}\"\n\ndef test_good_prompt(client, comment):\n    \"\"\"ì¢‹ì€ í”„ë¡¬í”„íŠ¸ë¡œ í…ŒìŠ¤íŠ¸\"\"\"\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": GOOD_PROMPT.format(comment=comment)}],\n            temperature=0.3\n        )\n        return response.choices[0].message.content\n    except Exception as e:\n        return f\"ì˜¤ë¥˜: {e}\"\n\ndef compare_prompts(client, test_comments):\n    \"\"\"í”„ë¡¬í”„íŠ¸ ë¹„êµ ì‹¤í–‰\"\"\"\n    results = []\n    \n    for comment in test_comments:\n        print(f\"\\\\nğŸ“ í…ŒìŠ¤íŠ¸ ëŒ“ê¸€: \\\\\"{comment}\\\\\"\")\n        print(\"=\" * 60)\n        \n        # ë‚˜ìœ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸\n        print(\"âŒ ë‚˜ìœ í”„ë¡¬í”„íŠ¸ ê²°ê³¼:\")\n        bad_result = test_bad_prompt(client, comment)\n        print(bad_result)\n        \n        # ì¢‹ì€ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸\n        print(\"\\\\nâœ… ì¢‹ì€ í”„ë¡¬í”„íŠ¸ ê²°ê³¼:\")\n        good_result = test_good_prompt(client, comment)\n        print(good_result)\n        \n        results.append({\n            \"comment\": comment,\n            \"bad_result\": bad_result,\n            \"good_result\": good_result\n        })\n        \n        print(\"\\\\n\" + \"-\" * 60)\n    \n    return results\n\ndef analyze_json_parsing(result_text):\n    \"\"\"JSON íŒŒì‹± ì„±ê³µë¥  í™•ì¸\"\"\"\n    try:\n        # JSON ì¶”ì¶œ ì‹œë„\n        if '{' in result_text and '}' in result_text:\n            start = result_text.find('{')\n            end = result_text.rfind('}') + 1\n            json_str = result_text[start:end]\n            parsed = json.loads(json_str)\n            return True, parsed\n        else:\n            return False, \"JSON í˜•ì‹ ì—†ìŒ\"\n    except json.JSONDecodeError as e:\n        return False, f\"JSON íŒŒì‹± ì˜¤ë¥˜: {e}\"\n\ndef prompt_engineering_checklist():\n    \"\"\"í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì²´í¬ë¦¬ìŠ¤íŠ¸\"\"\"\n    checklist = \"\"\"\n    âœ… í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì²´í¬ë¦¬ìŠ¤íŠ¸:\n    \n    1. ğŸ“‹ ì—­í•  ì •ì˜ (í˜ë¥´ì†Œë‚˜)\n       âœ… \"ë‹¹ì‹ ì€ ì „ë¬¸ ê°ì„± ë¶„ì„ê°€ì…ë‹ˆë‹¤\"\n       âŒ ì—­í•  ì—†ìŒ\n    \n    2. ğŸ¯ ëª…í™•í•œ íƒœìŠ¤í¬ ì •ì˜\n       âœ… \"ë‹¤ìŒ ëŒ“ê¸€ì˜ ê°ì„±ì„ ë¶„ì„í•˜ê³ \"\n       âŒ \"ë¶„ì„í•´ì¤˜\"\n    \n    3. ğŸ“ ë¶„ë¥˜ ê¸°ì¤€ ì œì‹œ\n       âœ… ê¸ì •/ë¶€ì •/ì¤‘ë¦½ì˜ êµ¬ì²´ì  ê¸°ì¤€\n       âŒ ê¸°ì¤€ ì—†ìŒ\n    \n    4. ğŸ”§ ì¶œë ¥ í˜•ì‹ ê°•ì œ\n       âœ… JSON ìŠ¤í‚¤ë§ˆ ëª…ì‹œ\n       âŒ ììœ  í˜•ì‹\n    \n    5. ğŸ“š ì˜ˆì‹œ ì œê³µ (Few-shot)\n       âœ… ì…ë ¥-ì¶œë ¥ ì˜ˆì œ 1-3ê°œ\n       âŒ ì˜ˆì‹œ ì—†ìŒ\n    \n    6. ğŸš« ì œì•½ì‚¬í•­ ëª…ì‹œ\n       âœ… ê¸ˆì§€ì–´, ì£¼ì˜ì‚¬í•­\n       âŒ ì œì•½ ì—†ìŒ\n    \n    7. ğŸšï¸ ì˜¨ë„ ì¡°ì ˆ\n       âœ… ì¼ê´€ì„±: 0.0-0.3\n       âœ… ì°½ì˜ì„±: 0.7-1.0\n    \"\"\"\n    return checklist\n\ndef common_prompt_mistakes():\n    \"\"\"í”í•œ í”„ë¡¬í”„íŠ¸ ì‹¤ìˆ˜ë“¤\"\"\"\n    mistakes = \"\"\"\n    âŒ í”í•œ í”„ë¡¬í”„íŠ¸ ì‹¤ìˆ˜ë“¤:\n    \n    1. ëª¨í˜¸í•œ ì§€ì‹œì‚¬í•­\n       ë‚˜ìœ ì˜ˆ: \"ì´ê²ƒì„ ë¶„ì„í•´ì¤˜\"\n       ì¢‹ì€ ì˜ˆ: \"ì´ ëŒ“ê¸€ì˜ ê°ì„±ì„ ê¸ì •/ë¶€ì •/ì¤‘ë¦½ìœ¼ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”\"\n    \n    2. í˜•ì‹ ë¶ˆì¼ì¹˜\n       ë‚˜ìœ ì˜ˆ: ë§¤ë²ˆ ë‹¤ë¥¸ ì¶œë ¥ í˜•ì‹\n       ì¢‹ì€ ì˜ˆ: JSON ìŠ¤í‚¤ë§ˆ ê°•ì œ\n    \n    3. ì»¨í…ìŠ¤íŠ¸ ë¶€ì¡±\n       ë‚˜ìœ ì˜ˆ: ë‹¨ìˆœ í…ìŠ¤íŠ¸ë§Œ ì œê³µ\n       ì¢‹ì€ ì˜ˆ: ë°°ê²½ ì„¤ëª…, ëª©ì  ëª…ì‹œ\n    \n    4. ì˜ˆì™¸ ì²˜ë¦¬ ë¶€ì¡±\n       ë‚˜ìœ ì˜ˆ: í™•ì‹  ì—†ëŠ” ê²½ìš° ì²˜ë¦¬ ì•ˆí•¨\n       ì¢‹ì€ ì˜ˆ: confidence ì ìˆ˜, ê¸°ë³¸ê°’ ì œê³µ\n    \n    5. ê³¼ë„í•œ ë³µì¡ì„±\n       ë‚˜ìœ ì˜ˆ: 10ê°œ ì´ìƒì˜ ë¶„ë¥˜, ë³µì¡í•œ ì¡°ê±´\n       ì¢‹ì€ ì˜ˆ: 3-5ê°œ ë¶„ë¥˜, ëª…í™•í•œ ê¸°ì¤€\n    \"\"\"\n    return mistakes\n\nif __name__ == \"__main__\":\n    print(\"ğŸš€ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ì´ˆ ì‹¤ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!\")\n    print(\"=\" * 70)\n    \n    # í…ŒìŠ¤íŠ¸ìš© ëŒ“ê¸€ë“¤\n    test_comments = [\n        \"ì •ë¶€ ì •ì±…ì´ ì •ë§ ìµœì•…ì´ë‹¤. ì™„ì „íˆ ì‹¤ë§í–ˆì–´ìš”.\",\n        \"ìƒˆë¡œìš´ ê¸°ìˆ ì´ í˜ì‹ ì ì´ë„¤ìš”! ê¸°ëŒ€ê°€ ë©ë‹ˆë‹¤.\",\n        \"ì˜¤ëŠ˜ ë‚ ì”¨ê°€ íë¦¬ê³  ë¹„ê°€ ì˜¬ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\",\n        \"ì´ ì œí’ˆ ê°€ê²©ì€ ì–¼ë§ˆì¸ê°€ìš”? êµ¬ë§¤ë¥¼ ê³ ë ¤ ì¤‘ì…ë‹ˆë‹¤.\",\n        \"ì •ë§ í›Œë¥­í•œ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ê°•ë ¥ ì¶”ì²œí•©ë‹ˆë‹¤!\"\n    ]\n    \n    try:\n        # 1. OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n        client = setup_openai_client()\n        print(\"âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n        \n        # 2. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì¶œë ¥\n        print(\"\\\\n1ï¸âƒ£ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì²´í¬ë¦¬ìŠ¤íŠ¸\")\n        checklist = prompt_engineering_checklist()\n        print(checklist)\n        \n        # 3. í”í•œ ì‹¤ìˆ˜ë“¤ ì†Œê°œ\n        print(\"\\\\n2ï¸âƒ£ í”í•œ í”„ë¡¬í”„íŠ¸ ì‹¤ìˆ˜ë“¤\")\n        mistakes = common_prompt_mistakes()\n        print(mistakes)\n        \n        # 4. ì‹¤ì œ ë¹„êµ í…ŒìŠ¤íŠ¸\n        print(\"\\\\n3ï¸âƒ£ ë‚˜ìœ í”„ë¡¬í”„íŠ¸ vs ì¢‹ì€ í”„ë¡¬í”„íŠ¸ ë¹„êµ\")\n        print(\"\\\\nğŸ” í”„ë¡¬í”„íŠ¸ ë¹„êµ:\")\n        print(f\"âŒ ë‚˜ìœ í”„ë¡¬í”„íŠ¸: \\\\\"{BAD_PROMPT}\\\\\"\")\n        print(f\"âœ… ì¢‹ì€ í”„ë¡¬í”„íŠ¸: (êµ¬ì¡°í™”ëœ í˜•ì‹, {len(GOOD_PROMPT.split())}ë‹¨ì–´)\")\n        \n        # ë¹„êµ ì‹¤í–‰ (ì²« ë²ˆì§¸ ëŒ“ê¸€ë§Œ ì˜ˆì‹œë¡œ)\n        print(f\"\\\\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ (ì˜ˆì‹œ):\")\n        test_comment = test_comments[0]\n        results = compare_prompts(client, [test_comment])\n        \n        # 5. JSON íŒŒì‹± ì„±ê³µë¥  í™•ì¸\n        if results:\n            result = results[0]\n            print(\"\\\\n4ï¸âƒ£ JSON íŒŒì‹± ì„±ê³µë¥  ë¹„êµ\")\n            \n            # ë‚˜ìœ í”„ë¡¬í”„íŠ¸ JSON íŒŒì‹±\n            bad_success, bad_parsed = analyze_json_parsing(result[\"bad_result\"])\n            print(f\"âŒ ë‚˜ìœ í”„ë¡¬í”„íŠ¸ JSON íŒŒì‹±: {'ì„±ê³µ' if bad_success else 'ì‹¤íŒ¨'}\")\n            if not bad_success:\n                print(f\"   ì‚¬ìœ : {bad_parsed}\")\n            \n            # ì¢‹ì€ í”„ë¡¬í”„íŠ¸ JSON íŒŒì‹±\n            good_success, good_parsed = analyze_json_parsing(result[\"good_result\"])\n            print(f\"âœ… ì¢‹ì€ í”„ë¡¬í”„íŠ¸ JSON íŒŒì‹±: {'ì„±ê³µ' if good_success else 'ì‹¤íŒ¨'}\")\n            if good_success:\n                print(f\"   íŒŒì‹±ëœ ë°ì´í„°: {good_parsed}\")\n        \n        print(\"\\\\nâœ… í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì‹¤ìŠµ ì™„ë£Œ!\")\n        print(\"\\\\nğŸ’¡ í•µì‹¬ êµí›ˆ:\")\n        print(\"   1. ëª…í™•í•œ ì—­í• ê³¼ ì§€ì‹œì‚¬í•­ ì œê³µ\")\n        print(\"   2. ì¶œë ¥ í˜•ì‹ì„ JSONìœ¼ë¡œ ê°•ì œ\")\n        print(\"   3. ë¶„ë¥˜ ê¸°ì¤€ì„ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œ\")\n        print(\"   4. ì˜¨ë„ ì„¤ì •ìœ¼ë¡œ ì¼ê´€ì„± í™•ë³´\")\n        print(\"\\\\nğŸ“š ë‹¤ìŒ ë‹¨ê³„:\")\n        print(\"   - 04_structured_output.py: Pydantic ê¸°ë°˜ Type-safe ì¶œë ¥\")\n        print(\"   - 05_function_calling.py: Function Calling íŒ¨í„´\")\n    \n    except Exception as e:\n        print(f\"âŒ ì‹¤ìŠµ ì˜¤ë¥˜: {e}\")\n        print(\"\\\\nğŸ”§ í•´ê²° ë°©ë²•:\")\n        print(\"   1. API í‚¤ ì„¤ì • í™•ì¸\")\n        print(\"   2. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸\")\n        print(\"   3. API í¬ë ˆë”§ ì”ì•¡ í™•ì¸\")\n'''\n\n# íŒŒì¼ ì €ì¥\nwith open(f\"{output_dir}/03_prompt_engineering.py\", \"w\", encoding=\"utf-8\") as f:\n    f.write(prompt_engineering_code)\n\nprint(\"âœ… 03_prompt_engineering.py ìƒì„± ì™„ë£Œ\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… 03_prompt_engineering.py ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {
        "file_changes": [
          {
            "path": "/home/user/output/session3_lab_codes/04_structured_output.py",
            "change_type": "modified",
            "version": 1,
            "tool_call_id": "tooluse_HHApkz9CRJSwhai_IAM51g",
            "execution_count": 5,
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/15431816-ea02-49a4-a525-af0aaadaff54/output/session3_lab_codes/04_structured_output.py",
              "mime_type": "text/x-python",
              "size_bytes": 10312
            },
            "upload_status": "queued",
            "upload_completed": false,
            "timestamp": "2025-12-18T07:09:20.047197Z"
          }
        ]
      },
      "source": [
        "# 04_structured_output.py - í˜ì´ì§€ 10: Pydantic ê¸°ë°˜ êµ¬ì¡°í™”ëœ ì¶œë ¥\nstructured_output_code = '''\"\"\"\n3íšŒì°¨ ì‹¤ìŠµ 04: Pydantic ê¸°ë°˜ êµ¬ì¡°í™”ëœ ì¶œë ¥\ní˜ì´ì§€ 10 - Structured Output (OpenAI)\n\nì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” OpenAIì˜ Structured Output ê¸°ëŠ¥ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n- Pydantic ëª¨ë¸ì„ ì´ìš©í•œ Type-safe ì¶œë ¥\n- ìë™ ê²€ì¦ ë° ë³€í™˜\n- íŒŒì‹± ì‹¤íŒ¨ ìµœì†Œí™”\n- í”„ë¡œë•ì…˜ê¸‰ ì•ˆì •ì„± í™•ë³´\n\"\"\"\n\nimport os\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom typing import Literal, List, Optional\nfrom dotenv import load_dotenv\nimport json\n\n# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\nload_dotenv()\n\nclass SentimentAnalysis(BaseModel):\n    \"\"\"ê°ì„± ë¶„ì„ ê²°ê³¼ ëª¨ë¸\"\"\"\n    sentiment: Literal[\"ê¸ì •\", \"ë¶€ì •\", \"ì¤‘ë¦½\"] = Field(\n        description=\"ëŒ“ê¸€ì˜ ê°ì„± ë¶„ë¥˜\"\n    )\n    confidence: float = Field(\n        ge=0.0, le=1.0,\n        description=\"ë¶„ë¥˜ ì‹ ë¢°ë„ (0.0-1.0)\"\n    )\n    reason: str = Field(\n        min_length=5, max_length=200,\n        description=\"ê°ì„± ë¶„ë¥˜ ê·¼ê±°\"\n    )\n    keywords: List[str] = Field(\n        description=\"ê°ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” í•µì‹¬ í‚¤ì›Œë“œ\",\n        max_items=5\n    )\n    is_sarcasm: Optional[bool] = Field(\n        default=None,\n        description=\"ë°˜ì–´ë²•/ë¹„ê¼¬ëŠ” í‘œí˜„ ì—¬ë¶€\"\n    )\n\nclass BatchSentimentAnalysis(BaseModel):\n    \"\"\"ë°°ì¹˜ ê°ì„± ë¶„ì„ ê²°ê³¼ ëª¨ë¸\"\"\"\n    total_comments: int = Field(description=\"ì „ì²´ ëŒ“ê¸€ ìˆ˜\")\n    results: List[SentimentAnalysis] = Field(description=\"ê°œë³„ ë¶„ì„ ê²°ê³¼\")\n    summary: dict = Field(description=\"ìš”ì•½ í†µê³„\")\n\ndef setup_openai_client():\n    \"\"\"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\"\"\"\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"OPENAI_API_KEYê°€ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n    \n    return OpenAI(api_key=api_key)\n\ndef analyze_with_structured_output(client, comment):\n    \"\"\"Structured Outputì„ ì‚¬ìš©í•œ ê°ì„± ë¶„ì„\"\"\"\n    system_prompt = \"\"\"ë‹¹ì‹ ì€ ì „ë¬¸ ë‰´ìŠ¤ ëŒ“ê¸€ ê°ì„± ë¶„ì„ê°€ì…ë‹ˆë‹¤.\n    ì£¼ì–´ì§„ ëŒ“ê¸€ì„ ë¶„ì„í•˜ì—¬ ê°ì„±, ì‹ ë¢°ë„, ê·¼ê±°, í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì œê³µí•˜ì„¸ìš”.\n    \n    ë¶„ë¥˜ ê¸°ì¤€:\n    - ê¸ì •: ì§€ì§€, ì¹­ì°¬, ê¸°ëŒ€ê°, ë§Œì¡±\n    - ë¶€ì •: ë¹„íŒ, ë¶„ë…¸, ì‹¤ë§, ìš°ë ¤  \n    - ì¤‘ë¦½: ì‚¬ì‹¤ ì „ë‹¬, ì§ˆë¬¸, ê· í˜• ì˜ê²¬\n    \n    ë°˜ì–´ë²•ì´ë‚˜ ë¹„ê¼¬ëŠ” í‘œí˜„ë„ ê°ì§€í•´ì£¼ì„¸ìš”.\"\"\"\n    \n    try:\n        response = client.beta.chat.completions.parse(\n            model=\"gpt-4o-2024-08-06\",  # Structured Output ì§€ì› ëª¨ë¸\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": f\"ë‹¤ìŒ ëŒ“ê¸€ì„ ë¶„ì„í•˜ì„¸ìš”: {comment}\"}\n            ],\n            response_format=SentimentAnalysis,\n            temperature=0.3\n        )\n        \n        return response.choices[0].message.parsed\n    \n    except Exception as e:\n        print(f\"âŒ Structured Output ì˜¤ë¥˜: {e}\")\n        return None\n\ndef compare_traditional_vs_structured(client, comment):\n    \"\"\"ì „í†µì  ë°©ì‹ vs Structured Output ë¹„êµ\"\"\"\n    \n    # 1. ì „í†µì  JSON ë°©ì‹\n    traditional_prompt = f\"\"\"ëŒ“ê¸€ì˜ ê°ì„±ì„ ë¶„ì„í•˜ê³  JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.\n    ëŒ“ê¸€: {comment}\n    \n    JSON í˜•ì‹:\n    {{\n        \"sentiment\": \"ê¸ì •|ë¶€ì •|ì¤‘ë¦½\",\n        \"confidence\": 0.0-1.0,\n        \"reason\": \"ë¶„ì„ ê·¼ê±°\"\n    }}\"\"\"\n    \n    print(\"1ï¸âƒ£ ì „í†µì  JSON ë°©ì‹\")\n    try:\n        traditional_response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": traditional_prompt}],\n            temperature=0.3\n        )\n        traditional_text = traditional_response.choices[0].message.content\n        print(f\"ğŸ“„ ì›ë³¸ ì‘ë‹µ: {traditional_text}\")\n        \n        # JSON íŒŒì‹± ì‹œë„\n        try:\n            if '{' in traditional_text:\n                start = traditional_text.find('{')\n                end = traditional_text.rfind('}') + 1\n                json_str = traditional_text[start:end]\n                traditional_parsed = json.loads(json_str)\n                print(f\"âœ… íŒŒì‹± ì„±ê³µ: {traditional_parsed}\")\n            else:\n                print(\"âŒ JSON í˜•ì‹ ì—†ìŒ\")\n                traditional_parsed = None\n        except json.JSONDecodeError as e:\n            print(f\"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}\")\n            traditional_parsed = None\n    \n    except Exception as e:\n        print(f\"âŒ ì „í†µì  ë°©ì‹ ì˜¤ë¥˜: {e}\")\n        traditional_parsed = None\n    \n    print(\"\\\\n\" + \"-\" * 50)\n    \n    # 2. Structured Output ë°©ì‹\n    print(\"2ï¸âƒ£ Structured Output ë°©ì‹\")\n    structured_result = analyze_with_structured_output(client, comment)\n    \n    if structured_result:\n        print(f\"âœ… Type-safe ê²°ê³¼:\")\n        print(f\"   ê°ì„±: {structured_result.sentiment}\")\n        print(f\"   ì‹ ë¢°ë„: {structured_result.confidence:.2f}\")\n        print(f\"   ê·¼ê±°: {structured_result.reason}\")\n        print(f\"   í‚¤ì›Œë“œ: {structured_result.keywords}\")\n        print(f\"   ë°˜ì–´ë²•: {structured_result.is_sarcasm}\")\n        \n        # Pydantic ëª¨ë¸ì˜ ì¥ì  ì‹œì—°\n        print(f\"\\\\nğŸ”§ Type-safe ì ‘ê·¼:\")\n        print(f\"   structured_result.sentiment: {structured_result.sentiment}\")\n        print(f\"   type(structured_result.confidence): {type(structured_result.confidence)}\")\n    \n    return traditional_parsed, structured_result\n\ndef batch_analysis_example(client, comments):\n    \"\"\"ë°°ì¹˜ ë¶„ì„ ì˜ˆì œ\"\"\"\n    print(\"ğŸ”„ ë°°ì¹˜ ê°ì„± ë¶„ì„ ì‹¤í–‰ ì¤‘...\")\n    \n    results = []\n    for i, comment in enumerate(comments, 1):\n        print(f\"   {i}/{len(comments)} ì²˜ë¦¬ ì¤‘...\")\n        result = analyze_with_structured_output(client, comment)\n        if result:\n            results.append(result)\n    \n    # ìš”ì•½ í†µê³„ ê³„ì‚°\n    if results:\n        sentiment_counts = {}\n        total_confidence = 0\n        \n        for result in results:\n            sentiment = result.sentiment\n            sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1\n            total_confidence += result.confidence\n        \n        summary = {\n            \"sentiment_distribution\": sentiment_counts,\n            \"average_confidence\": total_confidence / len(results),\n            \"total_analyzed\": len(results)\n        }\n        \n        # BatchSentimentAnalysis ëª¨ë¸ë¡œ ë˜í•‘\n        batch_result = BatchSentimentAnalysis(\n            total_comments=len(comments),\n            results=results,\n            summary=summary\n        )\n        \n        return batch_result\n    \n    return None\n\ndef demonstrate_validation():\n    \"\"\"Pydantic ê²€ì¦ ê¸°ëŠ¥ ì‹œì—°\"\"\"\n    print(\"ğŸ” Pydantic ë°ì´í„° ê²€ì¦ ì‹œì—°\")\n    \n    # 1. ì˜¬ë°”ë¥¸ ë°ì´í„°\n    try:\n        valid_data = SentimentAnalysis(\n            sentiment=\"ê¸ì •\",\n            confidence=0.85,\n            reason=\"ê¸ì •ì ì¸ í‘œí˜„ì´ ë§ì´ ì‚¬ìš©ë¨\",\n            keywords=[\"ì¢‹ë‹¤\", \"í›Œë¥­í•˜ë‹¤\"],\n            is_sarcasm=False\n        )\n        print(f\"âœ… ìœ íš¨í•œ ë°ì´í„°: {valid_data.sentiment}\")\n    except Exception as e:\n        print(f\"âŒ ìœ íš¨í•œ ë°ì´í„° ì˜¤ë¥˜: {e}\")\n    \n    # 2. ì˜ëª»ëœ ë°ì´í„° (ë²”ìœ„ ì´ˆê³¼)\n    try:\n        invalid_data = SentimentAnalysis(\n            sentiment=\"ê¸ì •\",\n            confidence=1.5,  # ë²”ìœ„ ì´ˆê³¼\n            reason=\"ì§§ìŒ\",     # ë„ˆë¬´ ì§§ìŒ\n            keywords=[\"í‚¤ì›Œë“œ1\", \"í‚¤ì›Œë“œ2\", \"í‚¤ì›Œë“œ3\", \"í‚¤ì›Œë“œ4\", \"í‚¤ì›Œë“œ5\", \"í‚¤ì›Œë“œ6\"]  # ë„ˆë¬´ ë§ìŒ\n        )\n        print(f\"ì´ ì¤„ì€ ì‹¤í–‰ë˜ë©´ ì•ˆë©ë‹ˆë‹¤: {invalid_data}\")\n    except Exception as e:\n        print(f\"âœ… ê²€ì¦ ì˜¤ë¥˜ ì •ìƒ ê°ì§€: {type(e).__name__}\")\n    \n    # 3. ì˜ëª»ëœ ê°ì„± ê°’\n    try:\n        invalid_sentiment = SentimentAnalysis(\n            sentiment=\"ë§¤ìš°ì¢‹ìŒ\",  # Literal íƒ€ì…ì— ì—†ëŠ” ê°’\n            confidence=0.8,\n            reason=\"ì ì ˆí•œ ê¸¸ì´ì˜ ê·¼ê±°ì…ë‹ˆë‹¤\",\n            keywords=[\"í‚¤ì›Œë“œ\"]\n        )\n        print(f\"ì´ ì¤„ì€ ì‹¤í–‰ë˜ë©´ ì•ˆë©ë‹ˆë‹¤: {invalid_sentiment}\")\n    except Exception as e:\n        print(f\"âœ… Literal íƒ€ì… ì˜¤ë¥˜ ì •ìƒ ê°ì§€: {type(e).__name__}\")\n\nif __name__ == \"__main__\":\n    print(\"ğŸš€ Pydantic ê¸°ë°˜ êµ¬ì¡°í™”ëœ ì¶œë ¥ ì‹¤ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!\")\n    print(\"=\" * 70)\n    \n    # í…ŒìŠ¤íŠ¸ìš© ëŒ“ê¸€ë“¤\n    test_comments = [\n        \"ì •ë§ í›Œë¥­í•œ ì •ì±…ì…ë‹ˆë‹¤! ì ê·¹ ì§€ì§€í•©ë‹ˆë‹¤.\",\n        \"ì´ê±´ ì •ë§ ìµœì•…ì˜ ê²°ì •ì´ë„¤ìš”. ì‹¤ë§ì…ë‹ˆë‹¤.\",\n        \"ë‚´ì¼ íšŒì˜ ì‹œê°„ì´ ì–¸ì œì¸ê°€ìš”?\"\n    ]\n    \n    try:\n        # 1. OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n        client = setup_openai_client()\n        print(\"âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n        \n        # 2. Pydantic ê²€ì¦ ì‹œì—°\n        print(\"\\\\n1ï¸âƒ£ Pydantic ë°ì´í„° ê²€ì¦ ì‹œì—°\")\n        print(\"-\" * 40)\n        demonstrate_validation()\n        \n        # 3. ì „í†µì  ë°©ì‹ vs Structured Output ë¹„êµ\n        print(\"\\\\n2ï¸âƒ£ ì „í†µì  ë°©ì‹ vs Structured Output ë¹„êµ\")\n        print(\"-\" * 50)\n        test_comment = test_comments[0]\n        print(f\"ğŸ“ í…ŒìŠ¤íŠ¸ ëŒ“ê¸€: \\\\\"{test_comment}\\\\\"\")\n        print()\n        \n        traditional, structured = compare_traditional_vs_structured(client, test_comment)\n        \n        # 4. ë°°ì¹˜ ë¶„ì„ ì˜ˆì œ\n        print(\"\\\\n3ï¸âƒ£ ë°°ì¹˜ ê°ì„± ë¶„ì„\")\n        print(\"-\" * 40)\n        batch_result = batch_analysis_example(client, test_comments)\n        \n        if batch_result:\n            print(f\"\\\\nğŸ“Š ë°°ì¹˜ ë¶„ì„ ê²°ê³¼:\")\n            print(f\"   ì „ì²´ ëŒ“ê¸€: {batch_result.total_comments}ê°œ\")\n            print(f\"   ë¶„ì„ ì™„ë£Œ: {batch_result.summary['total_analyzed']}ê°œ\")\n            print(f\"   í‰ê·  ì‹ ë¢°ë„: {batch_result.summary['average_confidence']:.3f}\")\n            print(f\"   ê°ì„± ë¶„í¬: {batch_result.summary['sentiment_distribution']}\")\n        \n        print(\"\\\\nâœ… Structured Output ì‹¤ìŠµ ì™„ë£Œ!\")\n        print(\"\\\\nğŸ’¡ í•µì‹¬ ì¥ì :\")\n        print(\"   1. Type-safe: ëŸ°íƒ€ì„ ì˜¤ë¥˜ ë°©ì§€\")\n        print(\"   2. ìë™ ê²€ì¦: ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥\")\n        print(\"   3. IDE ì§€ì›: ìë™ì™„ì„±, íƒ€ì… ì²´í¬\")\n        print(\"   4. íŒŒì‹± ì‹¤íŒ¨ ì œê±°: JSON ì˜¤ë¥˜ ì—†ìŒ\")\n        print(\"\\\\nğŸ“š ë‹¤ìŒ ë‹¨ê³„:\")\n        print(\"   - 05_function_calling.py: Function Callingìœ¼ë¡œ Tool ìë™ ì„ íƒ\")\n        print(\"   - 06_data_analyzer_tool.py: í”„ë¡œë•ì…˜ê¸‰ ê°ì„± ë¶„ì„ Tool\")\n    \n    except Exception as e:\n        print(f\"âŒ ì‹¤ìŠµ ì˜¤ë¥˜: {e}\")\n        print(\"\\\\nğŸ”§ í•´ê²° ë°©ë²•:\")\n        print(\"   1. OpenAI API í‚¤ í™•ì¸\")\n        print(\"   2. gpt-4o-2024-08-06 ëª¨ë¸ ì•¡ì„¸ìŠ¤ í™•ì¸\")\n        print(\"   3. pip install pydantic\")\n'''\n\n# íŒŒì¼ ì €ì¥\nwith open(f\"{output_dir}/04_structured_output.py\", \"w\", encoding=\"utf-8\") as f:\n    f.write(structured_output_code)\n\nprint(\"âœ… 04_structured_output.py ìƒì„± ì™„ë£Œ\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… 04_structured_output.py ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1. âœ… ğŸ”´ ì‹¤ìŠµ íŒ¨í‚¤ì§€ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± [completed] (ID: 1)\n2. âœ… ğŸ”´ 01-04 Python ì‹¤ìŠµ íŒŒì¼ ì‘ì„± [completed] (ID: 2)\n3. ğŸ”„ ğŸ”´ 05-09 Python ì‹¤ìŠµ íŒŒì¼ ì‘ì„± [in_progress] (ID: 3)\n4. â³ ğŸ”´ ì„¤ì • íŒŒì¼ ì‘ì„± [pending] (ID: 4)\n5. â³ ğŸ”´ README.md ì‘ì„± [pending] (ID: 5)\n6. â³ ğŸ”´ í†µí•© ë°ëª¨ ì‘ì„± [pending] (ID: 6)\n7. â³ ğŸ”´ ZIP ì••ì¶• ë° ì—…ë¡œë“œ [pending] (ID: 7)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {
        "file_changes": [
          {
            "path": "/home/user/output/session3_lab_codes/05_function_calling.py",
            "change_type": "modified",
            "version": 1,
            "tool_call_id": "tooluse_lAQQvfzmSueYajmPg7so9Q",
            "execution_count": 6,
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/704e19a6-9357-4f71-93ee-eec833152139/output/session3_lab_codes/05_function_calling.py",
              "mime_type": "text/x-python",
              "size_bytes": 12813
            },
            "upload_status": "queued",
            "upload_completed": false,
            "timestamp": "2025-12-18T07:10:16.055379Z"
          }
        ]
      },
      "source": [
        "# 05_function_calling.py - í˜ì´ì§€ 11: Function Calling íŒ¨í„´\nfunction_calling_code = '''\"\"\"\n3íšŒì°¨ ì‹¤ìŠµ 05: Function Calling íŒ¨í„´\ní˜ì´ì§€ 11 - Tool ì •ì˜ ë° ìë™ í˜¸ì¶œ\n\nì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” OpenAI Function Calling ê¸°ëŠ¥ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n- Tool ì •ì˜ ë° ìŠ¤í‚¤ë§ˆ ì‘ì„±\n- LLMì´ ì ì ˆí•œ Tool ìë™ ì„ íƒ\n- ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ì˜ ìë™í™”\n- ë‹¤ì¤‘ Tool ì²´ì¸ ì‹¤í–‰\n\"\"\"\n\nimport os\nimport json\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nfrom typing import Dict, Any\n\n# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\nload_dotenv()\n\ndef setup_openai_client():\n    \"\"\"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\"\"\"\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"OPENAI_API_KEYê°€ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n    \n    return OpenAI(api_key=api_key)\n\n# Tool í•¨ìˆ˜ë“¤ ì •ì˜\ndef analyze_sentiment(comment: str, language: str = \"ko\") -> Dict[str, Any]:\n    \"\"\"ê°ì„± ë¶„ì„ Tool í•¨ìˆ˜\"\"\"\n    # ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¶„ì„ ë¡œì§ì´ ë“¤ì–´ê°\n    positive_words = [\"ì¢‹ë‹¤\", \"í›Œë¥­í•˜ë‹¤\", \"ìµœê³ \", \"ì¶”ì²œ\", \"ë§Œì¡±\", \"ê¸°ëŒ€\"]\n    negative_words = [\"ìµœì•…\", \"ì‹¤ë§\", \"í™”ë‚˜ë‹¤\", \"ì§œì¦\", \"ë¬¸ì œ\", \"ë¶ˆë§Œ\"]\n    \n    sentiment = \"ì¤‘ë¦½\"\n    confidence = 0.5\n    keywords = []\n    \n    comment_lower = comment.lower()\n    \n    pos_count = sum(1 for word in positive_words if word in comment_lower)\n    neg_count = sum(1 for word in negative_words if word in comment_lower)\n    \n    if pos_count > neg_count:\n        sentiment = \"ê¸ì •\"\n        confidence = min(0.9, 0.6 + pos_count * 0.1)\n        keywords = [word for word in positive_words if word in comment_lower]\n    elif neg_count > pos_count:\n        sentiment = \"ë¶€ì •\"  \n        confidence = min(0.9, 0.6 + neg_count * 0.1)\n        keywords = [word for word in negative_words if word in comment_lower]\n    \n    return {\n        \"sentiment\": sentiment,\n        \"confidence\": confidence,\n        \"keywords\": keywords[:3],  # ìµœëŒ€ 3ê°œë§Œ\n        \"method\": \"keyword_based_analysis\"\n    }\n\ndef search_news(keyword: str, max_results: int = 5) -> Dict[str, Any]:\n    \"\"\"ë‰´ìŠ¤ ê²€ìƒ‰ Tool í•¨ìˆ˜ (ëª¨ì˜)\"\"\"\n    # ì‹¤ì œë¡œëŠ” ë‰´ìŠ¤ API í˜¸ì¶œ\n    mock_articles = [\n        {\"title\": f\"{keyword} ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ 1\", \"url\": \"https://news1.com\", \"summary\": \"ê¸ì •ì  ì „ë§\"},\n        {\"title\": f\"{keyword} ì‹œì¥ ë™í–¥ ë¶„ì„\", \"url\": \"https://news2.com\", \"summary\": \"ì¤‘ë¦½ì  ë¶„ì„\"},\n        {\"title\": f\"{keyword} ë…¼ë€ í™•ì‚°\", \"url\": \"https://news3.com\", \"summary\": \"ë¶€ì •ì  ì˜ê²¬\"},\n    ]\n    \n    return {\n        \"keyword\": keyword,\n        \"articles\": mock_articles[:max_results],\n        \"total_found\": len(mock_articles),\n        \"search_timestamp\": \"2024-12-18T10:00:00Z\"\n    }\n\ndef summarize_sentiment_trends(analysis_results: list) -> Dict[str, Any]:\n    \"\"\"ê°ì„± ë¶„ì„ ê²°ê³¼ ìš”ì•½ Tool í•¨ìˆ˜\"\"\"\n    if not analysis_results:\n        return {\"error\": \"ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤\"}\n    \n    sentiments = [result.get(\"sentiment\", \"ì¤‘ë¦½\") for result in analysis_results]\n    confidences = [result.get(\"confidence\", 0.5) for result in analysis_results]\n    \n    sentiment_counts = {}\n    for sentiment in sentiments:\n        sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1\n    \n    total = len(sentiments)\n    sentiment_percentages = {k: (v/total)*100 for k, v in sentiment_counts.items()}\n    \n    avg_confidence = sum(confidences) / len(confidences)\n    \n    # ì „ì²´ì ì¸ ê²½í–¥ íŒë‹¨\n    if sentiment_percentages.get(\"ê¸ì •\", 0) > 50:\n        overall_trend = \"ê¸ì •ì \"\n    elif sentiment_percentages.get(\"ë¶€ì •\", 0) > 50:\n        overall_trend = \"ë¶€ì •ì \"\n    else:\n        overall_trend = \"ì¤‘ë¦½ì \"\n    \n    return {\n        \"total_analyzed\": total,\n        \"sentiment_distribution\": sentiment_percentages,\n        \"average_confidence\": avg_confidence,\n        \"overall_trend\": overall_trend,\n        \"recommendation\": f\"ì „ë°˜ì ìœ¼ë¡œ {overall_trend} ë°˜ì‘ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.\"\n    }\n\n# Function Callingìš© Tool ìŠ¤í‚¤ë§ˆ ì •ì˜\nTOOLS = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"analyze_sentiment\",\n            \"description\": \"ì£¼ì–´ì§„ ëŒ“ê¸€ì´ë‚˜ í…ìŠ¤íŠ¸ì˜ ê°ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"comment\": {\n                        \"type\": \"string\",\n                        \"description\": \"ë¶„ì„í•  ëŒ“ê¸€ì´ë‚˜ í…ìŠ¤íŠ¸\"\n                    },\n                    \"language\": {\n                        \"type\": \"string\", \n                        \"enum\": [\"ko\", \"en\"],\n                        \"description\": \"í…ìŠ¤íŠ¸ ì–¸ì–´ (ê¸°ë³¸ê°’: ko)\",\n                        \"default\": \"ko\"\n                    }\n                },\n                \"required\": [\"comment\"]\n            }\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"search_news\",\n            \"description\": \"íŠ¹ì • í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"keyword\": {\n                        \"type\": \"string\",\n                        \"description\": \"ê²€ìƒ‰í•  í‚¤ì›Œë“œ\"\n                    },\n                    \"max_results\": {\n                        \"type\": \"integer\",\n                        \"minimum\": 1,\n                        \"maximum\": 10,\n                        \"description\": \"ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 5)\",\n                        \"default\": 5\n                    }\n                },\n                \"required\": [\"keyword\"]\n            }\n        }\n    },\n    {\n        \"type\": \"function\", \n        \"function\": {\n            \"name\": \"summarize_sentiment_trends\",\n            \"description\": \"ì—¬ëŸ¬ ê°ì„± ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì „ì²´ì ì¸ ê²½í–¥ì„ ìš”ì•½í•©ë‹ˆë‹¤\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"analysis_results\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"object\"\n                        },\n                        \"description\": \"ê°ì„± ë¶„ì„ ê²°ê³¼ë“¤ì˜ ë°°ì—´\"\n                    }\n                },\n                \"required\": [\"analysis_results\"]\n            }\n        }\n    }\n]\n\n# Tool í•¨ìˆ˜ ë§¤í•‘\nAVAILABLE_FUNCTIONS = {\n    \"analyze_sentiment\": analyze_sentiment,\n    \"search_news\": search_news, \n    \"summarize_sentiment_trends\": summarize_sentiment_trends\n}\n\ndef execute_function_call(function_name: str, arguments: str) -> Any:\n    \"\"\"Function Call ì‹¤í–‰\"\"\"\n    try:\n        # JSON íŒŒì‹±\n        args = json.loads(arguments)\n        \n        # í•¨ìˆ˜ ì‹¤í–‰\n        if function_name in AVAILABLE_FUNCTIONS:\n            function = AVAILABLE_FUNCTIONS[function_name]\n            result = function(**args)\n            print(f\"ğŸ”§ {function_name} ì‹¤í–‰ ì™„ë£Œ\")\n            return result\n        else:\n            return {\"error\": f\"Unknown function: {function_name}\"}\n    \n    except json.JSONDecodeError as e:\n        return {\"error\": f\"JSON parsing error: {e}\"}\n    except Exception as e:\n        return {\"error\": f\"Function execution error: {e}\"}\n\ndef chat_with_function_calling(client, user_message, max_iterations=3):\n    \"\"\"Function Callingì„ í™œìš©í•œ ëŒ€í™”\"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": \"\"\"ë‹¹ì‹ ì€ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì „ë¬¸ AIì…ë‹ˆë‹¤. \n        ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ ë‹µë³€í•˜ì„¸ìš”.\n        \n        ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:\n        1. analyze_sentiment: ëŒ“ê¸€/í…ìŠ¤íŠ¸ ê°ì„± ë¶„ì„\n        2. search_news: í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰\n        3. summarize_sentiment_trends: ê°ì„± ë¶„ì„ ê²°ê³¼ ì¢…í•©\n        \n        ë³µì¡í•œ ìš”ì²­ì˜ ê²½ìš° ì—¬ëŸ¬ ë„êµ¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\"\"},\n        {\"role\": \"user\", \"content\": user_message}\n    ]\n    \n    print(f\"ğŸ‘¤ ì‚¬ìš©ì: {user_message}\")\n    print(\"=\" * 60)\n    \n    for iteration in range(max_iterations):\n        print(f\"\\\\nğŸ”„ ë°˜ë³µ {iteration + 1}\")\n        \n        # OpenAI API í˜¸ì¶œ (Function Calling í¬í•¨)\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=messages,\n            tools=TOOLS,\n            tool_choice=\"auto\",  # ìë™ìœ¼ë¡œ ì ì ˆí•œ ë„êµ¬ ì„ íƒ\n            temperature=0.3\n        )\n        \n        response_message = response.choices[0].message\n        \n        # Tool í˜¸ì¶œì´ ìˆëŠ”ì§€ í™•ì¸\n        if response_message.tool_calls:\n            print(f\"ğŸ› ï¸  LLMì´ {len(response_message.tool_calls)}ê°œ ë„êµ¬ ì‚¬ìš© ê²°ì •\")\n            \n            # ë©”ì‹œì§€ ê¸°ë¡ì— ì¶”ê°€\n            messages.append(response_message)\n            \n            # ê° Tool í˜¸ì¶œ ì‹¤í–‰\n            for tool_call in response_message.tool_calls:\n                function_name = tool_call.function.name\n                function_args = tool_call.function.arguments\n                \n                print(f\"   ğŸ“ í˜¸ì¶œ: {function_name}({function_args})\")\n                \n                # í•¨ìˆ˜ ì‹¤í–‰\n                function_result = execute_function_call(function_name, function_args)\n                \n                # ê²°ê³¼ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€\n                messages.append({\n                    \"tool_call_id\": tool_call.id,\n                    \"role\": \"tool\",\n                    \"name\": function_name,\n                    \"content\": json.dumps(function_result, ensure_ascii=False)\n                })\n                \n                print(f\"   âœ… ê²°ê³¼: {function_result}\")\n        else:\n            # Tool í˜¸ì¶œì´ ì—†ìœ¼ë©´ ìµœì¢… ë‹µë³€\n            print(f\"ğŸ¤– ìµœì¢… ë‹µë³€:\")\n            print(response_message.content)\n            break\n    \n    return messages\n\ndef demonstrate_single_tool_call(client):\n    \"\"\"ë‹¨ì¼ Tool í˜¸ì¶œ ì˜ˆì œ\"\"\"\n    print(\"\\\\n1ï¸âƒ£ ë‹¨ì¼ Tool í˜¸ì¶œ ì˜ˆì œ\")\n    print(\"-\" * 40)\n    \n    user_query = \"\\'ì´ ì •ì±…ì€ ì •ë§ í›Œë¥­í•©ë‹ˆë‹¤!\\' ì´ ëŒ“ê¸€ì˜ ê°ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.\"\n    chat_with_function_calling(client, user_query)\n\ndef demonstrate_multi_tool_workflow(client):\n    \"\"\"ë‹¤ì¤‘ Tool ì›Œí¬í”Œë¡œìš° ì˜ˆì œ\"\"\"\n    print(\"\\\\n2ï¸âƒ£ ë‹¤ì¤‘ Tool ì›Œí¬í”Œë¡œìš° ì˜ˆì œ\")\n    print(\"-\" * 40)\n    \n    user_query = \"\"\"ì‚¼ì„±ì „ìì— ëŒ€í•œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³ , ë‹¤ìŒ ëŒ“ê¸€ë“¤ë„ ë¶„ì„í•´ì£¼ì„¸ìš”:\n    1. 'ì‚¼ì„±ì „ì ì£¼ê°€ê°€ ì˜¤ë¥´ë„¤ìš”! ì¢‹ì€ ì†Œì‹ì…ë‹ˆë‹¤.'\n    2. 'ë˜ ë‹¤ë¥¸ ë¬¸ì œê°€ í„°ì¡Œë‚˜ìš”? ì‹¤ë§ì´ì—ìš”.'\n    ê·¸ë¦¬ê³  ì „ì²´ì ì¸ ê°ì„± ë™í–¥ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.\"\"\"\n    \n    chat_with_function_calling(client, user_query)\n\ndef demonstrate_tool_schema_validation():\n    \"\"\"Tool ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹œì—°\"\"\"\n    print(\"\\\\n3ï¸âƒ£ Tool ìŠ¤í‚¤ë§ˆ ë° íŒŒë¼ë¯¸í„° ê²€ì¦\")\n    print(\"-\" * 40)\n    \n    print(\"ğŸ“‹ ì •ì˜ëœ Tools:\")\n    for i, tool in enumerate(TOOLS, 1):\n        func_info = tool[\"function\"]\n        print(f\"   {i}. {func_info['name']}\")\n        print(f\"      ì„¤ëª…: {func_info['description']}\")\n        print(f\"      í•„ìˆ˜ íŒŒë¼ë¯¸í„°: {func_info['parameters'].get('required', [])}\")\n    \n    print(\"\\\\nğŸ”§ íŒŒë¼ë¯¸í„° ê²€ì¦ ì˜ˆì œ:\")\n    \n    # ì˜¬ë°”ë¥¸ í˜¸ì¶œ\n    try:\n        result = analyze_sentiment(\"í…ŒìŠ¤íŠ¸ ëŒ“ê¸€ì…ë‹ˆë‹¤\")\n        print(f\"âœ… ì˜¬ë°”ë¥¸ í˜¸ì¶œ ì„±ê³µ: {result['sentiment']}\")\n    except Exception as e:\n        print(f\"âŒ ì˜¬ë°”ë¥¸ í˜¸ì¶œ ì‹¤íŒ¨: {e}\")\n    \n    # ì˜ëª»ëœ í˜¸ì¶œ (í•„ìˆ˜ íŒŒë¼ë¯¸í„° ëˆ„ë½)\n    try:\n        result = analyze_sentiment()  # comment íŒŒë¼ë¯¸í„° ëˆ„ë½\n        print(f\"ì´ ì¤„ì€ ì‹¤í–‰ë˜ë©´ ì•ˆë©ë‹ˆë‹¤: {result}\")\n    except Exception as e:\n        print(f\"âœ… ì˜ëª»ëœ í˜¸ì¶œ ì •ìƒ ê°ì§€: {type(e).__name__}\")\n\nif __name__ == \"__main__\":\n    print(\"ğŸš€ Function Calling íŒ¨í„´ ì‹¤ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!\")\n    print(\"=\" * 70)\n    \n    try:\n        # 1. OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n        client = setup_openai_client()\n        print(\"âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n        \n        # 2. Tool ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹œì—°\n        demonstrate_tool_schema_validation()\n        \n        # 3. ë‹¨ì¼ Tool í˜¸ì¶œ ì˜ˆì œ\n        demonstrate_single_tool_call(client)\n        \n        # 4. ë‹¤ì¤‘ Tool ì›Œí¬í”Œë¡œìš° ì˜ˆì œ  \n        demonstrate_multi_tool_workflow(client)\n        \n        print(\"\\\\nâœ… Function Calling ì‹¤ìŠµ ì™„ë£Œ!\")\n        print(\"\\\\nğŸ’¡ í•µì‹¬ ê°œë…:\")\n        print(\"   1. LLMì´ ìƒí™©ì— ë§ëŠ” Toolì„ ìë™ ì„ íƒ\")\n        print(\"   2. JSON ìŠ¤í‚¤ë§ˆë¡œ íŒŒë¼ë¯¸í„° ê²€ì¦\")\n        print(\"   3. ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ì˜ ìë™ ì²´ì´ë‹\")\n        print(\"   4. tool_choiceë¡œ í˜¸ì¶œ ë°©ì‹ ì œì–´\")\n        print(\"\\\\nğŸ“š ë‹¤ìŒ ë‹¨ê³„:\")\n        print(\"   - 06_data_analyzer_tool.py: í”„ë¡œë•ì…˜ê¸‰ ê°ì„± ë¶„ì„ Tool\")\n        print(\"   - 07_langgraph_sequential.py: Multi-Agent ì›Œí¬í”Œë¡œìš°\")\n    \n    except Exception as e:\n        print(f\"âŒ ì‹¤ìŠµ ì˜¤ë¥˜: {e}\")\n        print(\"\\\\nğŸ”§ í•´ê²° ë°©ë²•:\")\n        print(\"   1. OpenAI API í‚¤ í™•ì¸\")\n        print(\"   2. Function Calling ì§€ì› ëª¨ë¸ ì‚¬ìš© (gpt-4, gpt-3.5-turbo)\")\n        print(\"   3. API í¬ë ˆë”§ ì”ì•¡ í™•ì¸\")\n'''\n\n# 05_function_calling.py ì €ì¥\nwith open(f\"{output_dir}/05_function_calling.py\", \"w\", encoding=\"utf-8\") as f:\n    f.write(function_calling_code)\n\nprint(\"âœ… 05_function_calling.py ìƒì„± ì™„ë£Œ\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… 05_function_calling.py ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {
        "file_changes": [
          {
            "path": "/home/user/output/session3_lab_codes/06_data_analyzer_tool.py",
            "change_type": "modified",
            "version": 1,
            "tool_call_id": "tooluse_BYXCI-d3T623PvJAhIrVgQ",
            "execution_count": 7,
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/620cfa78-4379-4aea-9de0-8d1d27726914/output/session3_lab_codes/06_data_analyzer_tool.py",
              "mime_type": "text/x-python",
              "size_bytes": 11829
            },
            "upload_status": "queued",
            "upload_completed": false,
            "timestamp": "2025-12-18T07:11:00.567425Z"
          }
        ]
      },
      "source": [
        "# ë‚˜ë¨¸ì§€ íŒŒì¼ë“¤ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìƒì„± - 06ë¶€í„° 09ê¹Œì§€\n\n# 06_data_analyzer_tool.py - í˜ì´ì§€ 12: ì™„ì „í•œ DataAnalyzer Tool\ndata_analyzer_code = '''\"\"\"\n3íšŒì°¨ ì‹¤ìŠµ 06: DataAnalyzer Tool ì™„ì „ êµ¬í˜„\ní˜ì´ì§€ 12 - í”„ë¡œë•ì…˜ê¸‰ ê°ì„± ë¶„ì„ Tool\n\nì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ DataAnalyzer Toolì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n- ì¬ì‹œë„(Retry) ë¡œì§\n- ë°°ì¹˜ ë¶„ì„ (Batch API)\n- ìºì‹± ë° ì¤‘ë³µ ë°©ì§€\n- ì—ëŸ¬ í•¸ë“¤ë§ ë° ë¡œê¹…\n\"\"\"\n\nimport os\nimport logging\nimport hashlib\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\nfrom dotenv import load_dotenv\n\n# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\nload_dotenv()\n\n# ë¡œê¹… ì„¤ì •\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass SentimentResult(BaseModel):\n    \"\"\"ê°ì„± ë¶„ì„ ê²°ê³¼ ëª¨ë¸\"\"\"\n    sentiment: str = Field(description=\"ê°ì„± ë¶„ë¥˜\")\n    confidence: float = Field(ge=0.0, le=1.0, description=\"ì‹ ë¢°ë„\")\n    reason: str = Field(description=\"ë¶„ì„ ê·¼ê±°\")\n    keywords: List[str] = Field(description=\"í•µì‹¬ í‚¤ì›Œë“œ\")\n    processing_time: float = Field(description=\"ì²˜ë¦¬ ì‹œê°„(ì´ˆ)\")\n    timestamp: str = Field(description=\"ë¶„ì„ ì‹œê°\")\n\n@dataclass\nclass CacheEntry:\n    \"\"\"ìºì‹œ ì—”íŠ¸ë¦¬\"\"\"\n    result: SentimentResult\n    created_at: datetime\n    ttl_hours: int = 24\n\nclass DataAnalyzer:\n    \"\"\"í”„ë¡œë•ì…˜ê¸‰ ê°ì„± ë¶„ì„ Tool\"\"\"\n    \n    def __init__(self, api_key: str, enable_cache: bool = True, cache_ttl_hours: int = 24):\n        self.client = OpenAI(api_key=api_key)\n        self.enable_cache = enable_cache\n        self.cache_ttl_hours = cache_ttl_hours\n        self.cache: Dict[str, CacheEntry] = {}\n        \n        # í†µê³„ ì¶”ì \n        self.stats = {\n            \"total_requests\": 0,\n            \"cache_hits\": 0,\n            \"cache_misses\": 0,\n            \"api_errors\": 0,\n            \"retries\": 0\n        }\n        \n        logger.info(\"DataAnalyzer ì´ˆê¸°í™” ì™„ë£Œ\")\n    \n    def _generate_cache_key(self, comment: str, model: str = \"gpt-4\") -> str:\n        \"\"\"ìºì‹œ í‚¤ ìƒì„±\"\"\"\n        content = f\"{comment}:{model}\".encode('utf-8')\n        return hashlib.md5(content).hexdigest()\n    \n    def _is_cache_valid(self, entry: CacheEntry) -> bool:\n        \"\"\"ìºì‹œ ìœ íš¨ì„± ê²€ì¦\"\"\"\n        age = datetime.now() - entry.created_at\n        return age < timedelta(hours=entry.ttl_hours)\n    \n    def _get_from_cache(self, cache_key: str) -> Optional[SentimentResult]:\n        \"\"\"ìºì‹œì—ì„œ ê²°ê³¼ ì¡°íšŒ\"\"\"\n        if not self.enable_cache or cache_key not in self.cache:\n            self.stats[\"cache_misses\"] += 1\n            return None\n        \n        entry = self.cache[cache_key]\n        if self._is_cache_valid(entry):\n            self.stats[\"cache_hits\"] += 1\n            logger.debug(f\"ìºì‹œ íˆíŠ¸: {cache_key[:8]}...\")\n            return entry.result\n        else:\n            # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ\n            del self.cache[cache_key]\n            self.stats[\"cache_misses\"] += 1\n            return None\n    \n    def _save_to_cache(self, cache_key: str, result: SentimentResult):\n        \"\"\"ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥\"\"\"\n        if self.enable_cache:\n            entry = CacheEntry(\n                result=result,\n                created_at=datetime.now(),\n                ttl_hours=self.cache_ttl_hours\n            )\n            self.cache[cache_key] = entry\n            logger.debug(f\"ìºì‹œ ì €ì¥: {cache_key[:8]}...\")\n    \n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=1, max=10),\n        retry=retry_if_exception_type((Exception,))\n    )\n    def _call_openai_api(self, comment: str, model: str = \"gpt-4\") -> Dict[str, Any]:\n        \"\"\"OpenAI API í˜¸ì¶œ (ì¬ì‹œë„ í¬í•¨)\"\"\"\n        self.stats[\"retries\"] += 1\n        \n        system_prompt = \"\"\"ë‹¹ì‹ ì€ ì „ë¬¸ ë‰´ìŠ¤ ëŒ“ê¸€ ê°ì„± ë¶„ì„ê°€ì…ë‹ˆë‹¤.\n        ì£¼ì–´ì§„ ëŒ“ê¸€ì„ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.\n        \n        ë¶„ë¥˜ ê¸°ì¤€:\n        - ê¸ì •: ì§€ì§€, ì¹­ì°¬, ê¸°ëŒ€ê°, ë§Œì¡±\n        - ë¶€ì •: ë¹„íŒ, ë¶„ë…¸, ì‹¤ë§, ìš°ë ¤\n        - ì¤‘ë¦½: ì‚¬ì‹¤ ì „ë‹¬, ì§ˆë¬¸, ê· í˜• ì˜ê²¬\n        \n        ì‘ë‹µ í˜•ì‹:\n        {\"sentiment\": \"ê¸ì •|ë¶€ì •|ì¤‘ë¦½\", \"confidence\": 0.0-1.0, \"reason\": \"ê·¼ê±°\", \"keywords\": [\"í‚¤ì›Œë“œ\"]}\"\"\"\n        \n        try:\n            start_time = datetime.now()\n            \n            response = self.client.chat.completions.create(\n                model=model,\n                messages=[\n                    {\"role\": \"system\", \"content\": system_prompt},\n                    {\"role\": \"user\", \"content\": f\"ëŒ“ê¸€: {comment}\"}\n                ],\n                temperature=0.3,\n                max_tokens=300\n            )\n            \n            processing_time = (datetime.now() - start_time).total_seconds()\n            \n            # JSON íŒŒì‹±\n            content = response.choices[0].message.content\n            if '{' in content and '}' in content:\n                import json\n                start_idx = content.find('{')\n                end_idx = content.rfind('}') + 1\n                json_str = content[start_idx:end_idx]\n                result = json.loads(json_str)\n                \n                # ê²°ê³¼ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€\n                result[\"processing_time\"] = processing_time\n                result[\"timestamp\"] = datetime.now().isoformat()\n                \n                return result\n            else:\n                raise ValueError(\"JSON í˜•ì‹ì˜ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤\")\n        \n        except Exception as e:\n            self.stats[\"api_errors\"] += 1\n            logger.error(f\"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}\")\n            raise\n    \n    def analyze_sentiment(self, comment: str, model: str = \"gpt-4\") -> SentimentResult:\n        \"\"\"ë‹¨ì¼ ëŒ“ê¸€ ê°ì„± ë¶„ì„\"\"\"\n        self.stats[\"total_requests\"] += 1\n        \n        # ë¹ˆ ëŒ“ê¸€ ì²´í¬\n        if not comment or not comment.strip():\n            return SentimentResult(\n                sentiment=\"ì¤‘ë¦½\",\n                confidence=0.0,\n                reason=\"ë¹ˆ ëŒ“ê¸€\",\n                keywords=[],\n                processing_time=0.0,\n                timestamp=datetime.now().isoformat()\n            )\n        \n        # ìºì‹œ í™•ì¸\n        cache_key = self._generate_cache_key(comment, model)\n        cached_result = self._get_from_cache(cache_key)\n        \n        if cached_result:\n            return cached_result\n        \n        try:\n            # API í˜¸ì¶œ\n            raw_result = self._call_openai_api(comment, model)\n            \n            # Pydantic ëª¨ë¸ë¡œ ê²€ì¦\n            result = SentimentResult(**raw_result)\n            \n            # ìºì‹œì— ì €ì¥\n            self._save_to_cache(cache_key, result)\n            \n            logger.info(f\"ê°ì„± ë¶„ì„ ì™„ë£Œ: {result.sentiment} ({result.confidence:.2f})\")\n            return result\n        \n        except Exception as e:\n            logger.error(f\"ê°ì„± ë¶„ì„ ì‹¤íŒ¨: {e}\")\n            \n            # í´ë°± ê²°ê³¼ ë°˜í™˜\n            return SentimentResult(\n                sentiment=\"ì¤‘ë¦½\",\n                confidence=0.0,\n                reason=f\"ë¶„ì„ ì‹¤íŒ¨: {str(e)}\",\n                keywords=[],\n                processing_time=0.0,\n                timestamp=datetime.now().isoformat()\n            )\n    \n    def batch_analyze(self, comments: List[str], model: str = \"gpt-4\", \n                     batch_size: int = 10) -> List[SentimentResult]:\n        \"\"\"ë°°ì¹˜ ê°ì„± ë¶„ì„\"\"\"\n        logger.info(f\"ë°°ì¹˜ ë¶„ì„ ì‹œì‘: {len(comments)}ê°œ ëŒ“ê¸€\")\n        \n        results = []\n        \n        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬\n        for i in range(0, len(comments), batch_size):\n            batch = comments[i:i + batch_size]\n            logger.info(f\"ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ)\")\n            \n            batch_results = []\n            for comment in batch:\n                result = self.analyze_sentiment(comment, model)\n                batch_results.append(result)\n            \n            results.extend(batch_results)\n        \n        logger.info(f\"ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼\")\n        return results\n    \n    def get_statistics(self) -> Dict[str, Any]:\n        \"\"\"í†µê³„ ì •ë³´ ì¡°íšŒ\"\"\"\n        cache_hit_rate = 0.0\n        if self.stats[\"total_requests\"] > 0:\n            cache_hit_rate = self.stats[\"cache_hits\"] / (self.stats[\"cache_hits\"] + self.stats[\"cache_misses\"])\n        \n        return {\n            \"total_requests\": self.stats[\"total_requests\"],\n            \"cache_hits\": self.stats[\"cache_hits\"],\n            \"cache_misses\": self.stats[\"cache_misses\"],\n            \"cache_hit_rate\": cache_hit_rate,\n            \"api_errors\": self.stats[\"api_errors\"],\n            \"retries\": self.stats[\"retries\"],\n            \"cache_size\": len(self.cache)\n        }\n    \n    def clear_cache(self):\n        \"\"\"ìºì‹œ í´ë¦¬ì–´\"\"\"\n        self.cache.clear()\n        logger.info(\"ìºì‹œê°€ í´ë¦¬ì–´ë˜ì—ˆìŠµë‹ˆë‹¤\")\n\nif __name__ == \"__main__\":\n    print(\"ğŸš€ DataAnalyzer Tool ì™„ì „ êµ¬í˜„ ì‹¤ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!\")\n    print(\"=\" * 70)\n    \n    try:\n        # 1. DataAnalyzer ì´ˆê¸°í™”\n        api_key = os.getenv(\"OPENAI_API_KEY\")\n        if not api_key:\n            raise ValueError(\"OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”\")\n        \n        analyzer = DataAnalyzer(api_key=api_key, enable_cache=True)\n        \n        # 2. ë‹¨ì¼ ë¶„ì„ í…ŒìŠ¤íŠ¸\n        print(\"\\\\n1ï¸âƒ£ ë‹¨ì¼ ëŒ“ê¸€ ë¶„ì„\")\n        print(\"-\" * 40)\n        \n        test_comment = \"ì´ ìƒˆë¡œìš´ ì •ì±…ì€ ì •ë§ í›Œë¥­í•©ë‹ˆë‹¤! ì ê·¹ ì§€ì§€í•©ë‹ˆë‹¤.\"\n        result = analyzer.analyze_sentiment(test_comment)\n        \n        print(f\"ğŸ“ ëŒ“ê¸€: {test_comment}\")\n        print(f\"ğŸ¯ ê²°ê³¼: {result.sentiment} (ì‹ ë¢°ë„: {result.confidence:.2f})\")\n        print(f\"ğŸ“Š ê·¼ê±°: {result.reason}\")\n        print(f\"ğŸ”‘ í‚¤ì›Œë“œ: {result.keywords}\")\n        print(f\"â±ï¸  ì²˜ë¦¬ì‹œê°„: {result.processing_time:.3f}ì´ˆ\")\n        \n        # 3. ìºì‹œ í…ŒìŠ¤íŠ¸ (ë™ì¼ ëŒ“ê¸€ ì¬ë¶„ì„)\n        print(\"\\\\n2ï¸âƒ£ ìºì‹œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸\")\n        print(\"-\" * 40)\n        \n        print(\"ë™ì¼ ëŒ“ê¸€ ì¬ë¶„ì„ (ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸°)...\")\n        cached_result = analyzer.analyze_sentiment(test_comment)\n        print(f\"ğŸ¯ ìºì‹œëœ ê²°ê³¼: {cached_result.sentiment}\")\n        \n        # 4. ë°°ì¹˜ ë¶„ì„ í…ŒìŠ¤íŠ¸\n        print(\"\\\\n3ï¸âƒ£ ë°°ì¹˜ ë¶„ì„ í…ŒìŠ¤íŠ¸\")\n        print(\"-\" * 40)\n        \n        test_comments = [\n            \"ì •ë§ ì¢‹ì€ ì•„ì´ë””ì–´ë„¤ìš”!\",\n            \"ì´ê±´ ì™„ì „ ìµœì•…ì´ì—ìš”.\",\n            \"ë‚´ì¼ ë‚ ì”¨ëŠ” ì–´ë–¨ê¹Œìš”?\",\n            \"ìƒˆë¡œìš´ ê¸°ìˆ ì´ ê¸°ëŒ€ë©ë‹ˆë‹¤.\",\n            \"ë¬¸ì œê°€ ë„ˆë¬´ ë§ì•„ìš”.\"\n        ]\n        \n        batch_results = analyzer.batch_analyze(test_comments, batch_size=2)\n        \n        for i, (comment, result) in enumerate(zip(test_comments, batch_results), 1):\n            print(f\"{i}. {comment[:20]}... â†’ {result.sentiment} ({result.confidence:.2f})\")\n        \n        # 5. í†µê³„ ì •ë³´ ì¶œë ¥\n        print(\"\\\\n4ï¸âƒ£ ì„±ëŠ¥ í†µê³„\")\n        print(\"-\" * 40)\n        \n        stats = analyzer.get_statistics()\n        for key, value in stats.items():\n            if key == \"cache_hit_rate\":\n                print(f\"{key}: {value:.1%}\")\n            else:\n                print(f\"{key}: {value}\")\n        \n        print(\"\\\\nâœ… DataAnalyzer Tool ì‹¤ìŠµ ì™„ë£Œ!\")\n        print(\"\\\\nğŸ’¡ í•µì‹¬ ê¸°ëŠ¥:\")\n        print(\"   1. ìë™ ì¬ì‹œë„ (Exponential Backoff)\")\n        print(\"   2. ì¸í…”ë¦¬ì „íŠ¸ ìºì‹± (ì¤‘ë³µ ë°©ì§€)\")\n        print(\"   3. ë°°ì¹˜ ì²˜ë¦¬ (íš¨ìœ¨ì„± í–¥ìƒ)\")\n        print(\"   4. ì—ëŸ¬ ë³µêµ¬ (í´ë°± ê²°ê³¼)\")\n        print(\"   5. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (í†µê³„ ìˆ˜ì§‘)\")\n        print(\"\\\\nğŸ“š ë‹¤ìŒ ë‹¨ê³„:\")\n        print(\"   - 07_langgraph_sequential.py: Multi-Agent ì›Œí¬í”Œë¡œìš°\")\n        print(\"   - 08_langgraph_conditional.py: ì¡°ê±´ë¶€ ë¼ìš°íŒ…\")\n    \n    except Exception as e:\n        print(f\"âŒ ì‹¤ìŠµ ì˜¤ë¥˜: {e}\")\n        print(\"\\\\nğŸ”§ í•´ê²° ë°©ë²•:\")\n        print(\"   1. OpenAI API í‚¤ í™•ì¸\")\n        print(\"   2. pip install tenacity pydantic\")\n        print(\"   3. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸\")\n'''\n\n# 06_data_analyzer_tool.py ì €ì¥\nwith open(f\"{output_dir}/06_data_analyzer_tool.py\", \"w\", encoding=\"utf-8\") as f:\n    f.write(data_analyzer_code)\n\nprint(\"âœ… 06_data_analyzer_tool.py ìƒì„± ì™„ë£Œ\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… 06_data_analyzer_tool.py ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {
        "file_changes": [
          {
            "path": "/home/user/output/session3_lab_codes/07_langgraph_sequential.py",
            "change_type": "modified",
            "version": 1,
            "tool_call_id": "tooluse_DaULhYMpQ5qnhPuh2qjIww",
            "execution_count": 8,
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/d92fb52d-6848-43f1-ba50-1435e27c302b/output/session3_lab_codes/07_langgraph_sequential.py",
              "mime_type": "text/x-python",
              "size_bytes": 15286
            },
            "upload_status": "queued",
            "upload_completed": false,
            "timestamp": "2025-12-18T07:11:56.275876Z"
          }
        ]
      },
      "source": [
        "# 07_langgraph_sequential.py - í˜ì´ì§€ 20: LangGraph Sequential íŒ¨í„´\nlanggraph_sequential_code = '''\"\"\"\n3íšŒì°¨ ì‹¤ìŠµ 07: LangGraph Sequential ì›Œí¬í”Œë¡œìš°\ní˜ì´ì§€ 20 - Crawler â†’ Analyzer â†’ Reporter\n\nì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” LangGraphë¥¼ ì´ìš©í•œ ìˆœì°¨ì  Multi-Agent ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n- StateGraph ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬\n- Agent ê°„ ë°ì´í„° íë¦„\n- ìˆœì°¨ ì‹¤í–‰ íŒ¨í„´\n- ì—ëŸ¬ í•¸ë“¤ë§ ë° ë¡œê¹…\n\"\"\"\n\nimport os\nimport json\nfrom typing import TypedDict, List, Dict, Any, Optional\nfrom datetime import datetime\n\nfrom langgraph.graph import StateGraph, END\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage\nfrom dotenv import load_dotenv\n\n# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\nload_dotenv()\n\nclass AgentState(TypedDict):\n    \"\"\"Multi-Agent ê³µìœ  ìƒíƒœ\"\"\"\n    # ì…ë ¥ ë°ì´í„°\n    keyword: str\n    max_articles: int\n    \n    # Crawler Agent ê²°ê³¼\n    articles: List[Dict[str, Any]]\n    crawler_status: str\n    crawler_timestamp: str\n    \n    # Analyzer Agent ê²°ê³¼  \n    analysis_results: List[Dict[str, Any]]\n    analyzer_status: str\n    analyzer_timestamp: str\n    \n    # Reporter Agent ê²°ê³¼\n    final_report: str\n    summary_stats: Dict[str, Any]\n    reporter_status: str\n    reporter_timestamp: str\n    \n    # ë©”íƒ€ë°ì´í„°\n    workflow_id: str\n    total_processing_time: float\n    errors: List[str]\n\ndef setup_llm():\n    \"\"\"LLM ì´ˆê¸°í™”\"\"\"\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"OPENAI_API_KEYê°€ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n    \n    return ChatOpenAI(\n        model=\"gpt-4\",\n        temperature=0.3,\n        api_key=api_key\n    )\n\ndef crawler_agent(state: AgentState) -> AgentState:\n    \"\"\"ë‰´ìŠ¤ í¬ë¡¤ë§ Agent (ëª¨ì˜)\"\"\"\n    print(f\"ğŸ•·ï¸ Crawler Agent ì‹¤í–‰: '{state['keyword']}' ê²€ìƒ‰\")\n    \n    try:\n        start_time = datetime.now()\n        \n        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì—¬ê¸°ì„œ Selenium, Playwright, Firecrawl ì‚¬ìš©\n        mock_articles = [\n            {\n                \"title\": f\"{state['keyword']} ê´€ë ¨ ìµœì‹  ë™í–¥\",\n                \"url\": \"https://news1.example.com/article1\",\n                \"summary\": \"ê¸ì •ì ì¸ ì „ë§ì„ ì œì‹œí•˜ëŠ” ê¸°ì‚¬ì…ë‹ˆë‹¤.\",\n                \"comments\": [\n                    \"ì •ë§ ì¢‹ì€ ì†Œì‹ì´ë„¤ìš”!\",\n                    \"ë“œë””ì–´ ê°œì„ ë˜ëŠ”êµ°ìš”.\", \n                    \"ê¸°ëŒ€í•˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤.\"\n                ],\n                \"crawl_timestamp\": datetime.now().isoformat()\n            },\n            {\n                \"title\": f\"{state['keyword']} ë…¼ë€ í™•ì‚°\",\n                \"url\": \"https://news2.example.com/article2\", \n                \"summary\": \"ì¼ë¶€ ë¶€ì •ì  ì˜ê²¬ì´ ì œê¸°ë˜ê³  ìˆìŠµë‹ˆë‹¤.\",\n                \"comments\": [\n                    \"ì´ê±´ ë¬¸ì œê°€ ìˆì–´ ë³´ì—¬ìš”.\",\n                    \"ì™œ ì´ëŸ° ê²°ì •ì„ í–ˆì„ê¹Œìš”?\",\n                    \"ì‹¤ë§ìŠ¤ëŸ½ë„¤ìš”.\"\n                ],\n                \"crawl_timestamp\": datetime.now().isoformat()\n            },\n            {\n                \"title\": f\"{state['keyword']} ì¤‘ë¦½ì  ë¶„ì„ ë¦¬í¬íŠ¸\",\n                \"url\": \"https://news3.example.com/article3\",\n                \"summary\": \"ê°ê´€ì ì¸ ë¶„ì„ì„ ì œê³µí•˜ëŠ” ê¸°ì‚¬ì…ë‹ˆë‹¤.\", \n                \"comments\": [\n                    \"ìì„¸í•œ ë¶„ì„ ê°ì‚¬í•©ë‹ˆë‹¤.\",\n                    \"ë” ì§€ì¼œë´ì•¼ í•  ê²ƒ ê°™ë„¤ìš”.\",\n                    \"ê· í˜•ì¡íŒ ì‹œê°ì´êµ°ìš”.\"\n                ],\n                \"crawl_timestamp\": datetime.now().isoformat()\n            }\n        ]\n        \n        # ìš”ì²­ëœ ìˆ˜ë§Œí¼ë§Œ ë°˜í™˜\n        articles = mock_articles[:state['max_articles']]\n        \n        processing_time = (datetime.now() - start_time).total_seconds()\n        \n        # ìƒíƒœ ì—…ë°ì´íŠ¸\n        state[\"articles\"] = articles\n        state[\"crawler_status\"] = \"completed\"\n        state[\"crawler_timestamp\"] = datetime.now().isoformat()\n        \n        print(f\"âœ… Crawler ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ({processing_time:.2f}ì´ˆ)\")\n        \n        return state\n        \n    except Exception as e:\n        print(f\"âŒ Crawler ì˜¤ë¥˜: {e}\")\n        state[\"crawler_status\"] = \"error\"\n        state[\"errors\"].append(f\"Crawler: {str(e)}\")\n        state[\"articles\"] = []\n        return state\n\ndef analyzer_agent(state: AgentState) -> AgentState:\n    \"\"\"ê°ì„± ë¶„ì„ Agent\"\"\"\n    print(\"ğŸ” Analyzer Agent ì‹¤í–‰: ëŒ“ê¸€ ê°ì„± ë¶„ì„\")\n    \n    try:\n        start_time = datetime.now()\n        llm = setup_llm()\n        \n        analysis_results = []\n        \n        for article in state[\"articles\"]:\n            article_analysis = {\n                \"article_title\": article[\"title\"],\n                \"article_url\": article[\"url\"],\n                \"comment_analyses\": []\n            }\n            \n            print(f\"  ğŸ“° ë¶„ì„ ì¤‘: {article['title'][:30]}...\")\n            \n            # ê° ëŒ“ê¸€ ë¶„ì„\n            for comment in article[\"comments\"]:\n                prompt = f\"\"\"ë‹¤ìŒ ëŒ“ê¸€ì˜ ê°ì„±ì„ ë¶„ì„í•˜ê³  JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:\n                \n                ëŒ“ê¸€: \"{comment}\"\n                \n                ì‘ë‹µ í˜•ì‹:\n                {{\n                    \"sentiment\": \"ê¸ì •|ë¶€ì •|ì¤‘ë¦½\",\n                    \"confidence\": 0.0-1.0,\n                    \"keywords\": [\"í‚¤ì›Œë“œ1\", \"í‚¤ì›Œë“œ2\"]\n                }}\"\"\"\n                \n                try:\n                    response = llm.invoke([HumanMessage(content=prompt)])\n                    content = response.content\n                    \n                    # JSON íŒŒì‹±\n                    if '{' in content and '}' in content:\n                        start_idx = content.find('{')\n                        end_idx = content.rfind('}') + 1\n                        json_str = content[start_idx:end_idx]\n                        sentiment_data = json.loads(json_str)\n                        \n                        comment_analysis = {\n                            \"comment\": comment,\n                            \"sentiment\": sentiment_data.get(\"sentiment\", \"ì¤‘ë¦½\"),\n                            \"confidence\": sentiment_data.get(\"confidence\", 0.5),\n                            \"keywords\": sentiment_data.get(\"keywords\", [])\n                        }\n                    else:\n                        # í´ë°±\n                        comment_analysis = {\n                            \"comment\": comment,\n                            \"sentiment\": \"ì¤‘ë¦½\",\n                            \"confidence\": 0.0,\n                            \"keywords\": []\n                        }\n                    \n                    article_analysis[\"comment_analyses\"].append(comment_analysis)\n                    \n                except Exception as e:\n                    print(f\"    âš ï¸ ëŒ“ê¸€ ë¶„ì„ ì‹¤íŒ¨: {e}\")\n                    # ì—ëŸ¬ ì‹œ ê¸°ë³¸ê°’\n                    article_analysis[\"comment_analyses\"].append({\n                        \"comment\": comment,\n                        \"sentiment\": \"ì¤‘ë¦½\", \n                        \"confidence\": 0.0,\n                        \"keywords\": [],\n                        \"error\": str(e)\n                    })\n            \n            analysis_results.append(article_analysis)\n        \n        processing_time = (datetime.now() - start_time).total_seconds()\n        \n        # ìƒíƒœ ì—…ë°ì´íŠ¸\n        state[\"analysis_results\"] = analysis_results\n        state[\"analyzer_status\"] = \"completed\"\n        state[\"analyzer_timestamp\"] = datetime.now().isoformat()\n        \n        total_comments = sum(len(article[\"comments\"]) for article in state[\"articles\"])\n        print(f\"âœ… Analyzer ì™„ë£Œ: {total_comments}ê°œ ëŒ“ê¸€ ë¶„ì„ ({processing_time:.2f}ì´ˆ)\")\n        \n        return state\n        \n    except Exception as e:\n        print(f\"âŒ Analyzer ì˜¤ë¥˜: {e}\")\n        state[\"analyzer_status\"] = \"error\"\n        state[\"errors\"].append(f\"Analyzer: {str(e)}\")\n        state[\"analysis_results\"] = []\n        return state\n\ndef reporter_agent(state: AgentState) -> AgentState:\n    \"\"\"ë¦¬í¬íŠ¸ ìƒì„± Agent\"\"\"\n    print(\"ğŸ“Š Reporter Agent ì‹¤í–‰: ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±\")\n    \n    try:\n        start_time = datetime.now()\n        \n        # í†µê³„ ê³„ì‚°\n        all_sentiments = []\n        all_confidences = []\n        \n        for article_analysis in state[\"analysis_results\"]:\n            for comment_analysis in article_analysis[\"comment_analyses\"]:\n                all_sentiments.append(comment_analysis[\"sentiment\"])\n                all_confidences.append(comment_analysis[\"confidence\"])\n        \n        # ê°ì„± ë¶„í¬ ê³„ì‚°\n        sentiment_counts = {}\n        for sentiment in all_sentiments:\n            sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1\n        \n        total_comments = len(all_sentiments)\n        sentiment_percentages = {}\n        if total_comments > 0:\n            for sentiment, count in sentiment_counts.items():\n                sentiment_percentages[sentiment] = (count / total_comments) * 100\n        \n        avg_confidence = sum(all_confidences) / len(all_confidences) if all_confidences else 0.0\n        \n        # ì „ì²´ ê²½í–¥ íŒë‹¨\n        positive_pct = sentiment_percentages.get(\"ê¸ì •\", 0)\n        negative_pct = sentiment_percentages.get(\"ë¶€ì •\", 0)\n        neutral_pct = sentiment_percentages.get(\"ì¤‘ë¦½\", 0)\n        \n        if positive_pct > negative_pct and positive_pct > neutral_pct:\n            overall_trend = \"ê¸ì •ì \"\n        elif negative_pct > positive_pct and negative_pct > neutral_pct:\n            overall_trend = \"ë¶€ì •ì \"\n        else:\n            overall_trend = \"ì¤‘ë¦½ì \"\n        \n        # ìš”ì•½ í†µê³„\n        summary_stats = {\n            \"total_articles\": len(state[\"articles\"]),\n            \"total_comments\": total_comments,\n            \"sentiment_distribution\": sentiment_percentages,\n            \"average_confidence\": avg_confidence,\n            \"overall_trend\": overall_trend\n        }\n        \n        # ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±\n        report = f\"\"\"\nğŸ¯ {state['keyword']} ê°ì„± ë¶„ì„ ë¦¬í¬íŠ¸\n{'=' * 50}\n\nğŸ“Š ë¶„ì„ ê°œìš”:\n- ë¶„ì„ ê¸°ì‚¬ ìˆ˜: {summary_stats['total_articles']}ê°œ\n- ë¶„ì„ ëŒ“ê¸€ ìˆ˜: {summary_stats['total_comments']}ê°œ\n- ì „ì²´ ê²½í–¥: {overall_trend}\n- í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.2f}\n\nğŸ“ˆ ê°ì„± ë¶„í¬:\n- ê¸ì •: {positive_pct:.1f}%\n- ë¶€ì •: {negative_pct:.1f}%  \n- ì¤‘ë¦½: {neutral_pct:.1f}%\n\nğŸ“ ìƒì„¸ ë¶„ì„:\n\"\"\"\n        \n        for i, article_analysis in enumerate(state[\"analysis_results\"], 1):\n            report += f\"\\\\n{i}. {article_analysis['article_title']}\\\\n\"\n            \n            article_sentiments = [ca[\"sentiment\"] for ca in article_analysis[\"comment_analyses\"]]\n            pos = article_sentiments.count(\"ê¸ì •\")\n            neg = article_sentiments.count(\"ë¶€ì •\") \n            neu = article_sentiments.count(\"ì¤‘ë¦½\")\n            \n            report += f\"   ëŒ“ê¸€ ë°˜ì‘: ê¸ì • {pos}ê°œ, ë¶€ì • {neg}ê°œ, ì¤‘ë¦½ {neu}ê°œ\\\\n\"\n        \n        report += f\"\\\\nâ±ï¸ ì²˜ë¦¬ ì‹œê°„: {datetime.now().isoformat()}\"\n        \n        processing_time = (datetime.now() - start_time).total_seconds()\n        \n        # ìƒíƒœ ì—…ë°ì´íŠ¸\n        state[\"final_report\"] = report\n        state[\"summary_stats\"] = summary_stats\n        state[\"reporter_status\"] = \"completed\"\n        state[\"reporter_timestamp\"] = datetime.now().isoformat()\n        \n        print(f\"âœ… Reporter ì™„ë£Œ: ë¦¬í¬íŠ¸ ìƒì„± ({processing_time:.2f}ì´ˆ)\")\n        \n        return state\n        \n    except Exception as e:\n        print(f\"âŒ Reporter ì˜¤ë¥˜: {e}\")\n        state[\"reporter_status\"] = \"error\"\n        state[\"errors\"].append(f\"Reporter: {str(e)}\")\n        state[\"final_report\"] = f\"ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}\"\n        state[\"summary_stats\"] = {}\n        return state\n\ndef create_workflow() -> StateGraph:\n    \"\"\"LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±\"\"\"\n    \n    # StateGraph ì´ˆê¸°í™”\n    workflow = StateGraph(AgentState)\n    \n    # Agent ë…¸ë“œ ì¶”ê°€\n    workflow.add_node(\"crawler\", crawler_agent)\n    workflow.add_node(\"analyzer\", analyzer_agent) \n    workflow.add_node(\"reporter\", reporter_agent)\n    \n    # ìˆœì°¨ì  íë¦„ ì •ì˜\n    workflow.add_edge(\"crawler\", \"analyzer\")\n    workflow.add_edge(\"analyzer\", \"reporter\")\n    workflow.add_edge(\"reporter\", END)\n    \n    # ì‹œì‘ì  ì„¤ì •\n    workflow.set_entry_point(\"crawler\")\n    \n    return workflow.compile()\n\nif __name__ == \"__main__\":\n    print(\"ğŸš€ LangGraph Sequential ì›Œí¬í”Œë¡œìš° ì‹¤ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!\")\n    print(\"=\" * 70)\n    \n    try:\n        # 1. ì›Œí¬í”Œë¡œìš° ìƒì„±\n        app = create_workflow()\n        print(\"âœ… LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„± ì™„ë£Œ\")\n        \n        # 2. ì´ˆê¸° ìƒíƒœ ì„¤ì •\n        initial_state: AgentState = {\n            \"keyword\": \"ì‚¼ì„±ì „ì\",\n            \"max_articles\": 3,\n            \"articles\": [],\n            \"crawler_status\": \"pending\",\n            \"crawler_timestamp\": \"\",\n            \"analysis_results\": [],\n            \"analyzer_status\": \"pending\", \n            \"analyzer_timestamp\": \"\",\n            \"final_report\": \"\",\n            \"summary_stats\": {},\n            \"reporter_status\": \"pending\",\n            \"reporter_timestamp\": \"\",\n            \"workflow_id\": f\"wf_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n            \"total_processing_time\": 0.0,\n            \"errors\": []\n        }\n        \n        print(f\"\\\\nğŸ¯ ë¶„ì„ í‚¤ì›Œë“œ: {initial_state['keyword']}\")\n        print(f\"ğŸ“Š ìµœëŒ€ ê¸°ì‚¬ ìˆ˜: {initial_state['max_articles']}ê°œ\")\n        print(f\"ğŸ†” ì›Œí¬í”Œë¡œìš° ID: {initial_state['workflow_id']}\")\n        \n        # 3. ì›Œí¬í”Œë¡œìš° ì‹¤í–‰\n        print(\"\\\\nğŸ”„ Multi-Agent ì›Œí¬í”Œë¡œìš° ì‹¤í–‰\")\n        print(\"-\" * 50)\n        \n        overall_start = datetime.now()\n        \n        # LangGraph ì‹¤í–‰\n        final_state = app.invoke(initial_state)\n        \n        overall_time = (datetime.now() - overall_start).total_seconds()\n        \n        print(\"\\\\n\" + \"=\" * 70)\n        print(\"ğŸ‰ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ!\")\n        print(\"=\" * 70)\n        \n        # 4. ê²°ê³¼ ì¶œë ¥\n        print(f\"\\\\nğŸ“‹ ì‹¤í–‰ ìƒíƒœ:\")\n        print(f\"   ğŸ•·ï¸ Crawler: {final_state['crawler_status']}\")\n        print(f\"   ğŸ” Analyzer: {final_state['analyzer_status']}\")  \n        print(f\"   ğŸ“Š Reporter: {final_state['reporter_status']}\")\n        print(f\"   â±ï¸ ì´ ì²˜ë¦¬ì‹œê°„: {overall_time:.2f}ì´ˆ\")\n        \n        if final_state[\"errors\"]:\n            print(f\"\\\\nâš ï¸ ì˜¤ë¥˜ ëª©ë¡:\")\n            for error in final_state[\"errors\"]:\n                print(f\"   - {error}\")\n        \n        # 5. ìµœì¢… ë¦¬í¬íŠ¸ ì¶œë ¥\n        if final_state[\"final_report\"]:\n            print(f\"\\\\n{final_state['final_report']}\")\n        \n        # 6. ìš”ì•½ í†µê³„\n        if final_state[\"summary_stats\"]:\n            stats = final_state[\"summary_stats\"]\n            print(f\"\\\\nğŸ“ˆ í•µì‹¬ í†µê³„:\")\n            print(f\"   ì „ì²´ ê²½í–¥: {stats.get('overall_trend', 'N/A')}\")\n            print(f\"   ì‹ ë¢°ë„: {stats.get('average_confidence', 0):.2f}\")\n            \n            dist = stats.get('sentiment_distribution', {})\n            for sentiment, pct in dist.items():\n                print(f\"   {sentiment}: {pct:.1f}%\")\n        \n        print(\"\\\\nâœ… LangGraph Sequential ì‹¤ìŠµ ì™„ë£Œ!\")\n        print(\"\\\\nğŸ’¡ í•µì‹¬ ê°œë…:\")\n        print(\"   1. StateGraph: ìƒíƒœ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°\")\n        print(\"   2. Sequential Flow: A â†’ B â†’ C ìˆœì°¨ ì‹¤í–‰\")\n        print(\"   3. State Sharing: Agent ê°„ ë°ì´í„° ê³µìœ \")\n        print(\"   4. Error Handling: ê°œë³„ Agent ì˜¤ë¥˜ ì²˜ë¦¬\")\n        print(\"\\\\nğŸ“š ë‹¤ìŒ ë‹¨ê³„:\")\n        print(\"   - 08_langgraph_conditional.py: ì¡°ê±´ë¶€ ë¼ìš°íŒ…\")\n        print(\"   - 09_langchain_memory.py: ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬\")\n    \n    except Exception as e:\n        print(f\"âŒ ì‹¤ìŠµ ì˜¤ë¥˜: {e}\")\n        print(\"\\\\nğŸ”§ í•´ê²° ë°©ë²•:\")\n        print(\"   1. OpenAI API í‚¤ í™•ì¸\")\n        print(\"   2. pip install langgraph langchain-openai\")\n        print(\"   3. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸\")\n'''\n\n# 07_langgraph_sequential.py ì €ì¥\nwith open(f\"{output_dir}/07_langgraph_sequential.py\", \"w\", encoding=\"utf-8\") as f:\n    f.write(langgraph_sequential_code)\n\nprint(\"âœ… 07_langgraph_sequential.py ìƒì„± ì™„ë£Œ\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… 07_langgraph_sequential.py ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Please continue."
    },
    {
      "cell_type": "code",
      "metadata": {
        "file_changes": [
          {
            "path": "/home/user/output/session3_lab_codes/08_langgraph_conditional.py",
            "change_type": "modified",
            "version": 1,
            "tool_call_id": "tooluse_bLpXdHfzRNaP1zE21Dy40w",
            "execution_count": 9,
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/c98911e3-e7e7-453d-83a0-f0a9d7a00746/output/session3_lab_codes/08_langgraph_conditional.py",
              "mime_type": "text/x-python",
              "size_bytes": 15377
            },
            "upload_status": "queued",
            "upload_completed": false,
            "timestamp": "2025-12-18T07:12:54.219595Z"
          }
        ]
      },
      "source": [
        "# 08_langgraph_conditional.pyì™€ 09_langchain_memory.py ìƒì„±\n\n# 08_langgraph_conditional.py - í˜ì´ì§€ 21: LangGraph Conditional Routing\nconditional_code = '''\"\"\"\n3íšŒì°¨ ì‹¤ìŠµ 08: LangGraph Conditional Routing\ní˜ì´ì§€ 21 - ëŒ“ê¸€ ìˆ˜ ê¸°ì¤€ ë¶„ê¸° ë¼ìš°íŒ…\n\nì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” LangGraphì˜ ì¡°ê±´ë¶€ ë¼ìš°íŒ…ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n- ì¡°ê±´ë¶€ ë¶„ê¸° (Conditional Edge)\n- ëŒ“ê¸€ ìˆ˜ì— ë”°ë¥¸ ë°°ì¹˜/ì‹¤ì‹œê°„ ë¶„ì„ ì„ íƒ\n- ë™ì  ì›Œí¬í”Œë¡œìš° ì œì–´\n- ì„±ëŠ¥ ìµœì í™” ì „ëµ\n\"\"\"\n\nimport os\nfrom typing import TypedDict, List, Dict, Any\nfrom datetime import datetime\n\nfrom langgraph.graph import StateGraph, END\nfrom langchain_openai import ChatOpenAI\nfrom dotenv import load_dotenv\n\n# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\nload_dotenv()\n\nclass ConditionalAgentState(TypedDict):\n    \"\"\"ì¡°ê±´ë¶€ ë¼ìš°íŒ…ìš© ìƒíƒœ\"\"\"\n    keyword: str\n    articles: List[Dict[str, Any]]\n    total_comments: int\n    processing_mode: str  # \"batch\" ë˜ëŠ” \"realtime\"\n    \n    # ë¶„ì„ ê²°ê³¼\n    analysis_results: List[Dict[str, Any]]\n    processing_stats: Dict[str, Any]\n    \n    # ë©”íƒ€ë°ì´í„°\n    workflow_path: List[str]  # ì‹¤í–‰ëœ ë…¸ë“œ ê²½ë¡œ\n    decision_reasons: List[str]  # ë¶„ê¸° ê²°ì • ì´ìœ \n    errors: List[str]\n\ndef setup_llm():\n    \"\"\"LLM ì´ˆê¸°í™”\"\"\"\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"OPENAI_API_KEYê°€ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n    \n    return ChatOpenAI(model=\"gpt-4\", temperature=0.3, api_key=api_key)\n\ndef data_validator(state: ConditionalAgentState) -> ConditionalAgentState:\n    \"\"\"ë°ì´í„° ê²€ì¦ Agent\"\"\"\n    print(\"ğŸ” Data Validator ì‹¤í–‰: ì…ë ¥ ë°ì´í„° ê²€ì¦\")\n    \n    state[\"workflow_path\"].append(\"validator\")\n    \n    # ëª¨ì˜ ê¸°ì‚¬ ë°ì´í„° ìƒì„± (ëŒ“ê¸€ ìˆ˜ê°€ ë‹¤ë¥¸ ê¸°ì‚¬ë“¤)\n    mock_articles = [\n        {\n            \"title\": f\"{state['keyword']} ëŒ€ê·œëª¨ ì—…ë°ì´íŠ¸\",\n            \"comments\": [f\"ëŒ“ê¸€ {i}\" for i in range(150)]  # 150ê°œ ëŒ“ê¸€\n        },\n        {\n            \"title\": f\"{state['keyword']} ì†Œì‹\", \n            \"comments\": [f\"ëŒ“ê¸€ {i}\" for i in range(5)]   # 5ê°œ ëŒ“ê¸€\n        },\n        {\n            \"title\": f\"{state['keyword']} ë¶„ì„\",\n            \"comments\": [f\"ëŒ“ê¸€ {i}\" for i in range(200)]  # 200ê°œ ëŒ“ê¸€\n        }\n    ]\n    \n    state[\"articles\"] = mock_articles\n    \n    # ì´ ëŒ“ê¸€ ìˆ˜ ê³„ì‚°\n    total_comments = sum(len(article[\"comments\"]) for article in state[\"articles\"])\n    state[\"total_comments\"] = total_comments\n    \n    print(f\"âœ… ë°ì´í„° ê²€ì¦ ì™„ë£Œ: {len(state['articles'])}ê°œ ê¸°ì‚¬, {total_comments}ê°œ ëŒ“ê¸€\")\n    \n    return state\n\ndef should_use_batch_processing(state: ConditionalAgentState) -> str:\n    \"\"\"ì¡°ê±´ë¶€ ë¼ìš°íŒ…: ë°°ì¹˜ ì²˜ë¦¬ ì—¬ë¶€ ê²°ì •\"\"\"\n    \n    threshold = 100  # ëŒ“ê¸€ 100ê°œ ê¸°ì¤€\n    total_comments = state[\"total_comments\"]\n    \n    if total_comments > threshold:\n        decision = \"batch_analyzer\"\n        reason = f\"ì´ {total_comments}ê°œ ëŒ“ê¸€ > {threshold}ê°œ ê¸°ì¤€, ë°°ì¹˜ ì²˜ë¦¬ ì„ íƒ\"\n        state[\"processing_mode\"] = \"batch\"\n    else:\n        decision = \"realtime_analyzer\"\n        reason = f\"ì´ {total_comments}ê°œ ëŒ“ê¸€ â‰¤ {threshold}ê°œ ê¸°ì¤€, ì‹¤ì‹œê°„ ì²˜ë¦¬ ì„ íƒ\"\n        state[\"processing_mode\"] = \"realtime\"\n    \n    state[\"decision_reasons\"].append(reason)\n    \n    print(f\"ğŸ”€ ë¼ìš°íŒ… ê²°ì •: {decision}\")\n    print(f\"ğŸ“‹ ê²°ì • ê·¼ê±°: {reason}\")\n    \n    return decision\n\ndef realtime_analyzer(state: ConditionalAgentState) -> ConditionalAgentState:\n    \"\"\"ì‹¤ì‹œê°„ ê°ì„± ë¶„ì„ Agent\"\"\"\n    print(\"âš¡ Realtime Analyzer ì‹¤í–‰: ìˆœì°¨ ì²˜ë¦¬\")\n    \n    state[\"workflow_path\"].append(\"realtime_analyzer\")\n    start_time = datetime.now()\n    \n    try:\n        llm = setup_llm()\n        analysis_results = []\n        \n        for article in state[\"articles\"]:\n            article_analysis = {\n                \"title\": article[\"title\"],\n                \"comment_count\": len(article[\"comments\"]),\n                \"sentiments\": [],\n                \"processing_method\": \"realtime\"\n            }\n            \n            print(f\"  ğŸ“° ì‹¤ì‹œê°„ ë¶„ì„: {article['title']} ({len(article['comments'])}ê°œ ëŒ“ê¸€)\")\n            \n            # ê° ëŒ“ê¸€ì„ ê°œë³„ì ìœ¼ë¡œ ì¦‰ì‹œ ì²˜ë¦¬\n            for i, comment in enumerate(article[\"comments\"]):\n                # ì‹¤ì‹œê°„ ì²˜ë¦¬ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ë¶„ì„\n                if any(word in comment.lower() for word in [\"ì¢‹\", \"í›Œë¥­\", \"ìµœê³ \"]):\n                    sentiment = \"ê¸ì •\"\n                elif any(word in comment.lower() for word in [\"ë‚˜ì˜\", \"ìµœì•…\", \"ì‹¤ë§\"]):\n                    sentiment = \"ë¶€ì •\"\n                else:\n                    sentiment = \"ì¤‘ë¦½\"\n                \n                article_analysis[\"sentiments\"].append({\n                    \"comment_index\": i,\n                    \"sentiment\": sentiment,\n                    \"processing_time\": 0.001  # ë¹ ë¥¸ ì²˜ë¦¬\n                })\n            \n            analysis_results.append(article_analysis)\n        \n        processing_time = (datetime.now() - start_time).total_seconds()\n        \n        state[\"analysis_results\"] = analysis_results\n        state[\"processing_stats\"] = {\n            \"method\": \"realtime\",\n            \"total_processing_time\": processing_time,\n            \"comments_per_second\": state[\"total_comments\"] / processing_time if processing_time > 0 else 0,\n            \"advantages\": [\"ì¦‰ì‹œ ê²°ê³¼ í™•ì¸\", \"ë©”ëª¨ë¦¬ íš¨ìœ¨ì \", \"ì¤‘ê°„ ê²°ê³¼ í™œìš© ê°€ëŠ¥\"]\n        }\n        \n        print(f\"âœ… ì‹¤ì‹œê°„ ë¶„ì„ ì™„ë£Œ: {state['total_comments']}ê°œ ëŒ“ê¸€, {processing_time:.2f}ì´ˆ\")\n        \n    except Exception as e:\n        print(f\"âŒ ì‹¤ì‹œê°„ ë¶„ì„ ì˜¤ë¥˜: {e}\")\n        state[\"errors\"].append(f\"Realtime Analyzer: {str(e)}\")\n    \n    return state\n\ndef batch_analyzer(state: ConditionalAgentState) -> ConditionalAgentState:\n    \"\"\"ë°°ì¹˜ ê°ì„± ë¶„ì„ Agent\"\"\"\n    print(\"ğŸ“¦ Batch Analyzer ì‹¤í–‰: ë°°ì¹˜ ì²˜ë¦¬\")\n    \n    state[\"workflow_path\"].append(\"batch_analyzer\")\n    start_time = datetime.now()\n    \n    try:\n        llm = setup_llm()\n        analysis_results = []\n        \n        # ëª¨ë“  ëŒ“ê¸€ì„ ëª¨ì•„ì„œ ë°°ì¹˜ë¡œ ì²˜ë¦¬\n        all_comments = []\n        comment_mapping = []  # ëŒ“ê¸€ê³¼ ê¸°ì‚¬ ë§¤í•‘ ì •ë³´\n        \n        for article_idx, article in enumerate(state[\"articles\"]):\n            for comment_idx, comment in enumerate(article[\"comments\"]):\n                all_comments.append(comment)\n                comment_mapping.append({\n                    \"article_idx\": article_idx,\n                    \"comment_idx\": comment_idx,\n                    \"article_title\": article[\"title\"]\n                })\n        \n        print(f\"  ğŸ“Š ë°°ì¹˜ ë¶„ì„ ì¤€ë¹„: ì´ {len(all_comments)}ê°œ ëŒ“ê¸€\")\n        \n        # ë°°ì¹˜ í¬ê¸°ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬ (ì‹¤ì œë¡œëŠ” LLM Batch API ì‚¬ìš©)\n        batch_size = 50\n        batch_results = []\n        \n        for i in range(0, len(all_comments), batch_size):\n            batch = all_comments[i:i + batch_size]\n            print(f\"    ë°°ì¹˜ {i//batch_size + 1}: {len(batch)}ê°œ ëŒ“ê¸€ ì²˜ë¦¬\")\n            \n            # ë°°ì¹˜ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ LLM í˜¸ì¶œ)\n            for comment in batch:\n                if any(word in comment.lower() for word in [\"ì¢‹\", \"í›Œë¥­\", \"ìµœê³ \"]):\n                    sentiment = \"ê¸ì •\"\n                elif any(word in comment.lower() for word in [\"ë‚˜ì˜\", \"ìµœì•…\", \"ì‹¤ë§\"]):\n                    sentiment = \"ë¶€ì •\" \n                else:\n                    sentiment = \"ì¤‘ë¦½\"\n                \n                batch_results.append({\n                    \"comment\": comment,\n                    \"sentiment\": sentiment,\n                    \"batch_processed\": True\n                })\n        \n        # ê²°ê³¼ë¥¼ ê¸°ì‚¬ë³„ë¡œ ì¬êµ¬ì„±\n        for article_idx, article in enumerate(state[\"articles\"]):\n            article_analysis = {\n                \"title\": article[\"title\"],\n                \"comment_count\": len(article[\"comments\"]),\n                \"sentiments\": [],\n                \"processing_method\": \"batch\"\n            }\n            \n            # í•´ë‹¹ ê¸°ì‚¬ì˜ ëŒ“ê¸€ ê²°ê³¼ë§Œ ì¶”ì¶œ\n            for mapping, result in zip(comment_mapping, batch_results):\n                if mapping[\"article_idx\"] == article_idx:\n                    article_analysis[\"sentiments\"].append({\n                        \"comment_index\": mapping[\"comment_idx\"],\n                        \"sentiment\": result[\"sentiment\"],\n                        \"batch_processed\": True\n                    })\n            \n            analysis_results.append(article_analysis)\n        \n        processing_time = (datetime.now() - start_time).total_seconds()\n        \n        state[\"analysis_results\"] = analysis_results\n        state[\"processing_stats\"] = {\n            \"method\": \"batch\",\n            \"total_processing_time\": processing_time,\n            \"comments_per_second\": state[\"total_comments\"] / processing_time if processing_time > 0 else 0,\n            \"batch_size\": batch_size,\n            \"total_batches\": len(range(0, len(all_comments), batch_size)),\n            \"advantages\": [\"ë†’ì€ ì²˜ë¦¬ëŸ‰\", \"ë¹„ìš© íš¨ìœ¨ì \", \"ì¼ê´€ëœ í’ˆì§ˆ\"]\n        }\n        \n        print(f\"âœ… ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: {state['total_comments']}ê°œ ëŒ“ê¸€, {processing_time:.2f}ì´ˆ\")\n        \n    except Exception as e:\n        print(f\"âŒ ë°°ì¹˜ ë¶„ì„ ì˜¤ë¥˜: {e}\")\n        state[\"errors\"].append(f\"Batch Analyzer: {str(e)}\")\n    \n    return state\n\ndef results_aggregator(state: ConditionalAgentState) -> ConditionalAgentState:\n    \"\"\"ê²°ê³¼ ì§‘ê³„ Agent\"\"\"\n    print(\"ğŸ“ˆ Results Aggregator ì‹¤í–‰: ê²°ê³¼ ì§‘ê³„ ë° ìš”ì•½\")\n    \n    state[\"workflow_path\"].append(\"aggregator\")\n    \n    try:\n        # ì „ì²´ ê°ì„± ë¶„í¬ ê³„ì‚°\n        all_sentiments = []\n        for article_analysis in state[\"analysis_results\"]:\n            for sentiment_data in article_analysis[\"sentiments\"]:\n                all_sentiments.append(sentiment_data[\"sentiment\"])\n        \n        sentiment_counts = {}\n        for sentiment in all_sentiments:\n            sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1\n        \n        # ì²˜ë¦¬ ë°©ì‹ë³„ ì„±ëŠ¥ ë¹„êµ\n        processing_method = state[\"processing_stats\"][\"method\"]\n        processing_time = state[\"processing_stats\"][\"total_processing_time\"]\n        throughput = state[\"processing_stats\"][\"comments_per_second\"]\n        \n        summary_report = f\"\"\"\nğŸ¯ ì¡°ê±´ë¶€ ë¼ìš°íŒ… ë¶„ì„ ê²°ê³¼\n{'=' * 50}\n\nğŸ”€ ì›Œí¬í”Œë¡œìš° ê²½ë¡œ: {' â†’ '.join(state['workflow_path'])}\n\nğŸ“Š ì²˜ë¦¬ í†µê³„:\n- ì„ íƒëœ ë°©ì‹: {processing_method.upper()}\n- ì´ ëŒ“ê¸€ ìˆ˜: {state['total_comments']}ê°œ\n- ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ\n- ì²˜ë¦¬ëŸ‰: {throughput:.1f} ëŒ“ê¸€/ì´ˆ\n\nğŸ“‹ ë¶„ê¸° ê²°ì • ê³¼ì •:\n\"\"\"\n        \n        for reason in state[\"decision_reasons\"]:\n            summary_report += f\"- {reason}\\\\n\"\n        \n        summary_report += f\"\"\"\nğŸ“ˆ ê°ì„± ë¶„í¬:\n- ê¸ì •: {sentiment_counts.get('ê¸ì •', 0)}ê°œ ({sentiment_counts.get('ê¸ì •', 0)/len(all_sentiments)*100:.1f}%)\n- ë¶€ì •: {sentiment_counts.get('ë¶€ì •', 0)}ê°œ ({sentiment_counts.get('ë¶€ì •', 0)/len(all_sentiments)*100:.1f}%)\n- ì¤‘ë¦½: {sentiment_counts.get('ì¤‘ë¦½', 0)}ê°œ ({sentiment_counts.get('ì¤‘ë¦½', 0)/len(all_sentiments)*100:.1f}%)\n\nğŸš€ {processing_method.title()} ì²˜ë¦¬ì˜ ì¥ì :\n\"\"\"\n        \n        for advantage in state[\"processing_stats\"][\"advantages\"]:\n            summary_report += f\"- {advantage}\\\\n\"\n        \n        state[\"processing_stats\"][\"summary_report\"] = summary_report\n        state[\"processing_stats\"][\"sentiment_distribution\"] = sentiment_counts\n        \n        print(f\"âœ… ê²°ê³¼ ì§‘ê³„ ì™„ë£Œ\")\n        \n    except Exception as e:\n        print(f\"âŒ ê²°ê³¼ ì§‘ê³„ ì˜¤ë¥˜: {e}\")\n        state[\"errors\"].append(f\"Aggregator: {str(e)}\")\n    \n    return state\n\ndef create_conditional_workflow():\n    \"\"\"ì¡°ê±´ë¶€ ë¼ìš°íŒ… ì›Œí¬í”Œë¡œìš° ìƒì„±\"\"\"\n    \n    workflow = StateGraph(ConditionalAgentState)\n    \n    # ë…¸ë“œ ì¶”ê°€\n    workflow.add_node(\"validator\", data_validator)\n    workflow.add_node(\"realtime_analyzer\", realtime_analyzer)\n    workflow.add_node(\"batch_analyzer\", batch_analyzer)\n    workflow.add_node(\"aggregator\", results_aggregator)\n    \n    # ì‹œì‘ì  ì„¤ì •\n    workflow.set_entry_point(\"validator\")\n    \n    # ì¡°ê±´ë¶€ ë¶„ê¸° (í•µì‹¬!)\n    workflow.add_conditional_edges(\n        \"validator\",                    # ë¶„ê¸° ì‹œì‘ ë…¸ë“œ\n        should_use_batch_processing,    # ë¶„ê¸° ê²°ì • í•¨ìˆ˜\n        {\n            \"realtime_analyzer\": \"realtime_analyzer\",  # ì‹¤ì‹œê°„ ì²˜ë¦¬ ê²½ë¡œ\n            \"batch_analyzer\": \"batch_analyzer\"         # ë°°ì¹˜ ì²˜ë¦¬ ê²½ë¡œ\n        }\n    )\n    \n    # ë‘ ê²½ë¡œ ëª¨ë‘ ì§‘ê³„ê¸°ë¡œ ìˆ˜ë ´\n    workflow.add_edge(\"realtime_analyzer\", \"aggregator\")\n    workflow.add_edge(\"batch_analyzer\", \"aggregator\")\n    workflow.add_edge(\"aggregator\", END)\n    \n    return workflow.compile()\n\nif __name__ == \"__main__\":\n    print(\"ğŸš€ LangGraph Conditional Routing ì‹¤ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!\")\n    print(\"=\" * 70)\n    \n    try:\n        # 1. ì›Œí¬í”Œë¡œìš° ìƒì„±\n        app = create_conditional_workflow()\n        print(\"âœ… ì¡°ê±´ë¶€ ë¼ìš°íŒ… ì›Œí¬í”Œë¡œìš° ìƒì„± ì™„ë£Œ\")\n        \n        # 2. í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤\n        test_cases = [\n            {\"keyword\": \"AIê¸°ìˆ \", \"description\": \"ì†ŒëŸ‰ ëŒ“ê¸€ (ì‹¤ì‹œê°„ ì²˜ë¦¬ ì˜ˆìƒ)\"},\n            {\"keyword\": \"ê²½ì œì •ì±…\", \"description\": \"ëŒ€ëŸ‰ ëŒ“ê¸€ (ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆìƒ)\"}\n        ]\n        \n        for i, test_case in enumerate(test_cases, 1):\n            print(f\"\\\\n{'='*20} í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i} {'='*20}\")\n            print(f\"ğŸ¯ í‚¤ì›Œë“œ: {test_case['keyword']}\")\n            print(f\"ğŸ“ ì„¤ëª…: {test_case['description']}\")\n            \n            # ì´ˆê¸° ìƒíƒœ\n            initial_state: ConditionalAgentState = {\n                \"keyword\": test_case[\"keyword\"],\n                \"articles\": [],\n                \"total_comments\": 0,\n                \"processing_mode\": \"\",\n                \"analysis_results\": [],\n                \"processing_stats\": {},\n                \"workflow_path\": [],\n                \"decision_reasons\": [],\n                \"errors\": []\n            }\n            \n            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰\n            final_state = app.invoke(initial_state)\n            \n            # ê²°ê³¼ ì¶œë ¥\n            print(f\"\\\\nğŸ“Š ì‹¤í–‰ ê²°ê³¼:\")\n            print(f\"   ğŸ”€ ì›Œí¬í”Œë¡œìš° ê²½ë¡œ: {' â†’ '.join(final_state['workflow_path'])}\")\n            print(f\"   âš™ï¸ ì„ íƒëœ ì²˜ë¦¬ ë°©ì‹: {final_state['processing_mode']}\")\n            print(f\"   ğŸ’¬ ì´ ëŒ“ê¸€ ìˆ˜: {final_state['total_comments']}ê°œ\")\n            \n            if final_state[\"processing_stats\"]:\n                stats = final_state[\"processing_stats\"]\n                print(f\"   â±ï¸ ì²˜ë¦¬ ì‹œê°„: {stats.get('total_processing_time', 0):.2f}ì´ˆ\")\n                print(f\"   ğŸš€ ì²˜ë¦¬ëŸ‰: {stats.get('comments_per_second', 0):.1f} ëŒ“ê¸€/ì´ˆ\")\n            \n            # ìš”ì•½ ë¦¬í¬íŠ¸ ì¶œë ¥\n            if \"summary_report\" in final_state.get(\"processing_stats\", {}):\n                print(final_state[\"processing_stats\"][\"summary_report\"])\n        \n        print(\"\\\\nâœ… LangGraph Conditional Routing ì‹¤ìŠµ ì™„ë£Œ!\")\n        print(\"\\\\nğŸ’¡ í•µì‹¬ ê°œë…:\")\n        print(\"   1. Conditional Edge: ì¡°ê±´ì— ë”°ë¥¸ ë™ì  ë¼ìš°íŒ…\")\n        print(\"   2. Decision Function: ë¶„ê¸° ê²°ì • ë¡œì§\") \n        print(\"   3. Multi-Path Convergence: ì—¬ëŸ¬ ê²½ë¡œê°€ í•˜ë‚˜ë¡œ ìˆ˜ë ´\")\n        print(\"   4. Performance Optimization: ìƒí™©ë³„ ìµœì  ì²˜ë¦¬ ë°©ì‹\")\n        print(\"\\\\nğŸ“š ë‹¤ìŒ ë‹¨ê³„:\")\n        print(\"   - 09_langchain_memory.py: ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬\")\n        print(\"   - 10_integrated_demo.py: ì „ì²´ ê¸°ëŠ¥ í†µí•©\")\n    \n    except Exception as e:\n        print(f\"âŒ ì‹¤ìŠµ ì˜¤ë¥˜: {e}\")\n        print(\"\\\\nğŸ”§ í•´ê²° ë°©ë²•:\")\n        print(\"   1. OpenAI API í‚¤ í™•ì¸\")\n        print(\"   2. pip install langgraph langchain-openai\")\n        print(\"   3. ì¡°ê±´ í•¨ìˆ˜ ë°˜í™˜ê°’ í™•ì¸\")\n'''\n\n# 08_langgraph_conditional.py ì €ì¥\nwith open(f\"{output_dir}/08_langgraph_conditional.py\", \"w\", encoding=\"utf-8\") as f:\n    f.write(conditional_code)\n\nprint(\"âœ… 08_langgraph_conditional.py ìƒì„± ì™„ë£Œ\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… 08_langgraph_conditional.py ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "I'll continue creating the remaining lab code files and package them into a ZIP file as requested. Let me complete the LangChain memory implementation and integrated demo, then create the final package."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1. ğŸ”„ ğŸ”´ Create 09_langchain_memory.py (í˜ì´ì§€ 29 - ConversationBufferMemory) [in_progress] (ID: 1)\n2. â³ ğŸ”´ Create 10_integrated_demo.py (í†µí•© ì˜ˆì œ) [pending] (ID: 2)\n3. â³ ğŸŸ¡ Create requirements.txt with all dependencies [pending] (ID: 3)\n4. â³ ğŸŸ¡ Create .env.example with API key templates [pending] (ID: 4)\n5. â³ ğŸŸ¡ Create comprehensive README.md [pending] (ID: 5)\n6. â³ ğŸ”´ Package all files into ZIP archive [pending] (ID: 6)\n7. â³ ğŸ”´ Upload ZIP to AI Drive for distribution [pending] (ID: 7)"
    },
    {
      "cell_type": "code",
      "metadata": {
        "file_changes": [
          {
            "path": "/home/user/output/session3_lab_codes/09_langchain_memory.py",
            "change_type": "modified",
            "version": 1,
            "tool_call_id": "tooluse_4YS4D3PjT92VtobV2O8kow",
            "execution_count": 10,
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/b7242bd6-3076-46d5-bd12-7d2acee443be/output/session3_lab_codes/09_langchain_memory.py",
              "mime_type": "text/x-python",
              "size_bytes": 12766
            },
            "upload_status": "queued",
            "upload_completed": false,
            "timestamp": "2025-12-18T07:14:16.394460Z"
          }
        ]
      },
      "source": [
        "# Create 09_langchain_memory.py - LangChain ConversationBufferMemory implementation\nmemory_code = '''\"\"\"\nSession 3 - Lab 09: LangChain ConversationBufferMemory\ní˜ì´ì§€ 29: ëŒ€í™” ê¸°ì–µ ê´€ë¦¬ ë° ì»¨í…ìŠ¤íŠ¸ ìœ ì§€\n\nLangChainì˜ ConversationBufferMemoryë¥¼ ì‚¬ìš©í•˜ì—¬ \nëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ê´€ë¦¬í•˜ëŠ” AI ì—ì´ì „íŠ¸ êµ¬í˜„\n\ní•™ìŠµ ëª©í‘œ:\n- ConversationBufferMemory ê¸°ë³¸ ì‚¬ìš©ë²•\n- ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬\n- ë©”ëª¨ë¦¬ ì œí•œ ë° ìµœì í™”\n- ê°ì • ë¶„ì„ê³¼ ë©”ëª¨ë¦¬ í†µí•©\n\"\"\"\n\nimport os\nimport json\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime\nfrom langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\nfrom langchain.schema import BaseMessage, HumanMessage, AIMessage\nfrom langchain.llms.openai import OpenAI\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import ConversationChain\nfrom langchain.prompts import PromptTemplate\n\n\nclass MemoryManager:\n    \"\"\"ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤\"\"\"\n    \n    def __init__(self, memory_type: str = \"buffer\", max_token_limit: int = 2000):\n        \"\"\"\n        ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”\n        \n        Args:\n            memory_type: ë©”ëª¨ë¦¬ íƒ€ì… (\"buffer\" ë˜ëŠ” \"window\")\n            max_token_limit: ìµœëŒ€ í† í° ì œí•œ\n        \"\"\"\n        self.memory_type = memory_type\n        self.max_token_limit = max_token_limit\n        \n        # ë©”ëª¨ë¦¬ íƒ€ì…ì— ë”°ë¥¸ ë©”ëª¨ë¦¬ ê°ì²´ ìƒì„±\n        if memory_type == \"buffer\":\n            self.memory = ConversationBufferMemory(\n                memory_key=\"history\",\n                return_messages=True,\n                max_token_limit=max_token_limit\n            )\n        elif memory_type == \"window\":\n            self.memory = ConversationBufferWindowMemory(\n                memory_key=\"history\",\n                k=5,  # ìµœê·¼ 5ê°œ ëŒ€í™”ë§Œ ìœ ì§€\n                return_messages=True\n            )\n        else:\n            raise ValueError(\"memory_type must be 'buffer' or 'window'\")\n        \n        # ëŒ€í™” í†µê³„\n        self.conversation_count = 0\n        self.total_tokens_used = 0\n        \n        print(f\"âœ… {memory_type.upper()} ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ\")\n        print(f\"ğŸ“Š í† í° ì œí•œ: {max_token_limit}\")\n    \n    def add_conversation(self, user_input: str, ai_response: str) -> None:\n        \"\"\"ëŒ€í™”ë¥¼ ë©”ëª¨ë¦¬ì— ì¶”ê°€\"\"\"\n        try:\n            self.memory.chat_memory.add_user_message(user_input)\n            self.memory.chat_memory.add_ai_message(ai_response)\n            \n            self.conversation_count += 1\n            \n            # ëŒ€ëµì ì¸ í† í° ìˆ˜ ê³„ì‚° (1í† í° â‰ˆ 4ê¸€ì)\n            tokens_used = len(user_input + ai_response) // 4\n            self.total_tokens_used += tokens_used\n            \n            print(f\"ğŸ’¬ ëŒ€í™” ì¶”ê°€ë¨ (#{self.conversation_count})\")\n            print(f\"ğŸ“ ì‚¬ìš©ì: {user_input[:50]}...\")\n            print(f\"ğŸ¤– AI: {ai_response[:50]}...\")\n            print(f\"ğŸ¯ ì˜ˆìƒ í† í°: {tokens_used}\")\n            \n        except Exception as e:\n            print(f\"âŒ ëŒ€í™” ì¶”ê°€ ì‹¤íŒ¨: {e}\")\n    \n    def get_conversation_history(self) -> List[BaseMessage]:\n        \"\"\"ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜\"\"\"\n        return self.memory.chat_memory.messages\n    \n    def clear_memory(self) -> None:\n        \"\"\"ë©”ëª¨ë¦¬ ì´ˆê¸°í™”\"\"\"\n        self.memory.clear()\n        self.conversation_count = 0\n        self.total_tokens_used = 0\n        print(\"ğŸ—‘ï¸ ë©”ëª¨ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤\")\n    \n    def get_memory_stats(self) -> Dict[str, Any]:\n        \"\"\"ë©”ëª¨ë¦¬ ì‚¬ìš© í†µê³„ ë°˜í™˜\"\"\"\n        messages = self.get_conversation_history()\n        \n        return {\n            \"memory_type\": self.memory_type,\n            \"conversation_count\": self.conversation_count,\n            \"total_messages\": len(messages),\n            \"estimated_tokens\": self.total_tokens_used,\n            \"token_limit\": self.max_token_limit,\n            \"memory_usage_percent\": round((self.total_tokens_used / self.max_token_limit) * 100, 2)\n        }\n    \n    def export_conversations(self, filename: str) -> None:\n        \"\"\"ëŒ€í™” ë‚´ìš©ì„ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°\"\"\"\n        try:\n            messages = self.get_conversation_history()\n            conversations = []\n            \n            for i in range(0, len(messages), 2):\n                if i + 1 < len(messages):\n                    user_msg = messages[i]\n                    ai_msg = messages[i + 1]\n                    \n                    conversations.append({\n                        \"timestamp\": datetime.now().isoformat(),\n                        \"user\": user_msg.content,\n                        \"ai\": ai_msg.content\n                    })\n            \n            with open(filename, 'w', encoding='utf-8') as f:\n                json.dump(conversations, f, ensure_ascii=False, indent=2)\n            \n            print(f\"ğŸ’¾ ëŒ€í™” ë‚´ìš©ì´ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤\")\n            \n        except Exception as e:\n            print(f\"âŒ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}\")\n\n\nclass SentimentMemoryAgent:\n    \"\"\"ê°ì • ë¶„ì„ê³¼ ë©”ëª¨ë¦¬ë¥¼ í†µí•©í•œ AI ì—ì´ì „íŠ¸\"\"\"\n    \n    def __init__(self, openai_api_key: str, memory_type: str = \"buffer\"):\n        \"\"\"\n        ê°ì • ë¶„ì„ ë©”ëª¨ë¦¬ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”\n        \n        Args:\n            openai_api_key: OpenAI API í‚¤\n            memory_type: ë©”ëª¨ë¦¬ íƒ€ì…\n        \"\"\"\n        self.api_key = openai_api_key\n        self.memory_manager = MemoryManager(memory_type)\n        \n        # OpenAI ì±„íŒ… ëª¨ë¸ ì´ˆê¸°í™”\n        self.llm = ChatOpenAI(\n            openai_api_key=openai_api_key,\n            model_name=\"gpt-3.5-turbo\",\n            temperature=0.7\n        )\n        \n        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n        self.prompt = PromptTemplate(\n            input_variables=[\"history\", \"input\"],\n            template=\"\"\"ë‹¹ì‹ ì€ ê°ì •ì„ ì´í•´í•˜ê³  ê³µê°í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. \nì‚¬ìš©ìì˜ ê°ì •ì„ ë¶„ì„í•˜ê³ , ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì ì ˆí•œ ì‘ë‹µì„ ì œê³µí•˜ì„¸ìš”.\n\nì´ì „ ëŒ€í™”:\n{history}\n\ní˜„ì¬ ì…ë ¥: {input}\n\nì‘ë‹µ ì‹œ ë‹¤ìŒì„ í¬í•¨í•˜ì„¸ìš”:\n1. ê°ì • ë¶„ì„ ê²°ê³¼ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½)\n2. ì´ì „ ëŒ€í™”ì™€ì˜ ì—°ê´€ì„±\n3. ê³µê°ì ì´ê³  ë„ì›€ì´ ë˜ëŠ” ì‘ë‹µ\n\nì‘ë‹µ:\"\"\"\n        )\n        \n        # ëŒ€í™” ì²´ì¸ ìƒì„±\n        self.conversation = ConversationChain(\n            llm=self.llm,\n            memory=self.memory_manager.memory,\n            prompt=self.prompt,\n            verbose=True\n        )\n        \n        print(\"ğŸ¤– ê°ì • ë¶„ì„ ë©”ëª¨ë¦¬ ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n    \n    def analyze_and_respond(self, user_input: str) -> str:\n        \"\"\"ì‚¬ìš©ì ì…ë ¥ ë¶„ì„ ë° ì‘ë‹µ ìƒì„±\"\"\"\n        try:\n            print(f\"\\\\nğŸ” ë¶„ì„ ì¤‘: {user_input}\")\n            \n            # ëŒ€í™” ì²´ì¸ì„ í†µí•œ ì‘ë‹µ ìƒì„±\n            response = self.conversation.predict(input=user_input)\n            \n            print(f\"ğŸ’­ AI ì‘ë‹µ: {response}\")\n            \n            return response\n            \n        except Exception as e:\n            print(f\"âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}\")\n            return \"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\"\n    \n    def get_conversation_summary(self) -> str:\n        \"\"\"ëŒ€í™” ìš”ì•½ ìƒì„±\"\"\"\n        try:\n            history = self.memory_manager.get_conversation_history()\n            \n            if not history:\n                return \"ì•„ì§ ëŒ€í™” ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤.\"\n            \n            # ëŒ€í™” ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜\n            conversation_text = \"\\\\n\".join([\n                f\"{'ì‚¬ìš©ì' if isinstance(msg, HumanMessage) else 'AI'}: {msg.content}\"\n                for msg in history\n            ])\n            \n            # ìš”ì•½ í”„ë¡¬í”„íŠ¸\n            summary_prompt = f\"\"\"ë‹¤ìŒ ëŒ€í™”ë¥¼ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{conversation_text}\n\nìš”ì•½:\"\"\"\n            \n            summary_response = self.llm.predict(summary_prompt)\n            return summary_response\n            \n        except Exception as e:\n            print(f\"âŒ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}\")\n            return \"ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n\n\ndef demonstrate_basic_memory():\n    \"\"\"ê¸°ë³¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ë²• ì‹œì—°\"\"\"\n    print(\"\\\\n\" + \"=\"*60)\n    print(\"ğŸ§  ê¸°ë³¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ë²• ì‹œì—°\")\n    print(\"=\"*60)\n    \n    # Buffer Memory ì‹œì—°\n    buffer_memory = MemoryManager(\"buffer\", max_token_limit=1000)\n    \n    # ìƒ˜í”Œ ëŒ€í™” ì¶”ê°€\n    conversations = [\n        (\"ì•ˆë…•í•˜ì„¸ìš”!\", \"ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\"),\n        (\"ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì¢‹ì§€ ì•Šì•„ìš”\", \"ê·¸ë ‡êµ°ìš”. ë¬´ìŠ¨ ì¼ì´ ìˆì—ˆë‚˜ìš”?\"),\n        (\"íšŒì‚¬ì—ì„œ í”„ë ˆì  í…Œì´ì…˜ì´ ì˜ ì•ˆëì–´ìš”\", \"í˜ë“¤ì—ˆê² ë„¤ìš”. ë‹¤ìŒì—” ë” ì˜í•  ìˆ˜ ìˆì„ ê±°ì˜ˆìš”.\"),\n        (\"ì¡°ì–¸ ê³ ë§ˆì›Œìš”\", \"ì–¸ì œë“ ì§€ ë„ì›€ì´ í•„ìš”í•˜ë©´ ë§ì”€í•˜ì„¸ìš”!\")\n    ]\n    \n    for user, ai in conversations:\n        buffer_memory.add_conversation(user, ai)\n    \n    # ë©”ëª¨ë¦¬ í†µê³„ ì¶œë ¥\n    stats = buffer_memory.get_memory_stats()\n    print(f\"\\\\nğŸ“Š ë©”ëª¨ë¦¬ í†µê³„:\")\n    for key, value in stats.items():\n        print(f\"   {key}: {value}\")\n    \n    # Window Memory ì‹œì—°\n    print(f\"\\\\nğŸªŸ Window Memory ë¹„êµ (ìµœê·¼ 5ê°œë§Œ ìœ ì§€)\")\n    window_memory = MemoryManager(\"window\")\n    \n    # ë” ë§ì€ ëŒ€í™” ì¶”ê°€\n    for i in range(10):\n        window_memory.add_conversation(\n            f\"ì§ˆë¬¸ {i+1}: í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì…ë‹ˆë‹¤\",\n            f\"ë‹µë³€ {i+1}: í…ŒìŠ¤íŠ¸ ë‹µë³€ì…ë‹ˆë‹¤\"\n        )\n    \n    window_stats = window_memory.get_memory_stats()\n    print(f\"\\\\nğŸ“Š Window Memory í†µê³„:\")\n    for key, value in window_stats.items():\n        print(f\"   {key}: {value}\")\n\n\ndef demonstrate_sentiment_memory_agent():\n    \"\"\"ê°ì • ë¶„ì„ ë©”ëª¨ë¦¬ ì—ì´ì „íŠ¸ ì‹œì—°\"\"\"\n    print(\"\\\\n\" + \"=\"*60)\n    print(\"ğŸ’ ê°ì • ë¶„ì„ ë©”ëª¨ë¦¬ ì—ì´ì „íŠ¸ ì‹œì—°\")\n    print(\"=\"*60)\n    \n    # API í‚¤ í™•ì¸\n    api_key = os.getenv('OPENAI_API_KEY')\n    if not api_key:\n        print(\"âš ï¸ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n        print(\"ğŸ“ ë°ëª¨ìš© ëª¨ì˜ ì‘ë‹µì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n        \n        # ëª¨ì˜ ë©”ëª¨ë¦¬ ê´€ë¦¬ìë¡œ ì‹œì—°\n        memory_demo = MemoryManager(\"buffer\")\n        \n        demo_conversations = [\n            (\"ì˜¤ëŠ˜ ì •ë§ í˜ë“  í•˜ë£¨ì˜€ì–´ìš”\", \"[ê°ì •: ë¶€ì •] í˜ë“  í•˜ë£¨ë¥¼ ë³´ë‚´ì…¨êµ°ìš”. ë¬´ìŠ¨ ì¼ì´ ìˆì—ˆëŠ”ì§€ ë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.\"),\n            (\"í”„ë¡œì íŠ¸ ë§ˆê°ì´ ë‚´ì¼ì¸ë° ì•„ì§ ëª» ëëƒˆì–´ìš”\", \"[ê°ì •: ìŠ¤íŠ¸ë ˆìŠ¤] ë§ˆê° ì••ë°•ì´ í¬ì‹œê² ë„¤ìš”. ìš°ì„ ìˆœìœ„ë¥¼ ì •í•´ì„œ ì°¨ê·¼ì°¨ê·¼ ì§„í–‰í•´ë³´ì„¸ìš”.\"),\n            (\"ê·¸ë˜ë„ íŒ€ì›ë“¤ì´ ë„ì™€ì¤˜ì„œ ë‹¤í–‰ì´ì—ìš”\", \"[ê°ì •: ê°ì‚¬/ì•ˆë„] ì´ì „ì— í˜ë“¤ë‹¤ê³  í•˜ì…¨ëŠ”ë°, íŒ€ì›ë“¤ì˜ ë„ì›€ìœ¼ë¡œ ìƒí™©ì´ ë‚˜ì•„ì§€ê³  ìˆë‹¤ë‹ˆ ë‹¤í–‰ì…ë‹ˆë‹¤!\"),\n            (\"ë„¤, ë•ë¶„ì— ê¸°ë¶„ì´ ë‚˜ì•„ì¡Œì–´ìš”\", \"[ê°ì •: ê¸ì •] ì²˜ìŒ í˜ë“¤ë‹¤ê³  í•˜ì…¨ë˜ ê²ƒê³¼ ë¹„êµí•˜ë©´ ë§ì´ íšŒë³µë˜ì‹  ê²ƒ ê°™ì•„ ê¸°ì©ë‹ˆë‹¤.\")\n        ]\n        \n        for user, ai in demo_conversations:\n            memory_demo.add_conversation(user, ai)\n        \n        print(\"\\\\nğŸ“‹ ì‹œì—°ìš© ëŒ€í™” ë‚´ì—­:\")\n        messages = memory_demo.get_conversation_history()\n        for i, msg in enumerate(messages):\n            role = \"ğŸ§‘ ì‚¬ìš©ì\" if i % 2 == 0 else \"ğŸ¤– AI\"\n            print(f\"{role}: {msg.content}\")\n        \n        return\n    \n    # ì‹¤ì œ APIë¥¼ ì‚¬ìš©í•œ ì—ì´ì „íŠ¸\n    try:\n        agent = SentimentMemoryAgent(api_key)\n        \n        # ëŒ€í™”í˜• ì‹œì—°\n        test_inputs = [\n            \"ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í–ˆì–´ìš”\",\n            \"í•˜ì§€ë§Œ ì¡°ê¸ˆ ê±±ì •ì´ ë˜ë„¤ìš”. ë„ˆë¬´ ë³µì¡í•´ ë³´ì—¬ìš”\",\n            \"ì•„ê¹Œ ê±±ì •ëœë‹¤ê³  í–ˆëŠ”ë°, ì–´ë–»ê²Œ ê·¹ë³µí•  ìˆ˜ ìˆì„ê¹Œìš”?\"\n        ]\n        \n        for user_input in test_inputs:\n            print(f\"\\\\n\" + \"-\"*50)\n            response = agent.analyze_and_respond(user_input)\n        \n        # ëŒ€í™” ìš”ì•½\n        print(f\"\\\\nğŸ“„ ëŒ€í™” ìš”ì•½:\")\n        summary = agent.get_conversation_summary()\n        print(summary)\n        \n        # ë©”ëª¨ë¦¬ í†µê³„\n        stats = agent.memory_manager.get_memory_stats()\n        print(f\"\\\\nğŸ“Š ìµœì¢… ë©”ëª¨ë¦¬ í†µê³„:\")\n        for key, value in stats.items():\n            print(f\"   {key}: {value}\")\n            \n    except Exception as e:\n        print(f\"âŒ API ì—ì´ì „íŠ¸ ì‹œì—° ì‹¤íŒ¨: {e}\")\n\n\ndef main():\n    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n    print(\"ğŸš€ Session 3 - Lab 09: LangChain ConversationBufferMemory\")\n    print(\"=\" * 70)\n    \n    try:\n        # 1. ê¸°ë³¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ë²• ì‹œì—°\n        demonstrate_basic_memory()\n        \n        # 2. ê°ì • ë¶„ì„ ë©”ëª¨ë¦¬ ì—ì´ì „íŠ¸ ì‹œì—°\n        demonstrate_sentiment_memory_agent()\n        \n        print(f\"\\\\nâœ… ëª¨ë“  ì‹œì—°ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n        print(f\"\\\\nğŸ“š í•µì‹¬ í•™ìŠµ ë‚´ìš©:\")\n        print(f\"   1. ConversationBufferMemory vs ConversationBufferWindowMemory\")\n        print(f\"   2. ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ë° í† í° ì œí•œ\")\n        print(f\"   3. ê°ì • ë¶„ì„ê³¼ ë©”ëª¨ë¦¬ í†µí•©\")\n        print(f\"   4. ë©”ëª¨ë¦¬ í†µê³„ ë° ëª¨ë‹ˆí„°ë§\")\n        print(f\"   5. ëŒ€í™” ë‚´ìš© ë‚´ë³´ë‚´ê¸°\")\n        \n    except Exception as e:\n        print(f\"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n\n\nif __name__ == \"__main__\":\n    main()\n'''\n\n# íŒŒì¼ ì €ì¥\nwith open('/home/user/output/session3_lab_codes/09_langchain_memory.py', 'w', encoding='utf-8') as f:\n    f.write(memory_code)\n\nprint(\"âœ… 09_langchain_memory.py ìƒì„± ì™„ë£Œ\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… 09_langchain_memory.py ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1. âœ… ğŸ”´ Create 09_langchain_memory.py (í˜ì´ì§€ 29 - ConversationBufferMemory) [completed] (ID: 1)\n2. ğŸ”„ ğŸ”´ Create 10_integrated_demo.py (í†µí•© ì˜ˆì œ) [in_progress] (ID: 2)\n3. â³ ğŸŸ¡ Create requirements.txt with all dependencies [pending] (ID: 3)\n4. â³ ğŸŸ¡ Create .env.example with API key templates [pending] (ID: 4)\n5. â³ ğŸŸ¡ Create comprehensive README.md [pending] (ID: 5)\n6. â³ ğŸ”´ Package all files into ZIP archive [pending] (ID: 6)\n7. â³ ğŸ”´ Upload ZIP to AI Drive for distribution [pending] (ID: 7)"
    }
  ]
}