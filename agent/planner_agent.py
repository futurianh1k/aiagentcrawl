"""
AI ì—ì´ì „íŠ¸ ê¸°ë°˜ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì‹œìŠ¤í…œ - ì‹¤ìŠµ 4
==================================================
ì£¼ì œ: Planner Agent êµ¬í˜„ - Tools ë“±ë¡ ë° ì‹¤í–‰

ëª©í‘œ:
- ì—¬ëŸ¬ Toolsë¥¼ í†µí•©í•˜ëŠ” Planner Agent êµ¬í˜„
- ìì—°ì–´ ì˜ë„ íŒŒì•… ë° Tool ìˆœì°¨ ì‹¤í–‰
- ì‚¬ìš©ì ì§ˆì˜ì— ë”°ë¥¸ ë™ì  Tool ì„ íƒ
- ì „ì²´ End-to-End íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬:
pip install langchain openai python-dotenv
"""

import os
import json
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
from langchain.tools import tool
from langchain.agents import initialize_agent, AgentType
from langchain.llms import OpenAI
from langchain.memory import ConversationBufferMemory
from langchain.schema import AgentAction, AgentFinish

# ì´ì „ ì‹¤ìŠµë“¤ì—ì„œ êµ¬í˜„í•œ Toolë“¤ import (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë³„ë„ íŒŒì¼ì—ì„œ)
from lab2_news_scraper import NewsScraperTool
from lab3_data_analyzer import DataAnalyzerTool

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class NewsAnalysisAgent:
    """ë‰´ìŠ¤ ê°ì„± ë¶„ì„ì„ ìœ„í•œ í†µí•© AI Agent"""

    def __init__(self):
        """Agent ì´ˆê¸°í™”"""

        # OpenAI API í‚¤ ì„¤ì •
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        if not self.openai_api_key:
            print("âš ï¸  ê²½ê³ : OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.openai_api_key = "sk-test-key-replace-with-real-key"

        # LLM ì´ˆê¸°í™”
        self.llm = OpenAI(
            temperature=0.1,  # ë‚®ì€ temperatureë¡œ ì¼ê´€ëœ ì‘ë‹µ
            openai_api_key=self.openai_api_key,
            max_tokens=1000,
            verbose=True
        )

        # ë©”ëª¨ë¦¬ ì„¤ì • (ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€)
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            input_key="input",
            output_key="output"
        )

        # Tools ë“±ë¡
        self.tools = [
            self.scrape_news_tool,
            self.analyze_sentiment_tool,
            self.analyze_trend_tool,
            self.summarize_results_tool
        ]

        # Agent ì´ˆê¸°í™”
        self.agent = initialize_agent(
            tools=self.tools,
            llm=self.llm,
            agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
            memory=self.memory,
            verbose=True,
            max_iterations=5,  # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ì œí•œ
            early_stopping_method="generate"  # ì¡°ê¸° ì¢…ë£Œ ì„¤ì •
        )

        print("ğŸ¤– ë‰´ìŠ¤ ê°ì„± ë¶„ì„ Agentê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“š ë“±ë¡ëœ Tools: {len(self.tools)}ê°œ")

    @tool
    def scrape_news_tool(keyword: str, max_articles: int = 3) -> str:
        """ë‰´ìŠ¤ ê¸°ì‚¬ ë° ëŒ“ê¸€ ìˆ˜ì§‘ ë„êµ¬

        Args:
            keyword (str): ê²€ìƒ‰í•  í‚¤ì›Œë“œ
            max_articles (int): ìµœëŒ€ ìˆ˜ì§‘í•  ê¸°ì‚¬ ìˆ˜

        Returns:
            str: ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„° (JSON í˜•ì‹)
        """
        print(f"ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: {keyword}")

        # ì‹¤ì œë¡œëŠ” NewsScraperTool.scrape_news í˜¸ì¶œ
        # ì—¬ê¸°ì„œëŠ” í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„° ë°˜í™˜
        dummy_data = {
            "keyword": keyword,
            "articles": [
                {
                    "title": f"{keyword} ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤ 1",
                    "url": "https://news.example.com/1",
                    "content": f"{keyword}ì— ëŒ€í•œ ê¸ì •ì ì¸ ì „ë§ì´ ì œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.",
                    "comments": [
                        {"text": "ì¢‹ì€ ì†Œì‹ì´ë„¤ìš”!", "author": "ì‚¬ìš©ì1"},
                        {"text": "ê¸°ëŒ€ë©ë‹ˆë‹¤.", "author": "ì‚¬ìš©ì2"},
                        {"text": "ì‹ ì¤‘í•˜ê²Œ ì§€ì¼œë´ì•¼ê² ì–´ìš”.", "author": "ì‚¬ìš©ì3"}
                    ]
                },
                {
                    "title": f"{keyword} ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤ 2", 
                    "url": "https://news.example.com/2",
                    "content": f"{keyword}ì— ëŒ€í•œ ìš°ë ¤ì˜ ëª©ì†Œë¦¬ë„ ë‚˜ì˜¤ê³  ìˆìŠµë‹ˆë‹¤.",
                    "comments": [
                        {"text": "ê±±ì •ì´ ë©ë‹ˆë‹¤.", "author": "ì‚¬ìš©ì4"},
                        {"text": "ë” ì‹ ì¤‘í•´ì•¼ í•  ê²ƒ ê°™ì•„ìš”.", "author": "ì‚¬ìš©ì5"},
                        {"text": "ì¥ë‹¨ì ì„ ëª¨ë‘ ê³ ë ¤í•´ì•¼ì£ .", "author": "ì‚¬ìš©ì6"}
                    ]
                }
            ],
            "total_articles": 2,
            "total_comments": 6
        }

        return json.dumps(dummy_data, ensure_ascii=False)

    @tool
    def analyze_sentiment_tool(comment_text: str) -> str:
        """ë‹¨ì¼ ëŒ“ê¸€ ê°ì„± ë¶„ì„ ë„êµ¬

        Args:
            comment_text (str): ë¶„ì„í•  ëŒ“ê¸€ í…ìŠ¤íŠ¸

        Returns:
            str: ê°ì„± ë¶„ì„ ê²°ê³¼ (JSON í˜•ì‹)
        """
        print(f"ğŸ“ ëŒ“ê¸€ ê°ì„± ë¶„ì„: {comment_text[:30]}...")

        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì„± ë¶„ì„ (ì‹¤ì œë¡œëŠ” DataAnalyzerTool ì‚¬ìš©)
        positive_words = ["ì¢‹", "í›Œë¥­", "ê¸°ëŒ€", "ì°¬ì„±", "ì§€ì§€", "ë§Œì¡±", "í›Œë¥­"]
        negative_words = ["ë‚˜ì˜", "ê±±ì •", "ìš°ë ¤", "ë°˜ëŒ€", "ì‹¤ë§", "ë¬¸ì œ", "ìœ„í—˜"]

        text_lower = comment_text.lower()

        positive_score = sum(1 for word in positive_words if word in text_lower)
        negative_score = sum(1 for word in negative_words if word in text_lower)

        if positive_score > negative_score:
            sentiment = "ê¸ì •"
            confidence = min(0.9, 0.6 + positive_score * 0.1)
        elif negative_score > positive_score:
            sentiment = "ë¶€ì •"
            confidence = min(0.9, 0.6 + negative_score * 0.1)
        else:
            sentiment = "ì¤‘ë¦½"
            confidence = 0.5

        result = {
            "text": comment_text,
            "sentiment": sentiment,
            "confidence": confidence,
            "reason": f"{'ê¸ì •' if positive_score > 0 else 'ë¶€ì •' if negative_score > 0 else 'ì¤‘ë¦½ì '} í‘œí˜„ ê°ì§€",
            "keywords": positive_words[:2] if positive_score > 0 else negative_words[:2] if negative_score > 0 else ["ì¤‘ë¦½"]
        }

        return json.dumps(result, ensure_ascii=False)

    @tool
    def analyze_trend_tool(comments_json: str, keyword: str) -> str:
        """ëŒ“ê¸€ë“¤ì˜ ì „ì²´ ë™í–¥ ë¶„ì„ ë„êµ¬

        Args:
            comments_json (str): ëŒ“ê¸€ ë°ì´í„° JSON ë¬¸ìì—´
            keyword (str): ë¶„ì„ ëŒ€ìƒ í‚¤ì›Œë“œ

        Returns:
            str: ë™í–¥ ë¶„ì„ ê²°ê³¼ (JSON í˜•ê²©)
        """
        print(f"ğŸ“Š '{keyword}' ë™í–¥ ë¶„ì„ ì¤‘...")

        try:
            # JSON íŒŒì‹±
            data = json.loads(comments_json) if isinstance(comments_json, str) else comments_json

            # ëª¨ë“  ëŒ“ê¸€ ìˆ˜ì§‘
            all_comments = []
            if "articles" in data:
                for article in data["articles"]:
                    if "comments" in article:
                        all_comments.extend(article["comments"])

            if not all_comments:
                return json.dumps({
                    "error": "ë¶„ì„í•  ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.",
                    "keyword": keyword
                }, ensure_ascii=False)

            # ê° ëŒ“ê¸€ì˜ ê°ì„± ë¶„ì„
            sentiment_counts = {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0}

            for comment in all_comments:
                if isinstance(comment, dict) and "text" in comment:
                    # analyze_sentiment_tool í˜¸ì¶œ
                    sentiment_result = json.loads(NewsAnalysisAgent.analyze_sentiment_tool(comment["text"]))
                    sentiment_counts[sentiment_result["sentiment"]] += 1

            total = sum(sentiment_counts.values())
            if total == 0:
                return json.dumps({
                    "error": "ëŒ“ê¸€ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                    "keyword": keyword
                }, ensure_ascii=False)

            # ë¹„ìœ¨ ê³„ì‚°
            distribution = {
                sentiment: count / total 
                for sentiment, count in sentiment_counts.items()
            }

            # ì „ì²´ ê°ì„± ê²°ì •
            max_sentiment = max(distribution.keys(), key=lambda k: distribution[k])

            # ì£¼ìš” ì£¼ì œ ì¶”ì¶œ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ)
            all_text = " ".join([c.get("text", "") for c in all_comments if isinstance(c, dict)])
            common_words = ["ì •ì±…", "ê²½ì œ", "ê¸°ìˆ ", "ì‚¬íšŒ", "ì •ë¶€", "ê¸°ì—…", "ì‹œì¥", "íˆ¬ì"]
            key_topics = [word for word in common_words if word in all_text][:3]

            result = {
                "keyword": keyword,
                "overall_sentiment": max_sentiment,
                "sentiment_distribution": distribution,
                "key_topics": key_topics or [keyword],
                "summary": f"'{keyword}'ì— ëŒ€í•œ ì—¬ë¡ ì€ ì „ë°˜ì ìœ¼ë¡œ {max_sentiment}ì ì…ë‹ˆë‹¤. ì´ {total}ê°œì˜ ëŒ“ê¸€ì„ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤.",
                "total_comments": total
            }

            return json.dumps(result, ensure_ascii=False)

        except Exception as e:
            return json.dumps({
                "error": f"ë™í–¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}",
                "keyword": keyword
            }, ensure_ascii=False)

    @tool
    def summarize_results_tool(trend_json: str) -> str:
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ë° ì¸ì‚¬ì´íŠ¸ ì œê³µ ë„êµ¬

        Args:
            trend_json (str): ë™í–¥ ë¶„ì„ ê²°ê³¼ JSON

        Returns:
            str: ìµœì¢… ìš”ì•½ ë° ì¸ì‚¬ì´íŠ¸
        """
        print("ğŸ“‹ ê²°ê³¼ ìš”ì•½ ì¤‘...")

        try:
            data = json.loads(trend_json) if isinstance(trend_json, str) else trend_json

            if "error" in data:
                return f"âŒ ë¶„ì„ ì‹¤íŒ¨: {data['error']}"

            keyword = data.get("keyword", "ëŒ€ìƒ")
            overall_sentiment = data.get("overall_sentiment", "ì¤‘ë¦½")
            distribution = data.get("sentiment_distribution", {})
            key_topics = data.get("key_topics", [])
            total_comments = data.get("total_comments", 0)

            # í¼ì„¼íŠ¸ë¡œ ë³€í™˜
            percent_dist = {k: f"{v:.1%}" for k, v in distribution.items()}

            summary = f"""
ğŸ¯ **'{keyword}' ê°ì„± ë¶„ì„ ê²°ê³¼**

ğŸ“Š **ì „ì²´ ë™í–¥**: {overall_sentiment}
ğŸ“ˆ **ê°ì„± ë¶„í¬**:
   â€¢ ê¸ì •: {percent_dist.get('ê¸ì •', '0.0%')}
   â€¢ ë¶€ì •: {percent_dist.get('ë¶€ì •', '0.0%')}  
   â€¢ ì¤‘ë¦½: {percent_dist.get('ì¤‘ë¦½', '0.0%')}

ğŸ” **ì£¼ìš” í‚¤ì›Œë“œ**: {', '.join(key_topics) if key_topics else 'ì—†ìŒ'}
ğŸ“ **ë¶„ì„ ëŒ“ê¸€ ìˆ˜**: {total_comments}ê°œ

ğŸ’¡ **ì¸ì‚¬ì´íŠ¸**:
""".strip()

            # ì¸ì‚¬ì´íŠ¸ ìƒì„±
            if overall_sentiment == "ê¸ì •":
                summary += f"\nâ€¢ {keyword}ì— ëŒ€í•œ ì—¬ë¡ ì´ ì „ë°˜ì ìœ¼ë¡œ ê¸ì •ì ì…ë‹ˆë‹¤."
                summary += f"\nâ€¢ ê¸ì • ë¹„ìœ¨ì´ {percent_dist.get('ê¸ì •', '0%')}ë¡œ ë†’ì€ ì§€ì§€ë¥¼ ë°›ê³  ìˆìŠµë‹ˆë‹¤."
            elif overall_sentiment == "ë¶€ì •":
                summary += f"\nâ€¢ {keyword}ì— ëŒ€í•œ ìš°ë ¤ì˜ ëª©ì†Œë¦¬ê°€ ë†’ìŠµë‹ˆë‹¤."
                summary += f"\nâ€¢ ë¶€ì • ë¹„ìœ¨ì´ {percent_dist.get('ë¶€ì •', '0%')}ë¡œ ì‹ ì¤‘í•œ ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤."
            else:
                summary += f"\nâ€¢ {keyword}ì— ëŒ€í•œ ì—¬ë¡ ì´ ë¶„ì‚°ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
                summary += f"\nâ€¢ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì˜ê²¬ì´ ë‚˜ë‰˜ê³  ìˆì–´ ê· í˜•ì¡íŒ ì ‘ê·¼ì´ ì¤‘ìš”í•©ë‹ˆë‹¤."

            return summary

        except Exception as e:
            return f"âŒ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"

    def analyze_news_sentiment(self, user_query: str) -> str:
        """ì‚¬ìš©ì ì§ˆì˜ë¥¼ ë°›ì•„ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ë©”ì¸ ë©”ì„œë“œ"""
        print(f"\nğŸ¤– ì‚¬ìš©ì ì§ˆì˜: {user_query}")
        print("=" * 60)

        try:
            # Agent ì‹¤í–‰
            response = self.agent.run(input=user_query)

            print("=" * 60)
            print(f"âœ… ìµœì¢… ì‘ë‹µ:")
            print(response)

            return response

        except Exception as e:
            error_msg = f"âŒ Agent ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"
            print(error_msg)
            return error_msg

    def get_conversation_history(self) -> List[Dict]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return self.memory.chat_memory.messages

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Planner Agent ì‹¤ìŠµ ì‹œì‘")
    print("=" * 60)

    # Agent ì´ˆê¸°í™”
    agent = NewsAnalysisAgent()

    # í…ŒìŠ¤íŠ¸ ì§ˆì˜ë“¤
    test_queries = [
        "ì‚¼ì„±ì „ì ì£¼ê°€ì— ëŒ€í•œ ìµœê·¼ ë‰´ìŠ¤ì˜ ì—¬ë¡ ì„ ë¶„ì„í•´ì¤˜",
        "AI ê¸°ìˆ  ë°œì „ì— ëŒ€í•œ ì‚¬ëŒë“¤ì˜ ë°˜ì‘ì€ ì–´ë•Œ?",
        "ë¶€ë™ì‚° ì‹œì¥ ë™í–¥ì— ëŒ€í•œ ëŒ“ê¸€ë“¤ì„ ë¶„ì„í•´ì„œ ìš”ì•½í•´ì¤˜"
    ]

    print("\nğŸ“ í…ŒìŠ¤íŠ¸ ì§ˆì˜ ì‹¤í–‰:")

    for i, query in enumerate(test_queries, 1):
        print(f"\n\n[í…ŒìŠ¤íŠ¸ {i}]")
        print("-" * 50)

        response = agent.analyze_news_sentiment(query)

        print("\n" + "="*40)
        print(f"[í…ŒìŠ¤íŠ¸ {i} ì™„ë£Œ]\n")

        # ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ í™•ì¸
        history = agent.get_conversation_history()
        print(f"ğŸ“š ëŒ€í™” íˆìŠ¤í† ë¦¬ ê¸¸ì´: {len(history)}")

    print("\n\nğŸ¯ ì£¼ìš” í•™ìŠµ í¬ì¸íŠ¸:")
    print("1. ì—¬ëŸ¬ Toolsë¥¼ í•˜ë‚˜ì˜ Agentì— í†µí•© ë“±ë¡")
    print("2. ì‚¬ìš©ì ì§ˆì˜ì— ë”°ë¥¸ ë™ì  Tool ì„ íƒ ë° ì‹¤í–‰")
    print("3. ConversationBufferMemoryë¡œ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€")
    print("4. Tool ê°„ ë°ì´í„° ì „ë‹¬ ë° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•")
    print("5. Agentì˜ ReAct íŒ¨í„´ (Reason + Act) ê´€ì°°")

    print("\nâš ï¸  ì£¼ì˜ì‚¬í•­:")
    print("- ëª¨ë“  Toolsê°€ ì •ìƒ ì‘ë™í•´ì•¼ Agentê°€ ì˜¬ë°”ë¥´ê²Œ ì‹¤í–‰ë¨")
    print("- max_iterations ì„¤ì •ìœ¼ë¡œ ë¬´í•œ ë£¨í”„ ë°©ì§€")
    print("- Tool ê°„ ë°ì´í„° í˜•ì‹ ì¼ì¹˜ (JSON ë¬¸ìì—´ ì „ë‹¬)")
    print("- verbose=Trueë¡œ Agent ì¶”ë¡  ê³¼ì • ê´€ì°° ê°€ëŠ¥")

    print("\nâœ¨ í™•ì¥ ê°€ëŠ¥í•œ ê¸°ëŠ¥:")
    print("- ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ Tool ì¶”ê°€")
    print("- ì‹œê°í™” ìƒì„± Tool ì¶”ê°€") 
    print("- ì´ë©”ì¼/ìŠ¬ë™ ì•Œë¦¼ Tool ì¶”ê°€")
    print("- ìŠ¤ì¼€ì¤„ë§ ë° ìë™í™” Tool ì¶”ê°€")

if __name__ == "__main__":
    main()
